[
  {
    "objectID": "glossary/m2mo/derivatives.html",
    "href": "glossary/m2mo/derivatives.html",
    "title": "Glossary - Derivatives market",
    "section": "",
    "text": "Fondements\n\nArbitrage\n\nOpportunit√© de gain certain sans investissement initial et sans risque de perte.\nFormellement, il existe une strat√©gie autofinanc√©e de valeur initiale nulle dont la valeur finale est presque s√ªrement positive et strictement positive avec probabilit√© non nulle.\n\nMarch√© primaire\n\nEnsemble des actifs fondamentaux du march√©, g√©n√©ralement compos√© d‚Äôun actif sans risque et de \\(q\\) actifs risqu√©s.\n\nActif sans risque\n\nActif de prix \\(S^0_t\\) √©voluant de mani√®re d√©terministe, typiquement selon\n\\[\ndS^0_t = r_t S^0_t \\, dt,\n\\] o√π \\(r_t\\) est le taux d‚Äôint√©r√™t instantan√©.\n\nActif risqu√©\n\nActif de prix \\(S^i_t\\) dont la dynamique est soumise √† une composante al√©atoire, par exemple \\[\ndS^i_t = \\mu_t S^i_t dt + \\sigma_t S^i_t dW_t.\n\\]\n\nMarch√© complet\n\nMarch√© dans lequel tout payoff contingent mesurable peut √™tre parfaitement r√©pliqu√© par une strat√©gie dynamique autofinanc√©e.\n\nMesure risque-neutre\n\nMesure de probabilit√© √©quivalente √† la probabilit√© historique sous laquelle les prix actualis√©s des actifs sont des martingales.\n\nMartingale\n\nProcessus \\((M_t)_{t \\ge 0}\\) adapt√© tel que, pour tout \\(s \\le t\\), \\[\n\\mathbb{E}[M_t \\mid \\mathcal{F}_s] = M_s.\n\\]\n\nStrat√©gie autofinanc√©e\n\nStrat√©gie \\((\\delta^0_t, \\delta^1_t, \\dots, \\delta^q_t)\\) telle que la variation de richesse v√©rifie \\[\ndV_t = \\sum_{i=0}^q \\delta^i_t \\, dS^i_t,\n\\] c‚Äôest-√†-dire qu‚Äôaucun apport ni retrait externe de capital n‚Äôintervient.\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCalcul stochastique\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo/EDP\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "glossary/m2mo/edp.html",
    "href": "glossary/m2mo/edp.html",
    "title": "Monte-Carlo/EDP",
    "section": "",
    "text": "Num√©rique\n\nMonte Carlo\n\nM√©thode de simulation stochastique pour le pricing.\n\nDiff√©rences finies\n\nM√©thode num√©rique de r√©solution d‚ÄôEDP.\n\nCrank-Nicolson\n\nSch√©ma implicite semi-centr√© stable et d‚Äôordre 2.\n\nPSOR\n\nM√©thode it√©rative pour r√©soudre les probl√®mes √† obstacle.\n\nCalibration\n\nEstimation des param√®tres d‚Äôun mod√®le pour reproduire les prix de march√©.\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCalcul stochastique\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary - Derivatives market\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A few things keep me busy these days:"
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "",
    "text": "flowchart LR\n  A[Conditional Invitation] --&gt; \n  B(Appointment Letter) --&gt;\n  C{{DS-2019}} --&gt;\n  D{{Visa Application}}  --&gt;\n  E(Visiting Baruch)\n\n\n\n\n\n\n\n\nTo begin, you need to secure a conditional invitation letter from a faculty member who will support your scholarship application. The following documents are usually helpful:\n\nCV\nResearch Plan\nEnglish Proficiency Certificate\n\nYour potential host may schedule an interview to discuss your proposal and assess the possibilities.\n\n\n\nOnce you receive the fellowship or scholarship certificate, forward it to your host faculty member. The certificate is useful for the department or school to issue an official invitation letter. You may ignore the first step if you were hired or support by your host faculty.\n\nThe official letter, similar to the conditional invitation, will be signed by the Department Chair or Dean.\nThis step formalizes your appointment and enables the next stages of paperwork.\n\n\n\n\nThe DS-2019 form involves several internal approval processes before the designated office can issue it. These include:\n\nApprovals from the Department Chair, Dean, and Provost\n\nExport Control Clearance\n\nOverview of the Process from the Baruch College Export Control Clearance for J-1 Research Scholar\n\nTo comply with the ‚ÄúGuidance on Onboarding J-1 Researchers and Scholars Exchange Visitors‚Äù memorandum dated July 26, 2021 from Robert T. Maruca, CUNY Associate University Provost for Planning, and Tamera Schneider, CUNY Associate Vice Chancellor & University Provost for Research, the Weissman Center for International Business has devised the following procedure for obtaining export control clearance for all prospective J-1 Research Scholars.*\nThe process involves several steps.\nThe process begins whenever a Baruch department or other College office (‚ÄúDepartment‚Äù) receives an inquiry from someone requesting to be hosted as a prospective international J-1 Research Scholar (‚ÄúScholar‚Äù). If the Department decides it wishes to host this Scholar, these steps must be taken:\nPhase I: Departmental Preparation\n\nThe Department sends two forms to the Scholar:\n\n\n\nApplication for Prospective International J-1 Scholar\nCUNY J-1 Export-Control Questionnaire\n\n\nThe Scholar returns the completed and signed forms along with certified scanned copies of the required documentation, plus a current CV/Resume, to the Department.\nThe Department completes and signs these two forms:\n\n\n\nBaruch College Application to Host International J-1 Scholar\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar\n\n\nThe Department submits the following materials (completed and with all required signatures) to the SEVIS Responsible Officer (RO) for this J-1 program (currently Dr.¬†Richard Mitten):\n\n\nApplication for Prospective International J-1 Scholar and accompanying documentation (completed by Scholar)\n\nCUNY J-1 Export-Control Questionnaire (completed by Scholar)\n\nCV/Resume of the Scholar (completed by Scholar)\n\nBaruch College Application to Host International J-1 Scholar with required signatures (completed by Department)\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar (completed by Department)\nAn original official letter of invitation (completed by Department)\n\nPhase II: Preliminary Campus Review\n\nThe RO reviews documents and submits them to the Baruch College Provost for preliminary campus review.\nAfter consulting with the ‚Äúappropriate stakeholders‚Äù (per CUNY regulations), the Provost returns the entire dossier to the SEVIS RO, recording the decision with a signature and date on page 2 of this form (Provisional Approval-Provost).\n\nPhase III: CUNY Screening\n\nIf the Provost approves the request to host the Scholar, the RO forwards the dossier to the CUNY Office of International Student and Scholar Services (‚ÄúCUNY ISSS‚Äù) for ‚ÄúVisual Compliance Screening‚Äù (per CUNY regulations).\nAfter completing its screening, the CUNY ISSS returns the dossier to the RO with its findings.\nThe RO forwards the dossier to the Provost for final campus review.\n\nPhase IV: Final Campus Review\n\nThe Provost approves, or withholds final approval to the request to host the Scholar, and conveys this decision, along with the dossier of documents, to the RO, recording the decision with a signature and date on Export Control Clearance Form for Prospective J-1 Research Scholar.\nIf the Provost approves the request, the RO notifies the Department, issues the DS-2019 for the Scholar, and sends it and additional documentation needed to apply for a J-1 visa to the Scholar, with a copy to the Department.\n\n*Please note: The process for described below applies to any prospective J-1 Research Scholar only (Short-Term Scholars are processed by the International Student Service Center).\n\nAdditional documents:\nFinancial ability to cover the living expenses for J-1 exchange visitors & J-2 dependents:\n\nJ-1 exchange visitor $32,000/year; $2,700/month (research scholar, professor, short-term scholar & student)\n\nJ-2 spouse $7,200/year; $600/month\n\nJ-2 child (under 21) $4,800/year per child; $400/month per child\n\nIf the scholarship amount is less than the listed requirement, you may need to provide a bank certificate of personal savings to cover the difference.\nMinimum insurance coverage must provide:\n\nMedical benefits of at least $100,000 per accident or illness;\n\nRepatriation of remains in the amount of $25,000;\n\nExpenses associated with the medical evacuation of exchange visitors to his or her home country in the amount of $50,000; and\n\nDeductibles not to exceed $500 per accident or illness.\n\n\n\n\nCheck the latest information from the U.S. Embassy website to understand the requirements and materials needed for your visa application. Make sure to complete this step promptly after receiving your DS-2019 form.\n\n\n\nOnce all paperwork is completed, you can prepare for your arrival at Baruch College in New York City! New York City is Your Classroom and Office!\n\n\n\n\nTimeline\n\nThe entire process may take several months, particularly if additional documentation is requested. Begin early and allow flexibility in your start date to accommodate any delays.\n\nProactive Communication\n\nKeep in close contact with your host faculty and administrative offices to ensure you stay updated on requirements and approvals.\n\nUnofficial Guide\n\nThis is an information page for your reference only. You should always follow the official guide to prepare your documents."
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#paperwork-flow",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#paperwork-flow",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "",
    "text": "flowchart LR\n  A[Conditional Invitation] --&gt; \n  B(Appointment Letter) --&gt;\n  C{{DS-2019}} --&gt;\n  D{{Visa Application}}  --&gt;\n  E(Visiting Baruch)\n\n\n\n\n\n\n\n\nTo begin, you need to secure a conditional invitation letter from a faculty member who will support your scholarship application. The following documents are usually helpful:\n\nCV\nResearch Plan\nEnglish Proficiency Certificate\n\nYour potential host may schedule an interview to discuss your proposal and assess the possibilities.\n\n\n\nOnce you receive the fellowship or scholarship certificate, forward it to your host faculty member. The certificate is useful for the department or school to issue an official invitation letter. You may ignore the first step if you were hired or support by your host faculty.\n\nThe official letter, similar to the conditional invitation, will be signed by the Department Chair or Dean.\nThis step formalizes your appointment and enables the next stages of paperwork.\n\n\n\n\nThe DS-2019 form involves several internal approval processes before the designated office can issue it. These include:\n\nApprovals from the Department Chair, Dean, and Provost\n\nExport Control Clearance\n\nOverview of the Process from the Baruch College Export Control Clearance for J-1 Research Scholar\n\nTo comply with the ‚ÄúGuidance on Onboarding J-1 Researchers and Scholars Exchange Visitors‚Äù memorandum dated July 26, 2021 from Robert T. Maruca, CUNY Associate University Provost for Planning, and Tamera Schneider, CUNY Associate Vice Chancellor & University Provost for Research, the Weissman Center for International Business has devised the following procedure for obtaining export control clearance for all prospective J-1 Research Scholars.*\nThe process involves several steps.\nThe process begins whenever a Baruch department or other College office (‚ÄúDepartment‚Äù) receives an inquiry from someone requesting to be hosted as a prospective international J-1 Research Scholar (‚ÄúScholar‚Äù). If the Department decides it wishes to host this Scholar, these steps must be taken:\nPhase I: Departmental Preparation\n\nThe Department sends two forms to the Scholar:\n\n\n\nApplication for Prospective International J-1 Scholar\nCUNY J-1 Export-Control Questionnaire\n\n\nThe Scholar returns the completed and signed forms along with certified scanned copies of the required documentation, plus a current CV/Resume, to the Department.\nThe Department completes and signs these two forms:\n\n\n\nBaruch College Application to Host International J-1 Scholar\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar\n\n\nThe Department submits the following materials (completed and with all required signatures) to the SEVIS Responsible Officer (RO) for this J-1 program (currently Dr.¬†Richard Mitten):\n\n\nApplication for Prospective International J-1 Scholar and accompanying documentation (completed by Scholar)\n\nCUNY J-1 Export-Control Questionnaire (completed by Scholar)\n\nCV/Resume of the Scholar (completed by Scholar)\n\nBaruch College Application to Host International J-1 Scholar with required signatures (completed by Department)\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar (completed by Department)\nAn original official letter of invitation (completed by Department)\n\nPhase II: Preliminary Campus Review\n\nThe RO reviews documents and submits them to the Baruch College Provost for preliminary campus review.\nAfter consulting with the ‚Äúappropriate stakeholders‚Äù (per CUNY regulations), the Provost returns the entire dossier to the SEVIS RO, recording the decision with a signature and date on page 2 of this form (Provisional Approval-Provost).\n\nPhase III: CUNY Screening\n\nIf the Provost approves the request to host the Scholar, the RO forwards the dossier to the CUNY Office of International Student and Scholar Services (‚ÄúCUNY ISSS‚Äù) for ‚ÄúVisual Compliance Screening‚Äù (per CUNY regulations).\nAfter completing its screening, the CUNY ISSS returns the dossier to the RO with its findings.\nThe RO forwards the dossier to the Provost for final campus review.\n\nPhase IV: Final Campus Review\n\nThe Provost approves, or withholds final approval to the request to host the Scholar, and conveys this decision, along with the dossier of documents, to the RO, recording the decision with a signature and date on Export Control Clearance Form for Prospective J-1 Research Scholar.\nIf the Provost approves the request, the RO notifies the Department, issues the DS-2019 for the Scholar, and sends it and additional documentation needed to apply for a J-1 visa to the Scholar, with a copy to the Department.\n\n*Please note: The process for described below applies to any prospective J-1 Research Scholar only (Short-Term Scholars are processed by the International Student Service Center).\n\nAdditional documents:\nFinancial ability to cover the living expenses for J-1 exchange visitors & J-2 dependents:\n\nJ-1 exchange visitor $32,000/year; $2,700/month (research scholar, professor, short-term scholar & student)\n\nJ-2 spouse $7,200/year; $600/month\n\nJ-2 child (under 21) $4,800/year per child; $400/month per child\n\nIf the scholarship amount is less than the listed requirement, you may need to provide a bank certificate of personal savings to cover the difference.\nMinimum insurance coverage must provide:\n\nMedical benefits of at least $100,000 per accident or illness;\n\nRepatriation of remains in the amount of $25,000;\n\nExpenses associated with the medical evacuation of exchange visitors to his or her home country in the amount of $50,000; and\n\nDeductibles not to exceed $500 per accident or illness.\n\n\n\n\nCheck the latest information from the U.S. Embassy website to understand the requirements and materials needed for your visa application. Make sure to complete this step promptly after receiving your DS-2019 form.\n\n\n\nOnce all paperwork is completed, you can prepare for your arrival at Baruch College in New York City! New York City is Your Classroom and Office!\n\n\n\n\nTimeline\n\nThe entire process may take several months, particularly if additional documentation is requested. Begin early and allow flexibility in your start date to accommodate any delays.\n\nProactive Communication\n\nKeep in close contact with your host faculty and administrative offices to ensure you stay updated on requirements and approvals.\n\nUnofficial Guide\n\nThis is an information page for your reference only. You should always follow the official guide to prepare your documents."
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#links",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#links",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "Links",
    "text": "Links\n\nBaruch Faculty Profiles\nStudy Abroad Office\nCUNY International Student & Scholar Services (ISSS)\nCUNY Export Control\n\nCreated: Januaray 3, 2025"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nCalcul stochastique\n\n\n\n\nGlossary - Derivatives market\n\n\n\n\nMonte-Carlo/EDP\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html",
    "href": "posts/m2mo/edp/edp_european_opt.html",
    "title": "Finite difference Methods for European Options",
    "section": "",
    "text": "Dans ce tp, nous √©tudion des sch√©mas aux diff√©rences finies pour l‚Äô√©quation de Black-Scholes pos√©e dans les variables primitives, apr√®s localisation dans un rectangle dans les variables temps-spot.\nCes sch√©mas nous permettront d‚Äôobtenir une approximation num√©rique de la fonction de prix d‚Äôun put europ√©en \\(v(t,s)\\), avec \\(t \\in [0,T]\\) et \\(s \\in [S_{min}, S_{max}]\\). Cette fonction satisfait l‚Äô√©quation de Black-Scholes sur le domaine tronqu√© \\(\\Omega = [0,T] \\times [S_{min}, S_{max}]\\) :\n\\[\n\\begin{cases}\n\\frac{\\partial v}{\\partial t}\n+ \\frac{1}{2} \\sigma^2 s^2 \\frac{\\partial^2 v}{\\partial s^2}\n+ r s \\frac{\\partial v}{\\partial s}\n- r v = 0, \\quad (t,s) \\in \\Omega, \\\\\nv(T,s) = \\phi(s) = \\max(K - s, 0),\n\\quad s \\in [S_{\\min}, S_{\\max}], \\\\\nv(t,S_{\\min}) = K e^{-r(T - t)} - S_{\\min},\n\\quad t \\in (0,T), \\\\\nv(t,S_{\\max}) = 0,\n\\quad t \\in (0,T).\n\\end{cases}\n\\]\nNous consid√©rons la grille discr√®te suivante : \\(h = \\frac{S_{max} - S_{min}}{J + 1}\\) et \\(\\Delta t = \\frac{T}{N}\\), avec \\(J\\) et \\(N\\) des entiers positifs, et : - \\(s_j = S_{min} + j h\\), pour \\(j = 0, \\ldots, J + 1\\), - \\(t_n = n \\Delta t\\), pour \\(n = 0, \\ldots, N\\).\nOn cherche une approximation \\(U_j^n \\approx v(t_n, s_j)\\) pour \\(j = 1, \\ldots, J\\) et \\(n = 0, \\ldots, N\\).\nLes sch√©mas aux diff√©rences finies nous permettront de discr√©tiser l‚Äô√©quation de Black-Scholes du prix d‚Äôun put europ√©en sur cette grille. Dans ce TP, nous consid√©rons divers \\(\\theta\\)-sch√©mas num√©riques pour ce probl√®me: Le sch√©ma d‚ÄôEuler explicite, puis Euler implicite, et enfin de Crank-Nicolson. On √©tudie l‚Äôerreur de consistance pour ces sch√©mas. Ces sch√©mas conduisent √† des relations de r√©currence dans \\(\\mathbb{R}^n\\), chaque pas consistant √©ventuellement √† r√©soudre un syst√®me lin√©aire dans le cas des sch√©mas implicites.\nOn √©tudie enfin la convergence de ces \\(\\theta\\)-sch√©mas. On verra appara√Ætre la notion de stabilit√© conditionnelle (et de condition CFL) pour certains sch√©mas tels que le sch√©ma d‚ÄôEuler Explicite, et la notion de stabilit√© inconditionnelle pour d‚Äôautres sch√©mas tels que le sch√©ma d‚ÄôEuler Implicite ou de Crank-Nicolson."
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-deuler-explicite-ee",
    "href": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-deuler-explicite-ee",
    "title": "Finite difference Methods for European Options",
    "section": "Sch√©ma d‚ÄôEuler explicite (EE)",
    "text": "Sch√©ma d‚ÄôEuler explicite (EE)\nLe sch√©ma d‚ÄôEuler explicite est un sch√©ma bas√© sur une discr√©tisation explicite en temps. La discr√©tisation de l‚ÄôEDP est bas√©e sur des approximations centr√©es. D√®s lors, on approxime les d√©riv√©es partielles de la mani√®re suivante :\n\\[\n\\begin{cases}\n\\frac{U_j^{n+1} - U_j^n}{\\Delta t} + \\frac{1}{2} \\sigma^2 s_j^2 \\frac{U_{j+1}^n - 2 U_j^n + U_{j-1}^n}{h^2} + r s_j \\frac{U_{j+1}^n - U_{j-1}^n}{2 h} - r U_j^n = 0, \\quad j = 1, \\ldots, J,\\quad n = 0, \\ldots, N-1. \\\\\nU_j^0 = \\phi(s_j), \\quad j = 1, \\ldots, J. \\\\\nU_0^n = K e^{-r(T - t_n)} - S_{min}, \\quad n = 0, \\ldots, N. \\\\\nU_{J+1}^n = 0, \\quad n = 0, \\ldots, N.\n\\end{cases}\n\\]\nOn peut la r√©√©crire sous la forme matricielle afin d‚Äôextraire une solution num√©rique dite explicite : Sous forme matricielle, le sch√©ma s‚Äô√©crit :\n\\[\n\\begin{array}{l}\n\\frac{U^{n+1} - A U^n}{\\Delta t} +  A U^n + q(t_n) = 0, \\quad n = 0, \\ldots, N-1, \\\\\nU^0 = (\\phi(s_i))_{1 \\leq i \\leq J},\n\\end{array}\n\\]\no√π\n\n\\(A\\) est une matrice carr√©e tridiagonale de taille \\(J \\times J\\). En posant \\(\\alpha_j = \\frac{\\sigma^2}{2} \\frac{s_j^2}{h^2}\\) et \\(\\beta_j = r \\frac{s_j}{2 h}\\), les coefficients de la matrice \\(A\\) sont donn√©s par :\n\\[\n\\begin{cases}\nA_{j,j-1} = -\\alpha_j + \\beta_j, \\quad j= 2, \\ldots, J, \\\\\nA_{j,j} = 2\\alpha_j + r, \\quad j = 1, \\ldots, J, \\\\\nA_{j,j+1} = -\\alpha_j - \\beta_j, \\quad j = 1, \\ldots, J.\n\\end{cases}\n\\]\n\\(q(t_n)\\) un vecteur de \\(\\mathbb{R}^J\\) qui d√©pendent des param√®tres du mod√®le et de la discr√©tisation spatiale donn√© par :\n\\[\nq_j(t_n) =\n\\begin{cases}\n(-\\alpha_1 + \\beta_1) U_0^n, \\quad j = 1, \\\\\n0, \\quad j = 2, \\ldots, J-1, \\\\\n(-\\alpha_J + \\beta_J) U_{J+1}^n, \\quad j = J.\n\\end{cases}\n\\]\n\nDe fait, on obtient la relation de r√©currence explicite permettant de calculer \\(U^{n+1}\\) en fonction de \\(U^n\\) :\n\\[\nU^{n+1} = A U^n - \\Delta t ( A U^n + q(t_n) ), \\quad n = 0, \\ldots, N-1,\n\\]"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-deuler-implicite-ei",
    "href": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-deuler-implicite-ei",
    "title": "Finite difference Methods for European Options",
    "section": "Sch√©ma d‚ÄôEuler implicite (EI)",
    "text": "Sch√©ma d‚ÄôEuler implicite (EI)\nLe sch√©ma d‚ÄôEuler implicite est un sch√©ma bas√© sur une discr√©tisation implicite en temps. La discr√©tisation de l‚ÄôEDP est bas√©e sur des approximations centr√©es. D√®s lors, on approxime les d√©riv√©es partielles de la mani√®re suivante :\n\\[\\begin{cases}\n\\frac{U_j^{n+1} - U_j^n}{\\Delta t} + \\frac{1}{2} \\sigma^2 s_j^2 \\mathbf{\\frac{U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1}}{h^2} + r s_j \\frac{U_{j+1}^{n+1} - U_{j-1}^{n+1}}{2 h} - r U_j^{n+1}} = 0, \\quad j = 1, \\ldots, J,\\quad n = 0, \\ldots, N-1. \\\\\nU_j^0 = \\phi(s_j), \\quad j = 1, \\ldots, J. \\\\  \nU_0^n = K e^{-r(T - t_n)} - S_{min}, \\quad n = 0, \\ldots, N. \\\\\nU_{J+1}^n = 0, \\quad n = 0, \\ldots, N.\n\\end{cases}\n\\]\nDans ce cas, la relation de r√©currence s‚Äô√©crit sous la forme matricielle suivante :\n\\[\n\\begin{array}{l}\n\\frac{U^{n+1} - A U^n}{\\Delta t} +  A U^{n+1} + q(t_{n+1}) = 0, \\quad n = 0, \\ldots, N-1, \\\\\nU^0 = (\\phi(s_i))_{1 \\leq i \\leq J},\n\\end{array}\n\\]\no√π \\(A\\) et \\(q(t_n)\\) sont d√©finis comme pr√©c√©demment. On obtient ainsi la relation de r√©currence implicite suivante :\n\\[\n( I + \\Delta t A ) U^{n+1} = U^n - \\Delta t q(t_{n+1}), \\quad n = 0, \\ldots, N-1,\n\\] o√π \\(I\\) est la matrice identit√© de taille \\(J \\times J\\)."
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-de-crank-nicolson-cn",
    "href": "posts/m2mo/edp/edp_european_opt.html#sch√©ma-de-crank-nicolson-cn",
    "title": "Finite difference Methods for European Options",
    "section": "Sch√©ma de Crank-Nicolson (CN)",
    "text": "Sch√©ma de Crank-Nicolson (CN)\nLe sch√©ma de Crank-Nicolson est un sch√©ma bas√© sur une discr√©tisation implicite en temps. La discr√©tisation de l‚ÄôEDP est bas√©e sur des approximations centr√©es. D√®s lors, on approxime les d√©riv√©es partielles de la mani√®re suivante :\n\\[\n\\begin{cases}\n\\frac{U_j^{n+1} - U_j^n}{\\Delta t} + \\frac{1}{2} \\left( - \\sigma^2 s_j^2 \\frac{U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1}}{h^2}  - r s_j \\frac{U_{j+1}^{n+1} - U_{j-1}^{n+1}}{2 h} + r U_j^{n+1} \\right. \\\\\n\\quad + \\frac{1}{2} \\left( - \\sigma^2 s_j^2 \\frac{U_{j+1}^n - 2 U_j^n + U_{j-1}^n}{h^2} - r s_j \\frac{U_{j+1}^n - U_{j-1}^n}{2 h} + r U_j^n \\right) = 0, \\quad j = 1, \\ldots, J,\\quad n = 0, \\ldots, N-1. \\\\\nU_j^0 = \\phi(s_j), \\quad j = 1, \\ldots, J. \\\\\nU_0^n = K e^{-r(T - t_n)} - S_{min}, \\quad n = 0, \\ldots, N. \\\\\nU_{J+1}^n = 0, \\quad n = 0, \\ldots, N.\n\\end{cases}\n\\]\nLa relation de r√©currence s‚Äô√©crit sous la forme matricielle suivante : \\[\n\\begin{array}{l}\n\\frac{U^{n+1} - A U^n}{\\Delta t} +  \\frac{1}{2} A ( U^{n+1} + U^n ) + \\frac{1}{2} ( q(t_{n+1}) + q(t_n) ) = 0, \\quad n = 0, \\ldots, N-1, \\\\\nU^0 = (\\phi(s_i))_{1 \\leq i \\leq J},\n\\end{array}\n\\] o√π \\(A\\) et \\(q(t_n)\\) sont d√©finis comme pr√©c√©demment. On obtient ainsi la relation de r√©currence implicite suivante : \\[\\left( I + \\frac{\\Delta t}{2} A \\right) U^{n+1} = \\left( I - \\frac{\\Delta t}{2} A \\right) U^n - \\frac{\\Delta t}{2} ( q(t_{n+1}) + q(t_n) ), \\quad n = 0, \\ldots, N-1,\n\\] o√π \\(I\\) est la matrice identit√© de taille \\(J \\times J\\)."
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#choix-dimpl√©mentations",
    "href": "posts/m2mo/edp/edp_european_opt.html#choix-dimpl√©mentations",
    "title": "Finite difference Methods for European Options",
    "section": "Choix d‚Äôimpl√©mentations",
    "text": "Choix d‚Äôimpl√©mentations\nPuisque les sch√©mas impl√©ment√©s partagent de nombreuses caract√©ristiques communes, nous d√©finissons une classe de base abstraite SchemeBase qui encapsule les param√®tres financiers, la grille discr√®te, les conditions initiales et aux limites, ainsi que la construction de la matrice \\(A\\) et du vecteur \\(q(t)\\). Les sch√©mas sp√©cifiques h√©riteront de cette classe de base et impl√©menteront la m√©thode solve() pour r√©soudre le sch√©ma num√©rique particulier.\nTrois classes filles seront ensuite d√©finies : SchemeEE pour le sch√©ma d‚ÄôEuler explicite, SchemeEI pour le sch√©ma d‚ÄôEuler implicite, et SchemeCN pour le sch√©ma de Crank-Nicolson. Chacune de ces classes impl√©mentera la m√©thode solve() en fonction de la nature explicite ou implicite du sch√©ma.\nLes codes sont retrouv√©s sur ce lien github github.\n\n# Package imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as lng\nfrom scipy.sparse import diags,eye\nfrom abc import ABC, abstractmethod\nimport scipy.stats as stats\nfrom scipy.sparse import csr_matrix as sparse\nfrom scipy.sparse.linalg import spsolve\nimport pandas as pd\nimport time\nfrom pprint import pprint\n\n\nclass SchemeBase(ABC):\n    \"\"\"\n    Classe de base pour les sch√©mas num√©riques de l'√©quation de Black-Scholes.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        # Financial parameters\n        self.r = r\n        self.sigma = sigma\n        self.K = K\n        self.T = T\n\n        # Numerical parameters\n        self.N = N\n        self.J = J\n        self.Smin = Smin\n        self.Smax = Smax\n\n        # Grids\n        self.dt = T / N\n        self.h = (Smax - Smin) / (J + 1)\n        self.s = Smin + self.h * np.arange(1, J + 1)\n\n        # Operator\n        self.A, self.alpha, self.beta = self._build_matrix_A()\n\n    def phi(self, s):\n        \"\"\"\n        Condition initiale (payoff) pour un put europ√©en.\n        ùúô(s) = max(K - s, 0)\n        \"\"\"\n        return np.maximum(self.K - s, 0)\n\n    def uleft(self, t):\n        \"\"\"\n        Condition aux limites √† gauche pour un put europ√©en.\n        u( t, Smin ) = K * exp(-r * t) - S\n        \"\"\"\n        return self.K * np.exp(-self.r * t) - self.Smin\n\n    def uright(self, t):\n        \"\"\"\n        Condition aux limites √† droite pour un put europ√©en.\n        u( t, Smax ) = 0\n        \"\"\"\n        return 0.0\n\n    def _build_matrix_A(self):\n        \"\"\"\n        Matrice d'amplification A.\n        \"\"\"\n        alpha = 0.5 * self.sigma**2 * (self.s**2 / self.h**2)\n        beta  = self.r * self.s / (2 * self.h)\n\n        lower = -alpha[1:] + beta[1:]        # sous-diagonale\n        main  = 2 * alpha + self.r            # diagonale principale\n        upper = -alpha[:-1] - beta[:-1]       # sur-diagonale\n\n        A = diags(\n            diagonals=[lower, main, upper],\n            offsets=[-1, 0, 1],\n            shape=(self.J, self.J),\n            format=\"csr\"\n        )\n\n        return A, alpha, beta\n\n\n    def q(self, t):\n        \"\"\"\n        Vecteur des conditions aux limites.\n        \"\"\"\n        y = np.zeros(self.J)\n        y[0]  = (-self.alpha[0] + self.beta[0]) * self.uleft(t)\n        y[-1] = (-self.alpha[-1] - self.beta[-1]) * self.uright(t)\n        return y\n\n    def interpolate(self, Sval, U):\n        \"\"\"\n        Interpolation lin√©aire pour obtenir la valeur approxim√©e d'un put\n        en un point spot Sval donn√©.\n        \"\"\"\n        if Sval &lt;= self.Smin:\n            return self.uleft(self.T)\n        elif Sval &gt;= self.Smax:\n            return self.uright(self.T)\n        else:\n            return np.interp(Sval, self.s, U)\n\n    # Abstract method\n    @abstractmethod\n    def solve(self):\n        \"\"\"\n        M√©thode abstraite de r√©solution du sch√©ma num√©rique.\n        \"\"\"\n        raise NotImplementedError(\"M√©thode solve() √† impl√©menter dans la classe fille\")\n\n\nclass SchemeEE(SchemeBase):\n    \"\"\"\n    Sch√©ma d'Euler explicite.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Euler Explicite\"\n\n    def solve(self):\n        \"\"\"\n        R√©solution du sch√©ma d'Euler explicite.\n        \"\"\"\n        U = self.phi(self.s)\n\n        for n in range(self.N):\n            t = n * self.dt\n            U = U - self.dt * (self.A @ U + self.q(t))\n\n        return U,t\n\n\nclass SchemeEI(SchemeBase):\n    \"\"\"\n    Sch√©ma d'Euler implicite.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Euler Implicite\"\n\n    def solve(self):\n        U = self.phi(self.s)\n        I = eye(self.J, format=\"csr\")\n\n        for n in range(self.N):\n            t = n * self.dt\n            U = spsolve(I + self.dt * self.A, U - self.dt * self.q(t))\n\n        return U,t\n\n\nclass SchemeCN(SchemeBase):\n    \"\"\"\n    Sch√©ma de Crank-Nicolson.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Crank-Nicolson\"\n\n    def solve(self):\n        U = self.phi(self.s)\n        I = eye(self.J, format=\"csr\")\n        factor_minus = I - 0.5 * self.dt * self.A\n        factor_plus = I + 0.5 * self.dt * self.A\n\n        for n in range(self.N):\n            t = n * self.dt\n            U = spsolve(factor_plus, factor_minus@U - self.dt * self.q(t))\n        return U,t"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#r√©sultat-du-sch√©ma-deuler-explicite",
    "href": "posts/m2mo/edp/edp_european_opt.html#r√©sultat-du-sch√©ma-deuler-explicite",
    "title": "Finite difference Methods for European Options",
    "section": "R√©sultat du sch√©ma d‚ÄôEuler explicite",
    "text": "R√©sultat du sch√©ma d‚ÄôEuler explicite\nPour √©tudier le comportement du sch√©ma d‚ÄôEuler explicite, nous avons r√©alis√© des simulations en faisant varier les param√®tres \\(J\\) (nombre de points spatiaux) et \\(N\\) (nombre de points temporels). Nous avons consid√©r√© deux cas distincts :\n\nCas 1 : \\(N\\) fix√© √† 10 et \\(J\\) variant parmi les valeurs \\(\\{10, 20, 50\\}\\).\nCas 2 : \\(N\\) et \\(J\\) variant simultan√©ment parmi les valeurs \\(\\{10, 20, 50\\}\\), avec \\(N = J\\).\n\nLes r√©sultats obtenus sont pr√©sent√©s dans les graphiques ci-dessous. Par ailleurs, pour aider √† l‚Äôinterpr√©tation des r√©sultats, le r√©sultat de la condition de CFL (Courant-Friedrichs-Lewy) est √©galement affich√© sur chaque graphique.\n\n# Cas 1 : N = 10, J varie dans `values`\n\nvalues = [10, 20, 50]\nN_fixed = 10\n\ncfl_records1 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, J_ in enumerate(values):\n    params['N'] = N_fixed\n    params['J'] = J_\n\n    ee = SchemeEE(**params)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    #CFL condition\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    # Enregistrement dans la table\n    cfl_records1.append({\n        \"N\": N_fixed,\n        \"J\": J_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = {N_fixed}, J = {J_}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du put europ√©en -- Scheme {ee.scheme_name}, N fix√© √† {N_fixed}\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Cas 2 : N = J avec N, J dans `values`\nvalues = [10, 20, 50]\ncfl_records2 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, N_ in enumerate(values):\n    params['N'] = N_\n    params['J'] = N_\n\n    ee = SchemeEE(**params)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    #CFL condition\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    cfl_records2.append({\n        \"N\": N_,\n        \"J\": N_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = J = {N_}, CFL={CFL:.2f}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du put europ√©en -- Scheme {ee.scheme_name}, N = J\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nEn analysant les graphiques ci-dessus, nous constatons que le choix de \\(J\\) et \\(N\\), i.e.¬†le maillage spatial et temporel, influence la stabilit√© de l‚Äôapproximation du prix du put europ√©en.\nDans le cas 1, avec \\(N\\) fix√© √† 10 et \\(J\\) variant, nous constatons que l‚Äôapproximation de prix est tr√®s proche du prix du put europ√©en pour N=J=10, ce qui est attendu pour un sch√©ma stable. Cependant, lorsque \\(J\\) augmente, des oscillations apparaissent dans la solution num√©rique, indiquant une instabilit√© du sch√©ma.\nPar ailleurs, dans le cas 2, o√π \\(N\\) et \\(J\\) varient simultan√©ment avec \\(N=J\\), nous observons √©galement des oscillations et des instabilit√©s pour des valeurs plus √©lev√©es de \\(N\\) et \\(J\\). Ce comportement instable est similaire √† celui observ√© dans le cas 1, lorsque \\(J\\) augmente pour un \\(N\\) fix√©.\nCes oscillations et instabilit√©s observ√©es dans les deux cas √©tudi√©s sont caract√©ristiques des sch√©mas explicites lorsqu‚Äôils ne respectent pas la condition de stabilit√© requise, dite condition CFL. En effet,le sch√©ma d‚Äôeuler explicite est stable conditionnellement au respect de la condition CFL d√©pendant du rapport entre \\(\\Delta t\\) et \\(h^2\\).\nLa condition CFL qui, pour le sch√©ma explicite appliqu√© √† l‚Äô√©quation de Black-Scholes, peut √™tre exprim√©e comme suit : \\(\\frac{\\Delta t}{h^2} \\leq \\frac{1}{2 \\sigma^2 Smax^2}\\).Cette condition impose une relation entre le pas de temps \\(\\Delta t\\) et le pas d‚Äôespace h. De fait, lorsque J augmente et N diminue, cette condition de stabilit√© peut √™tre viol√©e. Dans notre cas, lorsque J=50 et N=10 ou N=50, la condition CFL n‚Äôest pas respect√©e comme on peut l‚Äôobserver dans le tableau ci-dessous, ce qui explique les oscillations et l‚Äôinstabilit√© observ√©es dans les r√©sultats num√©riques.\n\ncfl_df1 = pd.DataFrame(cfl_records1)  # Cas 1 : N fix√©\ncfl_df2 = pd.DataFrame(cfl_records2)  # Cas 2 : N = J\n\ncfl_df1 = cfl_df1.rename(columns={\"CFL\": \"CFL (N=10)\"})\ncfl_df2 = cfl_df2.rename(columns={\"CFL\": \"CFL (N=J)\"})\n\ncfl_table = pd.merge(\n    cfl_df1[[\"J\", \"CFL (N=10)\"]],\n    cfl_df2[[\"J\", \"CFL (N=J)\"]],\n    on=\"J\",\n    how=\"inner\"\n)\n\ncfl_table\n\n\n\n\n\n\n\n\nJ\nCFL (N=10)\nCFL (N=J)\n\n\n\n\n0\n10\n0.484\n0.4840\n\n\n1\n20\n1.764\n0.8820\n\n\n2\n50\n10.404\n2.0808\n\n\n\n\n\n\n\nPour mieux comprendre l‚Äôorigine des oscillations, nous avons √©tudi√© la matrice d‚Äôamplification du sch√©ma explicite pour N=10 et J=50. La matrice d‚Äôamplification est d√©finie par \\(B:= I - dt*A\\) o√π A est la matrice tridiagonale associ√©e au sch√©ma implicite. En calculant les valeurs propres de cette matrice, nous pouvons analyser la stabilit√© du sch√©ma.\nEn effet, cette matrice intervient dans l‚Äô√©volution de la solution num√©rique √† chaque pas de temps, et on a : \\[\n||U^n||_2 &lt;= ||B||^n ||U^0||_2, \\quad \\forall n \\geq 0.\\]\nSi la norme de B est sup√©rieure √† 1, alors les valeurs prises par la solution num√©rique peuvent cro√Ætre de mani√®re exponentielle par rapport √† la condition initiale, conduisant √† des erreurs d‚Äôapproximation possiblement importantes, des oscillations et ainsi une instabilit√© dans la solution num√©rique. Pour savoir si la norme de B est sup√©rieure √† 1, nous avons calcul√© le spectre de B et constat√© que la valeur absolue de la plus grande valeur propre d√©passe 1. Cela confirme que le sch√©ma explicite est instable pour les param√®tres choisis, ce qui explique les oscillations observ√©es dans les r√©sultats num√©riques.\nPour cette raison, nous avons test√© d‚Äôautres sch√©mas num√©riques tels que le sch√©ma d‚ÄôEuler implicite et le sch√©ma de Crank-Nicolson, qui sont connus pour leur stabilit√© inconditionnelle.\n\n# Param√®tres\nparams['N'] = 10\nparams['J'] = 50\n\nee = SchemeEE(**params)\nee.solve()\n\n# Matrice d'amplification\nA = ee.A.toarray()\nB = np.eye(ee.J) - ee.dt * A\n\n# Valeurs propres\neigenvalues = lng.eigvals(B)\nspectral_radius = np.max(np.abs(eigenvalues))\nnorm_B = lng.norm(B, 2)\n\n# Tableau r√©capitulatif\ndf_spectrum = pd.DataFrame({\n    \"Valeur propre\": eigenvalues,\n    \"|Valeur propre|\": np.abs(eigenvalues)\n}).sort_values(\"|Valeur propre|\", ascending=False)\n\n# Affichage\nprint(\"=== Analyse de stabilit√© du sch√©ma explicite ===\\n\")\n\nprint(\"Param√®tres num√©riques :\")\nprint(f\"  N = {ee.N},  J = {ee.J},  dt = {ee.dt:.2f},  h  = {ee.h:.2f}\\n\")\n\nprint(\"10 plus grandes valeurs propres de B (tri√©es par module d√©croissant) :\")\nprint(df_spectrum.head(10).to_string(index=False))\n\nprint(\"Norme matricielle induite (norme 2) :\")\nprint(f\"  ||B||‚ÇÇ = {norm_B:.6f}\\n\")\n\n=== Analyse de stabilit√© du sch√©ma explicite ===\n\nParam√®tres num√©riques :\n  N = 10,  J = 50,  dt = 0.10,  h  = 3.92\n\n10 plus grandes valeurs propres de B (tri√©es par module d√©croissant) :\n Valeur propre  |Valeur propre|\n    -16.469570        16.469570\n    -14.218162        14.218162\n    -12.525602        12.525602\n    -11.138073        11.138073\n     -9.955409         9.955409\n     -8.924546         8.924546\n     -8.012865         8.012865\n     -7.198391         7.198391\n     -6.465369         6.465369\n     -5.801977         5.801977\nNorme matricielle induite (norme 2) :\n  ||B||‚ÇÇ = 16.473976"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#r√©sultat-du-sch√©ma-deuler-implicite-et-de-crank-nicolson",
    "href": "posts/m2mo/edp/edp_european_opt.html#r√©sultat-du-sch√©ma-deuler-implicite-et-de-crank-nicolson",
    "title": "Finite difference Methods for European Options",
    "section": "R√©sultat du sch√©ma d‚ÄôEuler implicite et de Crank-Nicolson",
    "text": "R√©sultat du sch√©ma d‚ÄôEuler implicite et de Crank-Nicolson\n\nCas du sch√©ma d‚ÄôEuler implicite\nPour √©tudier le comportement du sch√©ma d‚ÄôEuler implicite, nous nous sommes directement plac√©s dans le cas o√π N=10 et J=50, qui avait montr√© des oscillations dans le sch√©ma explicite. Les r√©sultats obtenus sont pr√©sent√©s dans le graphique ci-dessous. Comme on peut le constater, le sch√©ma d‚ÄôEuler implicite produit une approximation stable et sans oscillations du prix du put europ√©en, m√™me pour des valeurs √©lev√©es de N et J. Cela confirme la stabilit√© inconditionnelle du sch√©ma implicite, qui ne d√©pend pas de la relation entre le pas de temps et le pas spot.\n\nparams['J'] = 50\nparams['N'] = 10\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.20 K=100 T=1\nParam√®tres num√©riques:\nJ=50 N=10\n\n\n\nei = SchemeEI(**params)\nU,t = ei.solve()\ns = ei.s\ndt = ei.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t+dt))\nplt.plot(s,ei.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put europ√©en au cours du temps [Euler Implicite]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCas du sch√©ma de Crank-Nicolson\nLe sch√©ma de Crank-Nicolson, quant √† lui, est une m√©thode implicite qui combine les avantages des sch√©mas explicites et implicites. Il est bas√© sur une moyenne pond√©r√©e des valeurs aux temps n et n+1, ce qui permet d‚Äôobtenir une meilleure pr√©cision temporelle tout en maintenant la stabilit√©. Lors de l‚Äô√©tude du sch√©ma de Crank-Nicolson, nous avons √©galement choisi les param√®tres N=10 et J=50. Les r√©sultats obtenus sont pr√©sent√©s dans le graphique ci-dessous. Comme on peut le constater, le sch√©ma de Crank-Nicolson produit √©galement une approximation stable et sans oscillations du prix du put europ√©en, confirmant ainsi sa stabilit√© inconditionnelle.\n\nparams['J'] = 50\nparams['N'] = 10\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.20 K=100 T=1\nParam√®tres num√©riques:\nJ=50 N=10\n\n\n\ncn = SchemeCN(**params)\nU,t = cn.solve()\ns = cn.s\ndt = cn.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t+dt))\nplt.plot(s,cn.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put europ√©en au cours du temps [Crank-Nicolson]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#m√©thodes",
    "href": "posts/m2mo/edp/edp_european_opt.html#m√©thodes",
    "title": "Finite difference Methods for European Options",
    "section": "M√©thodes",
    "text": "M√©thodes\nAfin de comparer les r√©sultats num√©riques obtenus avec les diff√©rents \\(\\theta-\\)sch√©mas au prix th√©orique du put europ√©en, nous avons impl√©ment√© la formule de Black and Scholes dans une fonction Python. Cette fonction calcule le prix du put en utilisant les param√®tres financiers tels que le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice, le taux d‚Äôint√©r√™t sans risque, la volatilit√© et le temps jusqu‚Äô√† l‚Äô√©ch√©ance.\nNous analysons la convergence des sch√©mas num√©riques consid√©r√©s, ainsi que les ordres de convergence. La convergence est ensuite √©tudi√©e num√©riquement en comparant les prix obtenus par les sch√©mas avec la solution analytique du put europ√©en, et les ordres de convergence sont estim√©s √† partir du comportement asymptotique de l‚Äôerreur lorsque les pas de temps et d‚Äôespace tendent vers z√©ro.\nPour estimer num√©riquement l‚Äôordre de convergence des sch√©mas, on utilise la relation \\(e_k \\sim C h_k^\\alpha\\), o√π \\(C\\) est une constante ind√©pendante de \\(h_k\\) et \\(\\alpha\\) est l‚Äôordre de convergence du sch√©ma, \\(h_k\\) √©tant le pas de discr√©tisation spatial, d√©fini par \\(h_k = \\frac{S_{max} - S_{min}}{J_k + 1}\\), et \\(J_k\\) le nombre de points spatiaux utilis√©s dans la discr√©tisation. De fait, on obtient l‚Äôordre de convergence \\(\\alpha\\) en comparant les erreurs \\(e_k\\) et \\(e_{k+1}\\) pour deux maillages successifs. D‚Äôo√π \\[\\alpha_k\n= \\frac{\\log\\!\\left(\\frac{e_{k+1}}{e_k}\\right)}\n       {\\log\\!\\left(\\frac{h_{k+1}}{h_k}\\right)}.\n\\]\nNous consid√©rons deux grilles de discr√©tisation successives d√©finies par les couples \\((N_k, J_k)\\), √† savoir \\(N=J\\) et \\(N = \\frac{J^2}{10}\\), avec \\(J \\in \\{10, 20, 40, 80, 160\\}\\).\n\nprint(\"Param√®tres financiers:\")\nSval = 80\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_, \"Sval=%.2f\" %Sval)\n\nParam√®tres financiers:\nr=0.10 sigma=0.20 K=100 T=1 Sval=80.00\n\n\n\n# Implementation de la formule de Black and Scholes\ndef dplus(x,K,r,sigma,T,t):\n    tau = T-t\n    numerator = np.log(x/K) + tau * (r + 0.5*sigma**2)\n    denominator = sigma * np.sqrt(tau)\n    return numerator / denominator\n\ndef dmoins(dplus_,sigma,tau):\n    return dplus_ - sigma * np.sqrt(tau)\n\ndef compute_BS_price(x,K,r,sigma,T,t=0,type_='call'):\n    tau = T-t\n    dplus_ = dplus(x,K,r,sigma,T,t)\n    dmoins_ = dmoins(dplus_,sigma,tau)\n    if type_ == 'call':\n        return x*stats.norm.cdf(dplus_) - K * np.exp(-r*tau) * stats.norm.cdf(dmoins_)\n    elif type_=='put':\n        return K* np.exp(-r*tau)* stats.norm.cdf(-dmoins_) - x * stats.norm.cdf(-dplus_)\n\n\nprice_BS = compute_BS_price(Sval, params['K'], params['r'], params['sigma'], params['T'], t=0, type_='put')\nprint(\"Prix th√©orique du put pour S=%.2f : %.4f\" %(Sval, price_BS))\n\nPrix th√©orique du put pour S=80.00 : 13.2737\n\n\n\ndef get_convergence_table(N_grid, J_grid, params, Sval, scheme_class):\n\n    est_prices = []\n    errex = []\n    errors = []\n    cpu_times = []\n\n    # Prix exact (identique pour tous)\n    BS_price = compute_BS_price(\n        Sval, params['K'], params['r'],\n        params['sigma'], params['T'],\n        t=0, type_='put'\n    )\n\n    for N, J in zip(N_grid, J_grid):\n        params['N'] = N\n        params['J'] = J\n\n        start = time.time()\n        scheme = scheme_class(**params)\n        U, _ = scheme.solve()\n        tcpu = time.time() - start\n\n        price_est = scheme.interpolate(Sval, U)\n\n        est_prices.append(price_est)\n        errex.append(abs(price_est - BS_price))\n        cpu_times.append(tcpu)\n\n    est_prices = np.array(est_prices)\n    errors = np.zeros(len(est_prices))\n    errors[1:] = np.abs(np.diff(est_prices))\n    errex = np.array(errex)\n    cpu_times = np.array(cpu_times)\n\n    # Ordre de convergence global\n    alpha = np.zeros(len(errors))\n    h_step = (params[\"Smax\"] - params[\"Smin\"]) / (J_grid + 1)\n    alpha[1:] = np.log(errex[:-1] / errex[1:]) / np.log(h_step[:-1] / h_step[1:])\n\n    df = pd.DataFrame({\n        \"J\": J_grid,\n        \"N\": N_grid,\n        \"U(s)\": est_prices,\n        \"error\": errors,\n        \"alpha\": alpha,\n        \"errex\": errex,\n        \"tcpu\": cpu_times\n    })\n\n    return df.round(6)"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#r√©sultats",
    "href": "posts/m2mo/edp/edp_european_opt.html#r√©sultats",
    "title": "Finite difference Methods for European Options",
    "section": "R√©sultats",
    "text": "R√©sultats\n\nOn constate que les trois sch√©mas pr√©sentent des comportements tr√®s contrast√©s en termes de stabilit√©, ordre de convergence et co√ªt de calcul lorsque l‚Äôon impose \\(N = J\\).\nOn observe tout d‚Äôabord que le sch√©ma d‚ÄôEuler explicite ne peut pas √™tre consid√©r√© comme fiable dans ce cadre, comme pr√©cis√© pr√©c√©demment. Pour des maillages grossiers, l‚Äôerreur semble initialement d√©cro√Ætre et l‚Äôordre estim√© est proche de 2, ce qui pourrait sugg√©rer une bonne convergence. Cependant, d√®s que la discr√©tisation devient plus fine, la solution diverge et on observe une explosion num√©rique √† partir de \\(N = J = 80\\) qui montre clairement que la condition de stabilit√© n‚Äôest plus satisfaite lorsque \\(\\Delta t\\) et \\(\\Delta s\\) sont raffin√©s simultan√©ment.\nLe sch√©ma d‚ÄôEuler implicite est parfaitement stable sur toute la plage de discr√©tisations consid√©r√©e. L‚Äôerreur d√©cro√Æt de fa√ßon r√©guli√®re lorsque \\(N = J\\) augmente, sans aucune instabilit√© num√©rique. De plus, l‚Äôordre de convergence estim√© reste inf√©rieur √† 2 et tend progressivement vers une valeur proche de 1, ce qui est coh√©rent avec le fait que, lorsque \\(\\Delta t \\sim \\Delta s\\), l‚Äôerreur temporelle d‚Äôordre 1 domine l‚Äôerreur spatiale.\nEnfin, on constate que le sch√©ma de Crank‚ÄìNicolson pr√©sente le comportement le plus satisfaisant. Il est stable sur toute la grille, l‚Äôerreur d√©cro√Æt rapidement et l‚Äôordre de convergence estim√© est tr√®s proche de 2 d√®s les maillages interm√©diaires, puis se stabilise autour de cette valeur pour les maillages fins. Ce r√©sultat est parfaitement conforme √† la th√©orie, puisque le sch√©ma est d‚Äôordre 2 √† la fois en temps et en espace.\nPour conclure, lorsque \\(N = J\\), en termes de temps de calcul, le sch√©ma EE est certes le plus rapide sur les grilles grossi√®res, avec des temps de simulation de l‚Äôordre de la milliseconde, mais cet avantage est sans int√©r√™t pratique puisque la solution devient totalement inutilisable pour des maillages fins. Le sch√©ma implicite constitue une solution robuste mais relativement co√ªteuse et moins pr√©cise. En effet, il n√©cessite la r√©solution d‚Äôun syst√®me lin√©aire √† chaque pas de temps. Cela se refl√®te clairement dans les temps de calcul, qui augmentent sensiblement avec le raffinement et deviennent significatifs pour les maillages fins, atteignant plus de 0.1 seconde pour \\(N = J = 160\\).\nLe sch√©ma de Crank‚ÄìNicolson appara√Æt comme le meilleur compromis, combinant stabilit√©, convergence d‚Äôordre 2 et temps de calcul raisonnables. En termes de co√ªt de calcul, le sch√©ma CN est plus cher que le sch√©ma explicite mais reste nettement plus efficace que le sch√©ma implicite pur. Les temps de simulation restent mod√©r√©s, m√™me pour des valeurs √©lev√©es de \\(N = J\\), et demeurent largement compatibles avec une utilisation pratique, tout en offrant une pr√©cision bien sup√©rieure.\n\nN_grid = J_grid = np.array([10, 20, 40, 80, 160])\nprint(\"Cas N = J\\n\", \"=\"*75)\n\nprint(\"Convergence Table for Scheme EE:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeEE))\n\nprint(\"=\"*75, \"\\nConvergence Table for Scheme EI:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeEI))\n\nprint(\"=\"*75, \"\\nConvergence Table for Scheme CN:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeCN))\n\nCas N = J\n ===========================================================================\nConvergence Table for Scheme EE:\n     J    N          U(s)         error       alpha         errex      tcpu\n0   10   10  1.425509e+01  0.000000e+00    0.000000  9.814290e-01  0.000212\n1   20   20  1.351532e+01  7.397760e-01    2.167407  2.416540e-01  0.000158\n2   40   40  1.332033e+01  1.949870e-01    2.457937  4.666600e-02  0.000257\n3   80   80 -1.061474e+04  1.062806e+04  -18.117778  1.062801e+04  0.000558\n4  160  160 -8.121408e+69  8.121408e+69 -220.831903  8.121408e+69  0.001096\n=========================================================================== \nConvergence Table for Scheme EI:\n     J    N       U(s)     error     alpha     errex      tcpu\n0   10   10  14.448406  0.000000  0.000000  1.174743  0.000530\n1   20   20  13.642159  0.806247  1.792956  0.368496  0.000878\n2   40   40  13.386145  0.256015  1.773621  0.112482  0.001978\n3   80   80  13.310502  0.075642  1.639391  0.036839  0.003961\n4  160  160  13.287066  0.023436  1.471804  0.013403  0.009982\n=========================================================================== \nConvergence Table for Scheme CN:\n     J    N       U(s)     error     alpha     errex      tcpu\n0   10   10  14.353451  0.000000  0.000000  1.079788  0.000380\n1   20   20  13.578892  0.774559  1.953920  0.305229  0.000576\n2   40   40  13.353140  0.225752  2.011209  0.079477  0.001089\n3   80   80  13.293944  0.059195  2.005878  0.020281  0.002566\n4  160  160  13.278788  0.015156  2.002416  0.005125  0.006933\n\n\n\nOn constate que le choix \\(N = J^2/10\\) modifie l‚Äôanalyse par rapport au cas \\(N = J\\), en particulier pour le sch√©ma explicite. Alors que ce dernier √©tait instable lorsque les pas de temps et d‚Äôespace √©taient raffin√©s simultan√©ment, il devient ici parfaitement stable. L‚Äôerreur d√©cro√Æt r√©guli√®rement et l‚Äôordre de convergence est tr√®s proche de 2 sur l‚Äôensemble des maillages, ce qui montre que la condition de stabilit√© est d√©sormais respect√©e et que l‚Äôerreur spatiale domine. Le sch√©ma EE retrouve ainsi une convergence th√©oriquement optimale, tout en conservant un co√ªt de calcul globalement inf√©rieur √† celui des sch√©mas implicites.\nLe sch√©ma d‚ÄôEuler implicite reste, comme pr√©c√©demment, inconditionnellement stable. Cependant, contrairement au cas \\(N = J\\), l‚Äôordre de convergence n‚Äôest plus limit√© par l‚Äôerreur temporelle et tend lui aussi vers 2 lorsque \\(J\\) augmente. Cette am√©lioration de la pr√©cision se fait au prix d‚Äôun co√ªt de calcul tr√®s √©lev√©, les temps de simulation augmentant fortement avec \\(N\\), ce qui rend le sch√©ma peu comp√©titif pour des grilles fines.\nLe sch√©ma de Crank‚ÄìNicolson conserve un comportement tr√®s r√©gulier et conforme √† la th√©orie. Il est stable, pr√©sente une convergence d‚Äôordre 2 et offre des erreurs comparables √† celles du sch√©ma explicite stabilis√©. En revanche, son co√ªt de calcul augmente sensiblement avec le raffinement et devient proche de celui du sch√©ma implicite pour les maillages les plus fins.\nAinsi, contrairement au cas \\(N = J\\) o√π Crank‚ÄìNicolson apparaissait clairement comme le meilleur compromis, le r√©gime \\(N = J^2/10\\) met en √©vidence l‚Äôefficacit√© du sch√©ma explicite, qui combine ici stabilit√©, convergence d‚Äôordre 2 et temps de calcul plus faibles, √† condition d‚Äôaccepter une contrainte forte sur le pas de temps.\n\nN_grid = np.array([(j**2)/10 for j in J_grid]).astype(int)\n\nprint(\"Cas N = J^2/10\\n\", \"=\"*75)\nprint(\"Convergence Table for Scheme EE:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeEE))\n\nprint(\"=\"*75, \"\\nConvergence Table for Scheme EI:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeEI))\n\nprint(\"=\"*75, \"\\nConvergence Table for Scheme CN:\")\nprint(get_convergence_table(N_grid, J_grid, params, Sval, SchemeCN))\n\nCas N = J^2/10\n ===========================================================================\nConvergence Table for Scheme EE:\n     J     N       U(s)     error     alpha     errex      tcpu\n0   10    10  14.255092  0.000000  0.000000  0.981429  0.000155\n1   20    40  13.547634  0.707459  1.973300  0.273971  0.000214\n2   40   160  13.345106  0.202528  2.009007  0.071443  0.000719\n3   80   640  13.291930  0.053175  2.002965  0.018267  0.002883\n4  160  2560  13.278284  0.013646  2.000790  0.004621  0.010337\n=========================================================================== \nConvergence Table for Scheme EI:\n     J     N       U(s)     error     alpha     errex      tcpu\n0   10    10  14.448406  0.000000  0.000000  1.174743  0.000565\n1   20    40  13.611061  0.837345  1.929307  0.337398  0.001650\n2   40   160  13.361562  0.249499  2.010421  0.087899  0.006941\n3   80   640  13.296062  0.065500  2.007975  0.022399  0.031444\n4  160  2560  13.279318  0.016744  2.003733  0.005655  0.157797\n=========================================================================== \nConvergence Table for Scheme CN:\n     J     N       U(s)     error     alpha     errex      tcpu\n0   10    10  14.353451  0.000000  0.000000  1.079788  0.000538\n1   20    40  13.579386  0.774065  1.951418  0.305723  0.001003\n2   40   160  13.353328  0.226058  2.010092  0.079665  0.003859\n3   80   640  13.293996  0.059332  2.005652  0.020333  0.019154\n4  160  2560  13.278801  0.015195  2.002383  0.005138  0.106421"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#choix-dimpl√©mentations-1",
    "href": "posts/m2mo/edp/edp_european_opt.html#choix-dimpl√©mentations-1",
    "title": "Finite difference Methods for European Options",
    "section": "Choix d‚Äôimpl√©mentations",
    "text": "Choix d‚Äôimpl√©mentations\nL‚Äôimpl√©mentation des sch√©mas num√©riques est rest√©e globalement inchang√©e par rapport au cas du put europ√©en. La m√™me classe de base SchemeBase est utilis√©e pour d√©finir la grille de discr√©tisation et les op√©rateurs num√©riques, tandis que seules la condition initiale, correspondant au payoff du call europ√©en, ainsi que les conditions aux limites ont √©t√© adapt√©es. Les sch√©mas d‚ÄôEuler explicite, d‚ÄôEuler implicite et de Crank‚ÄìNicolson sont impl√©ment√©s de mani√®re identique via les classes SchemeEE, SchemeEI et SchemeCN.\n\nclass SchemeBase(ABC):\n    \"\"\"\n    Classe de base pour les sch√©mas num√©riques de l'√©quation de Black-Scholes.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        # Financial parameters\n        self.r = r\n        self.sigma = sigma\n        self.K = K\n        self.T = T\n\n        # Numerical parameters\n        self.N = N\n        self.J = J\n        self.Smin = Smin\n        self.Smax = Smax\n\n        # Grids\n        self.dt = T / N\n        self.h = (Smax - Smin) / (J + 1)\n        self.s = Smin + self.h * np.arange(1, J + 1)\n\n        # Operator\n        self.A, self.alpha, self.beta = self._build_matrix_A()\n\n    def phi(self, s):\n        \"\"\"\n        Condition initiale (payoff) pour un call europ√©en.\n        ùúô(s) = max(s - K, 0)\n        \"\"\"\n        return np.maximum(s-self.K, 0)\n\n    def uleft(self, t):\n        \"\"\"\n        Condition aux limites √† gauche pour un call europ√©en.\n        \"\"\"\n        return 0.0\n\n    def uright(self, t):\n        \"\"\"\n        Condition aux limites √† droite pour un put europ√©en.\n        \"\"\"\n        return self.Smax - self.K * np.exp(-self.r * t)\n\n    def _build_matrix_A(self):\n        \"\"\"\n        Matrice d'amplification A.\n        \"\"\"\n        alpha = 0.5 * self.sigma**2 * (self.s**2 / self.h**2)\n        beta  = self.r * self.s / (2 * self.h)\n\n        lower = -alpha[1:] + beta[1:]        # sous-diagonale\n        main  = 2 * alpha + self.r            # diagonale principale\n        upper = -alpha[:-1] - beta[:-1]       # sur-diagonale\n\n        A = diags(\n            diagonals=[lower, main, upper],\n            offsets=[-1, 0, 1],\n            shape=(self.J, self.J),\n            format=\"csr\"\n        )\n\n        return A, alpha, beta\n\n    def q(self, t):\n        \"\"\"\n        Vecteur des conditions aux limites.\n        \"\"\"\n        y = np.zeros(self.J)\n        y[0]  = (-self.alpha[0] + self.beta[0]) * self.uleft(t)\n        y[-1] = (-self.alpha[-1] - self.beta[-1]) * self.uright(t)\n        return y\n\n    def interpolate(self, Sval, U):\n        \"\"\"\n        Interpolation lin√©aire pour obtenir la valeur approxim√©e d'un put\n        en un point spot Sval donn√©.\n        \"\"\"\n        if Sval &lt;= self.Smin:\n            return self.uleft(self.T)\n        elif Sval &gt;= self.Smax:\n            return self.uright(self.T)\n        else:\n            return np.interp(Sval, self.s, U)\n\n    # Abstract method\n    @abstractmethod\n    def solve(self):\n        \"\"\"\n        M√©thode abstraite de r√©solution du sch√©ma num√©rique.\n        \"\"\"\n        raise NotImplementedError(\"M√©thode solve() √† impl√©menter dans la classe fille\")\n\n\nclass SchemeEE(SchemeBase):\n    \"\"\"\n    Sch√©ma d'Euler explicite.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Euler Explicite\"\n\n    def solve(self):\n        \"\"\"\n        R√©solution du sch√©ma d'Euler explicite.\n        \"\"\"\n        U = self.phi(self.s)\n\n        for n in range(self.N):\n            t = n * self.dt\n            U = U - self.dt * (self.A @ U + self.q(t))\n\n        return U,t\n\n\nclass SchemeEI(SchemeBase):\n    \"\"\"\n    Sch√©ma d'Euler implicite.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Euler Implicite\"\n\n    def solve(self):\n        U = self.phi(self.s)\n        I = eye(self.J, format=\"csr\")\n\n        for n in range(self.N):\n            t = n * self.dt\n            U = spsolve(I + self.dt * self.A, U - self.dt * self.q(t))\n\n        return U,t\n\n\nclass SchemeCN(SchemeBase):\n    \"\"\"\n    Sch√©ma de Crank-Nicolson.\n    \"\"\"\n    def __init__(self, r, sigma, K, T, N, J, Smin, Smax):\n        super().__init__(r, sigma, K, T, N, J, Smin, Smax)\n        self.scheme_name = \"Crank-Nicolson\"\n\n    def solve(self):\n        U = self.phi(self.s)\n        I = eye(self.J, format=\"csr\")\n        factor_minus = I - 0.5 * self.dt * self.A\n        factor_plus = I + 0.5 * self.dt * self.A\n\n        for n in range(self.N):\n            t = n * self.dt\n\n            U = spsolve(factor_plus, factor_minus@U - self.dt * self.q(t))\n        return U,t"
  },
  {
    "objectID": "posts/m2mo/edp/edp_european_opt.html#r√©sultats-des-sch√©mas-num√©riques-1",
    "href": "posts/m2mo/edp/edp_european_opt.html#r√©sultats-des-sch√©mas-num√©riques-1",
    "title": "Finite difference Methods for European Options",
    "section": "R√©sultats des sch√©mas num√©riques",
    "text": "R√©sultats des sch√©mas num√©riques\n\nCas du sch√©ma d‚ÄôEuler explicite\nLes r√©sultats pr√©c√©dents ont montr√© que le sch√©ma d‚ÄôEuler explicite est conditionnellement stable en fonction de \\(J\\), \\(N\\) et de la condition CFL. Ici, nous l‚Äôappliquons √† un call europ√©en en utilisant les m√™mes configurations que pour le put : - (1) \\(N = 10\\) et \\(J \\in {10, 20, 50}\\), - (2) \\(N = J \\in {10, 20, 50}\\).\nLes r√©sultats, accompagn√©s de l‚Äôindication de la condition CFL, sont pr√©sent√©s dans les graphiques ci‚Äëdessous.\n\n# Cas 1 : N = 10, J varie dans `values` pour le call europ√©en\nvalues = [10, 20, 50]\nN_fixed = 10\ncfl_records1 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, J_ in enumerate(values):\n    params['N'] = N_fixed\n    params['J'] = J_\n\n    ee = SchemeEE(**params)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    # Condition CFL\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    # Enregistrement\n    cfl_records1.append({\n        \"N\": N_fixed,\n        \"J\": J_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = {N_fixed}, J = {J_}, CFL={CFL:.2f}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du call europ√©en -- Scheme {ee.scheme_name}, N fix√© √† {N_fixed}\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n#Cas 2 : N = J avec N, J dans `values` pour le call europ√©en\ncfl_records2 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, N_ in enumerate(values):\n    params['N'] = N_\n    params['J'] = N_\n\n    ee = SchemeEE(**params)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    # Condition CFL\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    cfl_records2.append({\n        \"N\": N_,\n        \"J\": N_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = J = {N_}, CFL={CFL:.2f}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du call europ√©en -- Scheme {ee.scheme_name}, N = J\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nComme pour le put, le sch√©ma d‚ÄôEuler explicite pour le call est stable pour de petites valeurs de \\(J\\) et \\(N\\), et devient instable lorsque la condition CFL n‚Äôest pas respect√©e, entra√Ænant des oscillations.\n\n\nCas des sch√©mas d‚ÄôEuler implicite et de Crank-Nicolson\nLes sch√©mas d‚ÄôEuler implicite et de Crank-Nikolson, appliqu√©s au cas du call europ√©en, demeurent inconditionnellement stables. Ils sont test√©s sur la configuration N= 10 et J= 50, qui avait conduit √† des instabilit√©s et √† des oscillations pour le schema explicite. Les r√©sultats obtenus montrent une approximation stable et sans oscillations du prix du call.\n\nparams['J'] = 50\nparams['N'] = 10\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.20 K=100 T=1\nParam√®tres num√©riques:\nJ=50 N=10\n\n\n\nei = SchemeEI(**params)\nU,t = ei.solve()\ns = ei.s\ndt = ei.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t+dt))\nplt.plot(s,ei.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du call europ√©en au cours du temps [Euler Implicite]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ncn = SchemeCN(**params)\nU,t = cn.solve()\ns = cn.s\ndt = cn.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t+dt))\nplt.plot(s,cn.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du call europ√©en au cours du temps [Crank-Nicolson]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/m2mo/derivatives/01_intro_market.html",
    "href": "posts/m2mo/derivatives/01_intro_market.html",
    "title": "Mathematical foundations of financial market modeling",
    "section": "",
    "text": "A financial market is an institutional framework in which financial instruments are issued and exchanged. In the primary market, newly created securities such as shares or bonds are sold for the first time by firms or governments to investors. The objective of this article is to introduce the economic role of financial markets and to present the fundamental structural properties required for coherent financial modeling.\nFor theoretical clarity, we adopt a discrete-time framework. Time is indexed by \\[\nt \\in \\{0,1,\\dots,T\\},\n\\] where \\(T\\) denotes a fixed time horizon."
  },
  {
    "objectID": "posts/m2mo/derivatives/01_intro_market.html#structural-properties-of-a-financial-market",
    "href": "posts/m2mo/derivatives/01_intro_market.html#structural-properties-of-a-financial-market",
    "title": "Mathematical foundations of financial market modeling",
    "section": "Structural properties of a financial market",
    "text": "Structural properties of a financial market\nFor a financial market to function efficiently, several properties are desirable. Liquidity ensures that assets can be traded without significant price impact. Transparency guarantees that relevant information is accessible to participants. Fairness implies equal access to market information. Efficiency requires that prices reflect available information and that arbitrage opportunities do not persist.\nThe absence of arbitrage is a central assumption in modern financial mathematics and underlies pricing theory."
  },
  {
    "objectID": "posts/m2mo/derivatives/01_intro_market.html#trading-strategies-and-wealth-process",
    "href": "posts/m2mo/derivatives/01_intro_market.html#trading-strategies-and-wealth-process",
    "title": "Mathematical foundations of financial market modeling",
    "section": "Trading strategies and wealth process",
    "text": "Trading strategies and wealth process\nA trading strategy is a predictable process \\[\n\\delta_t = (\\delta_t^0, \\delta_t^1, \\dots, \\delta_t^q),\n\\] where \\(\\delta_t^i\\) represents the number of units of asset \\(i\\) held at time \\(t\\).\nThe associated wealth process is defined by \\[\nV_t = \\sum_{i=0}^q \\delta_t^i S_t^i.\n\\]\nA strategy is said to be self-financing if changes in wealth are solely due to variations in asset prices. In discrete time, this condition reads \\[\nV_{t+1} - V_t = \\delta_t^0 \\, dS_t^0  + \\sum_{i=1}^q \\delta_t^i (S_{t+1}^i - S_t^i).\n\\] Equivalently, \\[\nV_t = V_0 + \\delta_t^0 \\, dS_t^0  + \\sum_{s=1}^{t-1} \\sum_{i=0}^q \\delta_s^i (S_{s+1}^i - S_s^i).\n\\]\n\n\n\n\n\n\nSelf financing condition in continuous time\n\n\n\nIn continuous time, the self-financing condition is expressed as \\[\n\\begin{align}\ndV_t &= \\delta_t^0 \\, dS_t^0  + \\sum_{i=1}^q \\delta_t^i \\, dS_t^i. \\\\\n\\Leftrightarrow\nV_t &= V_0 + \\delta_t^0 \\, dS_t^0  + \\int_1^t \\sum_{i=0}^q \\delta_s^i \\, dS_s^i.\n\\end{align}\n\\]\nIn the case where the risky assets give dividends, the self-financing condition must be modified to account for dividend payments. Let \\(D_t^i\\) denote the cumulative dividends paid by asset \\(i\\) up to time \\(t\\). The wealth process then satisfies : \\[\ndV_t = \\delta_t^0 \\, dS_t^0  + \\sum_{i=1}^q \\delta_t^i \\, dS_t^i + \\sum_{i=1}^q \\delta_t^i \\, dD_t^i.\n\\]\nOne can talk in term of discounted wealth process \\(\\tilde V_t = \\frac{V_t}{S_t^0}\\), which satisfies \\[\nd\\tilde V_t = \\sum_{i=1}^q \\delta_t^i \\, d\\tilde S_t^i,\n\\] where \\(\\tilde S_t^i = \\frac{S_t^i}{S_t^0} = \\beta_t S_t^i\\) is the discounted price of the risky asset \\(i\\)."
  },
  {
    "objectID": "posts/m2mo/derivatives/01_intro_market.html#risk-neutral-measure",
    "href": "posts/m2mo/derivatives/01_intro_market.html#risk-neutral-measure",
    "title": "Mathematical foundations of financial market modeling",
    "section": "Risk-neutral measure",
    "text": "Risk-neutral measure\nUnder the absence of arbitrage, there exists an equivalent probability measure \\(\\mathcal{Q}\\) under which discounted asset prices are martingales. Let \\[\n\\tilde S_t^i = \\frac{S_t^i}{S_t^0}\n\\] denote the discounted price process. Then \\[\n\\mathbb{E}^{\\mathcal{Q}}[\\tilde S_{t+1}^i \\mid \\mathcal{F}_t] = \\tilde S_t^i.\n\\] This result allows derivative pricing via expectation: \\[\n\\Pi_0 = S_0^0 \\, \\mathbb{E}^{\\mathcal{Q}}\\left[\\frac{\\varepsilon}{S_T^0}\\right],\n\\] where \\(\\varepsilon\\) denotes the contingent claim payoff at maturity."
  },
  {
    "objectID": "posts/m2mo/derivatives/01_intro_market.html#market-completeness",
    "href": "posts/m2mo/derivatives/01_intro_market.html#market-completeness",
    "title": "Mathematical foundations of financial market modeling",
    "section": "Market completeness",
    "text": "Market completeness\nThe market is said to be complete if every contingent claim \\(\\varepsilon\\) can be replicated by a self-financing strategy. In a complete market, the risk-neutral measure is unique and every derivative admits a unique arbitrage-free price. In an incomplete market, multiple risk-neutral measures may exist and perfect replication is not always possible, leading to pricing and hedging challenges.\nAs an illustration, the payoff of a European call option with strike \\(K\\) and maturity \\(T\\) is given by \\[\n\\varepsilon = \\max(S_T - K, 0).\n\\]\n\n\n\n\n\n\nTake away\n\n\n\nFinancial markets play a fundamental role in capital allocation, risk transfer, and price discovery within the economy. From a mathematical perspective, their analysis relies on a structured framework built upon trading strategies, wealth dynamics, and probabilistic modeling.\nThe absence of arbitrage constitutes the cornerstone of modern financial theory. It guarantees the existence of a risk-neutral measure under which discounted asset prices behave as martingales, thereby providing a coherent foundation for derivative pricing.\nFinally, the notion of market completeness determines whether contingent claims can be perfectly replicated and uniquely priced. These structural concepts form the basis of quantitative finance and underpin the development of more advanced pricing and hedging models."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp4.html",
    "href": "posts/ensai/apprentisage-stat/Tp4.html",
    "title": "Features selection",
    "section": "",
    "text": "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features.\nThe usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\nIn this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n\\[I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)\\]\nwhere \\(p(x,y)\\) is the joint probability distribution function of X and Y, and \\(p(x)\\) and \\(p(y)\\) are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\nIn order to have confidence in this measure, we will consider a bivariate gaussian variable \\(Z=(X,Y)\\) with mean \\(\\mu = (0,0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}\\). We will compute the mutual information between X and Y for a grid a \\(\\rho\\) between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to \\(-\\frac{1}{2} \\log(1-\\rho^2)\\).\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset.\n\n\n\nWe will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n\n\n\n\nWe can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  1  3  5  9 12]"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "href": "posts/ensai/apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "title": "Features selection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "href": "posts/ensai/apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "title": "Features selection",
    "section": "",
    "text": "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "href": "posts/ensai/apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "title": "Features selection",
    "section": "",
    "text": "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "href": "posts/ensai/apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "title": "Features selection",
    "section": "",
    "text": "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  1  3  5  9 12]"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html",
    "href": "posts/ensai/apprentisage-stat/Tp2.html",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear.\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable. In particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. The optimization problem of KRR is given by: \\[\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n\\] where \\(x_i\\) is the \\(i\\)-th row of the matrix \\(X\\) and \\(y_i\\) is the \\(i\\)-th element of the vector \\(y\\). The parameter \\(\\lambda\\) is the regularization parameter. The solution of the optimization problem is given by:\n\\[\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n\\]\nwhere \\(I_d\\) and \\(I_n\\) are the identity matrix.\nIn prediction, the target variable is given by: \\[\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n\\] where \\(\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}\\). We easily see that the prediction is a linear combination of the scalar product between the test point \\(x^*\\) and the training points \\(x_i\\), we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\\[\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n\\]\nwhere \\(K(x_i, x^*)\\) is the kernel function.\n\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Documents/Repositories/website/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': np.float64(0.07196856730011521), 'gamma': np.float64(35.564803062231285)}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by \\(\\tilde{x}=\\frac{\\sqrt(2)}{\\pi}\\cos(\\pi x)\\) and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': np.float64(0.07196856730011521)}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "href": "posts/ensai/apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Documents/Repositories/website/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': np.float64(0.07196856730011521), 'gamma': np.float64(35.564803062231285)}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "href": "posts/ensai/apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Now we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by \\(\\tilde{x}=\\frac{\\sqrt(2)}{\\pi}\\cos(\\pi x)\\) and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html#iii.-insigths",
    "href": "posts/ensai/apprentisage-stat/Tp2.html#iii.-insigths",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Linear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': np.float64(0.07196856730011521)}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html#i.-toy-dataset",
    "href": "posts/ensai/apprentisage-stat/Tp2.html#i.-toy-dataset",
    "title": "Kernel Trick and SVM",
    "section": "I. Toy dataset",
    "text": "I. Toy dataset\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n\nText(0.5, 1.0, 'Two moons dataset')\n\n\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n\n\n1. K-nearest neighbors\n\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nBest parameters by CV : {'n_neighbors': np.int64(1)}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Logistic regression\n\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n\n\n\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. SVM\n\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': np.float64(0.030888435964774818), 'gamma': np.float64(3.727593720314938)}\n\n\n\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Conclusion\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\nWhenever we have a classification problem, it is hence always useful to try the SVM and the KNN."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp2.html#ii.-image-dataset",
    "href": "posts/ensai/apprentisage-stat/Tp2.html#ii.-image-dataset",
    "title": "Kernel Trick and SVM",
    "section": "II. Image dataset",
    "text": "II. Image dataset\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255). We are going to use the SVM with the gaussian kernel to classify the images.\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n\nDescription of train dataset : \n\n\n\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n\n\n\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n\n\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n\n\n\n\n\n\n\n\n\n\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n\n\n\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_heston.html",
    "href": "posts/ensai/proc_stochastique/modele_heston.html",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "",
    "text": "Le but de ce TP est de calculer des prix d‚Äôoptions sous le mod√®le d‚ÄôHeston puis de calibrer ce mod√®le par filtrage. On consid√®re le mod√®le suivant :\n\\[\n\\begin{cases}\ndS_s = S_s \\left( rds + \\sqrt{v_s} dW_s^1 \\right) \\\\\ndv_s = \\kappa (\\beta - v_s) ds + \\sigma \\sqrt{v_s} dW_s^2 \\\\\ndW_s^1 dW_s^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\no√π \\(W_s^1\\) et \\(W_s^2\\) sont deux mouvements browniens et \\(r\\) est le taux sans risque. Pour ce mod√®le, les rendements sont mod√©lis√©s par un mouvement brownien g√©om√©trique avec une variance stochastique.\nLa volatilit√© non observ√©e \\(v_t\\) est d√©termin√©e par un processus stochastique de retour √† la moyenne (1) introduit en 1985 par Cox, Ingersoll et Ross pour la mod√©lisation des taux d‚Äôint√©r√™t √† court terme.\nLe param√®tre \\(\\kappa\\) est le param√®tre de retour √† la moyenne positive, \\(\\beta\\) est le param√®tre positif √† long terme et \\(\\eta\\) la volatilit√© positive du param√®tre de variance. De plus, Heston a introduit une corr√©lation entre les deux mouvements browniens \\(W_s^1\\) et \\(W_s^2\\), repr√©sent√©e par le param√®tre \\(\\rho\\) appartenant √† \\([-1,1]\\)."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "href": "posts/ensai/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la forme close",
    "text": "Avec la forme close\nSoit un Call de strike K et √† √©ch√©ance \\(\\tau\\) sous le mod√®le (1) avec les param√®tres suivants : \\(\\kappa\\) = 4,\\(\\beta\\) = 0.03,\\(\\sigma\\) = 0.4,r =0.05,\\(\\rho\\)=‚àí0.5,\\(\\tau\\) = 1, \\(S_0\\) = K=100,\\(v_0\\) = \\(\\beta\\).\nPour calculer le prix d‚Äôun Call, on peut utiliser la formule close de Heston (Heston 1993) :\n\\[\nC(S_0, K, \\tau) = S_0 P_1 - K e^{-r \\tau} P_2\n\\]\navec :\n\\[\nP_j(x, \\nu, T, \\ln(K)) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\Re \\left( \\frac{e^{-i \\ln(K) u} f_j(x,\\nu,t,u)}{i u} \\right) du\n\\]\no√π :\n\\[\nx = \\ln(S_t), \\quad f(x,\\nu,t,u) = \\exp(C(t,u) + D(t,u) \\nu + i \\phi x)\n\\]\net :\n\\[\nC(T-t = \\tau, \\phi) = r i \\phi t + \\frac{a}{\\sigma^2} \\left( (bj - \\rho \\sigma \\phi i + d)\\tau - 2 \\ln \\left( \\frac{1 - g e^{d \\tau}}{1 - g} \\right) \\right)\n\\]\n\\[\nD(T-t = \\tau, \\phi) = \\left( \\frac{bj - \\rho \\sigma \\phi i + d}{\\sigma^2} \\right) \\left( \\frac{1 - e^{d \\tau}}{1 - g e^{d \\tau}} \\right)\n\\]\n\\[\ng = \\frac{bj - \\rho \\sigma \\phi i + d}{bj - \\rho \\sigma \\phi i - d}\n\\]\n\\[\nd = \\sqrt{(\\rho \\sigma \\phi i - bj)^2 - \\sigma^2 (2 u_j \\phi i - \\phi^2)}\n\\]\n\\[\nu_1 = 1/2, \\quad u_2 = -1/2, a = \\lambda, b = \\kappa \\beta, \\quad t_1 = \\kappa - \\rho \\sigma, \\quad t_2 = \\kappa\n\\]\nPour ce faire, nous allons utiliser la fonction Heston_Call_Function.R qui permet de calculer le prix d‚Äôun Call sous le mod√®le d‚ÄôHeston avec la formule close.\n\n# Param√®tres\nkappa &lt;- 4\nbeta &lt;- 0.03\nsigma &lt;- 0.4\nr &lt;- 0.05\nrho &lt;- -0.5\ntau &lt;- 1\nS0&lt;- 100\nK &lt;- 100\nv0 &lt;- beta\n\n# Import Heston_Call_Function.R\nsource(\"data/Heston_Call_Function.R\")\n\n# Calcul du prix du Call\nCall_Heston &lt;- HestonCallClosedForm(lambda = kappa, vbar = beta, eta = sigma, rho = rho, v0 = v0, r = r, tau = tau, S0 = S0, K = K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.410405"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "href": "posts/ensai/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)",
    "text": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)\nLorsqu‚Äôon a pas acc√®s √† la formule close, on peut utiliser la m√©thode de Monte Carlo pour calculer le prix d‚Äôun Call. Il s‚Äôagit de simuler le mod√®le (1) et de calculer le prix du Call √† partir des simulations. Pour simuler le mod√®le (1), on peut utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston (Euler and Milstein Discretization, Fabrice Douglas Rouah) ou utiliser la formule de Ito pour le mod√®le de Heston.\nDans notre cas, nous allons utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston pour simuler le mod√®le (1) comme suit : \\[\n\\begin{cases}\nS_t = S_{t-1} \\left(1 + r \\Delta + \\sqrt{\\Delta v_t} W_t^1 \\right) \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\beta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}} \\Delta W_t^2 \\right| \\\\[10pt]\n\\text{Cov}(W_t^1, W_t^2) = \\rho\n\\end{cases}\n\\]\navec \\(W_t^1\\) et \\(W_t^2\\) des variables al√©atoires gaussiennes centr√©es r√©duites et corr√©l√©es entre elles telles que \\(\\text{Cov}(W_t^1, W_t^2) = \\rho\\). De plus, \\(\\Delta = \\frac{\\tau}{n}\\) est le pas de discr√©tisation, avec \\(n\\) le nombre de pas de discr√©tisation.\nDans notre cas, on d√©finit \\(n = 100\\) et on simule \\(M = 1000\\) mod√®le (1) pour calculer le prix d‚Äôun Call.\n\nHestonCallMC &lt;- function(M, N, lambda, vbar, eta, rho, v0, r, tau, S0, K){\n  # M: Number of Monte Carlo simulations\n  # N: Number of time steps\n  \n  set.seed(123)\n  dt &lt;- tau / N  # Time step\n\n  # Store final stock prices\n  ST &lt;- numeric(M)\n  \n  for (i in 1:M){\n    S &lt;- numeric(N+1)\n    v &lt;- numeric(N+1)\n    \n    S[1] &lt;- S0\n    v[1] &lt;- v0\n    \n    for (t in 1:N){\n      # Generate correlated Brownian motions\n      W1 &lt;- rnorm(1)\n      W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n      \n      # Euler discretization of variance process (ensure non-negativity)\n      v[t+1] &lt;- abs(v[t] + lambda * (vbar - v[t]) * dt + eta * sqrt(v[t] * dt) * W1)\n      \n      # Euler discretization of the stock price process (log-normal form)\n      S[t+1] &lt;- S[t] * exp((r - 0.5 * v[t]) * dt + sqrt(v[t] * dt) * W2)\n    }\n    \n    # Store final stock price\n    ST[i] &lt;- S[N+1]\n  }\n\n  # Compute Call option price using Monte Carlo method\n  Call &lt;- exp(-r * tau) * mean(pmax(ST - K, 0), na.rm=TRUE)\n  \n  return(Call)\n}\n\nM &lt;- 1000\nN &lt;- 100\nCall_Heston &lt;-HestonCallMC(M,N, kappa, beta, sigma, rho, v0, r, tau, S0, K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.797915"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part2.html",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part2.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "Le mod√®le classique de volatilit√© stochastique est d√©fini par les √©quations suivantes :\n\nProcessus des rendements :\n\\[ r_t = \\exp(x_t / 2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0,1) \\]\nProcessus de la volatilit√© logarithmique :\n\\[ x_t = \\mu + \\phi x_{t-1} + \\sigma_t \\eta_t, \\quad \\eta_t \\sim N(0,1) \\]\n\n\n( x_t ) suit un processus autor√©gressif de premier ordre (AR(1)) et suit une distribution normale conditionnelle : \\[ p(x_t) \\sim N(\\frac{\\mu}{1-\\phi} , \\frac{\\sigma_t^2}{1-\\phi^2}) \\]\n\\[ x_t | x_{t-1} \\sim N(\\mu + \\phi x_{t-1}, \\sigma_t^2) \\]\n( r_t ) suit une distribution normale conditionnelle :\n\\[ r_t | x_t \\sim N(0, \\exp(x_t)) \\]\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\nn &lt;- 252\nmu &lt;- -0.8\nphi &lt;- 0.9\nsigma_squared &lt;- 0.09\n\nx &lt;- numeric(n)  # Log-volatilit√©\nr &lt;- numeric(n)  # Rendements simul√©s\n\nfor (t in 1:n) {\n  if (t == 1) {\n    # Densit√© de transition stationnaire de x_t\n    x[t] &lt;- rnorm(1, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n  } else {\n    # √âvolution de l'√©tat\n    x[t] &lt;- mu + phi *x[t-1] + sqrt(sigma_squared) * rnorm(1, mean = 0, sd = 1)\n  }\n  # Simulation des rendements\n  r[t] &lt;- exp(x[t] / 2) * rnorm(1, mean = 0, sd = 1)\n}\n\n# extraction dans fichier csv\nwrite.csv(data.frame(r, x), \"true_sv_taylor.csv\", row.names = FALSE)\n\n\npar(mfrow=c(1,2))\nplot(x, lwd = 2, type = \"l\", col = \"blue\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© simul√©\")\nplot(r, lwd = 2, type = \"l\", col = \"red\", ylab = \"Rendements\", xlab = \"Temps\", main = \"Rendements simul√©s\")\n\n\n\n\n\n\n\n\n\n\n\nparams &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "params &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "set.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap avec les rendements",
    "text": "Filtre bootstrap avec les rendements\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Definition des variables\n# D√©finition des param√®tres\n\nparams &lt;- c(mu,phi,sigma_squared)\n\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t,] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ] - mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nUtilisation de library(pmhtutorial)\n\nlibrary(pmhtutorial)\n\n# particleFilterSVmodel takes sigma as parameters\nparams[3] &lt;- sqrt(params[3])\nx_hat_2&lt;- particleFilterSVmodel(r,params,M)\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"log-volatilit√©\")\nlines(x_hat_2$xHatFiltered, type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© estim√©\")\n# legend\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat_2$xHatFiltered / 2) * rnorm(length(x_hat_2$xHatFiltered), mean = 0, sd = 1)\n\n# Superposition des trajectoires\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",\n  col=\"#1B9E77\", main=\"True returns\")\n\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", \n     main = \"Estimated returns\",bty=\"n\")\n\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap sur le mod√®le log-sv de taylor",
    "text": "Filtre bootstrap sur le mod√®le log-sv de taylor\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nparams &lt;- c(mu,phi,sigma_squared)\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ]-mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_bs.html",
    "href": "posts/ensai/proc_stochastique/modele_bs.html",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "Le mod√®le de Black-Scholes est un mod√®le math√©matique qui permet de d√©terminer le prix d‚Äôune option √† partir de plusieurs param√®tres. Il est bas√© sur l‚Äôhypoth√®se que le prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique :\n\\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t\n\\]\nAvec \\(S_t\\) le prix de l‚Äôactif, \\(\\mu\\) le taux de rendement moyen, \\(\\sigma\\) la volatilit√© et \\(W_t\\) un mouvement brownien.\nDe ce fait, le prix d‚Äôune option europ√©enne peut √™tre calcul√© par la formule de Black-Scholes :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\nAvec \\(C\\) le prix de l‚Äôoption, \\(S_t\\) le prix de l‚Äôactif sous-jacent, \\(K\\) le prix d‚Äôexercice de l‚Äôoption, \\(T\\) la maturit√© de l‚Äôoption, \\(r\\) le taux d‚Äôint√©r√™t sans risque, \\(\\sigma\\) la volatilit√© de l‚Äôactif, \\(N\\) la fonction de r√©partition de la loi normale centr√©e r√©duite, et :\n\\[\nd_1 = \\frac{1}{\\sigma \\sqrt{T}} \\left( \\ln \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T \\right)\n\\]\n\\[\nd_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\n\n# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.752646541960356\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.7500497462893\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.669764558628767\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.10712199610432\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.26963328231244\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.429289467911312\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4209503065079776, gamma=0.027343746144537384, vega=29.15680602998068\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_bs.html#calcul-dun-call",
    "href": "posts/ensai/proc_stochastique/modele_bs.html#calcul-dun-call",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.752646541960356\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.7500497462893\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.669764558628767\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.10712199610432\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.26963328231244\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.429289467911312\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4209503065079776, gamma=0.027343746144537384, vega=29.15680602998068\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "posts/ensai/risques_financiers/risque_def.html",
    "href": "posts/ensai/risques_financiers/risque_def.html",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "",
    "text": "En finance, le risque peut √™tre d√©fini comme la survenance d‚Äôun √©v√©nement incertain qui peut avoir des cons√©quences n√©gatives sur le bilan, ou le compte de r√©sultat d‚Äôune banque. Par exemple, une fraude aura un impact n√©gative sur la r√©putation d‚Äôune banque ce qui peut entrainer des pertes importants ayant un impact n√©gatif sur le r√©sultat net de celle-ci. En √©conomie, le risque est un √©v√©nement probabilisable tandis que l‚Äôincertitude est non probabilisable.\nNous pouvons caract√©riser 3 grands types de risques √©tablis par le comit√© de B√¢le qui veille au renforcement et √† la stabilit√© du syst√®me financier. (rang√©s par ordre d‚Äôimportance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la cons√©quence. Toutefois, certains risque sont difficiles √† distinguer. Ils se trouvent √† la fronti√®re entre le risque de march√©, de cr√©dit et le risque op√©rationnel.\nIl est important de noter que le but d‚Äôune banque n‚Äôest pas de prendre le moins de risque, mais d‚Äôatteindre une rentabilit√© maximale pour un risque donn√©. La th√©orie financi√®re nous apprend que seul le risque est r√©mun√©r√©. La banque proc√®de donc √† une arbitrage entre risque et rentabilit√©. C‚Äôest pourquoi la gestion des risques est un √©l√©ment cl√© de la strat√©gie de d√©cision de la banque. La mesure du risque intervient pour calculer les fonds propres n√©cessaires pour assurer chaque op√©ration financi√®re. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "posts/ensai/risques_financiers/risque_def.html#les-mesures-de-risque",
    "href": "posts/ensai/risques_financiers/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al.¬†ont d√©fini les propri√©t√©s que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  coh√©rente. Une mesure de risque est une fonction qui permet de quantifier le risque d‚Äôun portefeuille. Elle est coh√©rente si elle satisfait les propri√©t√©s suivantes :\n\nsous-additivit√© : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomog√©n√©it√© positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivit√© signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal √† la somme des risques des actifs qui le composent. Ce ph√©nom√®ne est appel√© effet de diversification. En effet, la diversification permet de r√©duire le risque d‚Äôun portefeuille en investissant dans des actifs non corr√©l√©s. Ainsi, en agr√©geant deux porte-feuilles, il n‚Äôy a pas de cr√©ation de risque suppl√©mentaire.\nL‚Äôhomog√©n√©it√© positive signifie que le risque d‚Äôun portefeuille est proportionnel √† la taille du portefeuille. Cette propri√©t√© ignore les probl√®mes de liquidit√©.\nL‚Äôinvariance par translation signifie que l‚Äôaddition au portefeuille initiale un montant s√ªr r√©mun√©r√© au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d‚Äôimmobiliser des fonds propres √©gaux √† la mesure du risque.\nLa monotonie signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal au risque d‚Äôun autre portefeuille si la distribution de probabilit√© de la perte potentielle du premier portefeuille est inf√©rieure ou √©gale √† celle du deuxi√®me portefeuille. Cel√† traduit l‚Äôordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d‚Äôun portefeuille √† un instant t donn√©, le risque est la variation n√©gative de ce portefeuille dans le futur. Le risque se caract√©risait donc par une perte relativfe (par rapport √† la valeur initiale du portefeuille √† un instant t). Pendant tr√®s longtemps, les banques utilisaient la volatilit√© (√©cart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup √©volu√©e et celle qui est la plus r√©pandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d‚Äôun portefeuille √† un horizon \\(h\\) donn√© et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\no√π F est la distribution de probabilit√© de la perte potentielle du portefeuille.\nPar exemple, une VaR √† \\(\\alpha=1\\%\\) de 1 million d‚Äôeuros signifie que la probabilit√© que la banque perde plus de 1 million d‚Äôeuros est √©gale √† 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d‚Äôeuros sur une p√©riode donn√©e (C‚Äôest la perte maximale encourue par la banque avec un intervalle de confiance √† 99%). Nous allons pr√©f√©rer la deuxi√®me formulation de l‚Äôinterpr√©tation.\nDeux √©l√©ments sont n√©cessaires pour d√©terminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance √† un indicateur de tol√©rance pour le risque. Une couverture √† 99% est beaucoup plus exigente et donc plus co√ªteuse qu‚Äôune couverture √† 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait d√©finir l‚Äôhorizon h. Par exemple, une couverture √† 1 jour est moins co√ªteuse qu‚Äôune couverture √† 1 mois. C‚Äôest la combinaison de ces deux √©l√©ments qui d√©termine le degr√© de la couverture qui peut √™tre exprim√© en temps de retour 1 \\(t¬∞\\)qui est la dur√©e moyenne entre deux d√©passements de la VaR. Il permet de caract√©riser la raret√© d‚Äôun √©v√®nement (dont la probabilit√© d‚Äôoccurence est petite)\n\\[t¬∞= \\frac{h}{1-\\alpha}\\]\nLorsqu‚Äôon entend parler de gestion de risque d√©cennal, cel√† revient √† consid√©rer une valeur en risque (VaR) journali√®re (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non coh√©rente car elle ne respecte pas la propri√©t√© de sous-additivit√©. De nombreux professionnels recommanderaient alors l‚Äôutilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque coh√©rente. La CVAR est l‚Äôesp√©rance de la perte au del√† de la VaR. Toutefois, la VaR reste une mesure de risque tr√®s utilis√©e en pratique, qui ne respecte pas la propri√©t√© de sous-additivit√© que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilit√©s sont principalement localis√©es dans les quantiles extr√™mes.\n\n\n\nD‚Äôautres mesures de risque\nD‚Äôautres mesures peuvent √™tre d√©finis comme celle de la perte exceptionnelle (Unexpected Loss - UL) d√©finie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] o√π L est la distribution de la perte potentielle.\nIl s‚Äôagit l√† de la diff√©rence entre la VaR et la perte moyenne (expected loss - EL). Il y a √©galement le regret esp√©r√© d√©fini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donn√© repr√©sentant le montant de la perte tol√©rable par l‚Äôinstitut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l‚Äôinstitut financier. Lorsque H est endog√®ne, c‚Äôest-√†-dire d√©pendant de la distribution de la perte potentielle, et √©gale √† la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l‚Äôesp√©rance de la perte au del√† de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au del√† d‚Äôun certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au del√† de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c‚Äôest la semi variance."
  },
  {
    "objectID": "posts/ensai/risques_financiers/risque_def.html#footnotes",
    "href": "posts/ensai/risques_financiers/risque_def.html#footnotes",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\np√©riode de retour doit √™tre interpr√©t√©e comme la probabilit√© statistique qu‚Äôun √©v√®nement se produise‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "",
    "text": "Ce TP est fait dans le but de mod√©liser la Value at Risk qui est une mesure de risque financier. La Value at Risk (VaR) est une mesure essentielle du risque de march√© qui estime la perte potentielle maximale d‚Äôun portefeuille sur un horizon h donn√©, avec un certain niveau de confiance \\(\\alpha\\). La VaR est utilis√©e par les institutions financi√®res et les gestionnaires de risques pour √©valuer l‚Äôexposition aux pertes extr√™mes et ajuster leurs strat√©gies d‚Äôinvestissement. Elle est d√©finie comme suit :\n\\[VaR_{\\alpha}(h) = inf \\{ l \\in \\mathbb{R} | P(PnL \\geq -l) \\geq 1 - \\alpha \\}\\]\nLe PnL repr√©sente le profit and loss, c‚Äôest-√†-dire la variation de la valeur du portefeuille. Dans notre cas, nous utilserons les rendements logarithmiques des actifs financiers, qui est stationnaire, pour calculer la VaR. Les rendements logarithmiques sont calcul√©s comme suit :\n\\[r_t = log(\\frac{P_t}{P_{t-1}}) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}.\\]\nCette mesure est pr√©f√©r√©e aux rendements simples car sa d√©composition en termes additifs permet de mieux mod√©liser les variations de prix des actifs financiers. De plus, en supposant la distribution identique et ind√©pendante des rendements, on peut facilement d√©terminer sa loi de probabilit√© et calculer la VaR.\nDans ce projet, nous explorerons plusieurs m√©thodes pour calculer la VaR, en tenant compte de la nature des rendements financiers et des hypoth√®ses sous-jacentes :\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tqdm import tqdm\nfrom scipy.stats import bootstrap\n\nwarnings.filterwarnings(\"ignore\")\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\",start=\"2008-01-01\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\ndata.columns = data.columns.get_level_values(0)\n\n[*********************100%***********************]  1 of 1 completed\ndata.head()\n\n\n\n\n\n\n\nPrice\nClose\nHigh\nLow\nOpen\nVolume\nlog_return\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2008-01-03\n5546.080078\n5559.049805\n5515.609863\n5538.200195\n117504500\n-0.000771\n\n\n2008-01-04\n5446.790039\n5567.089844\n5417.529785\n5543.689941\n162947200\n-0.018065\n\n\n2008-01-07\n5452.830078\n5475.250000\n5429.319824\n5432.000000\n181161800\n0.001108\n\n\n2008-01-08\n5495.669922\n5533.930176\n5471.140137\n5476.509766\n174857100\n0.007826\n\n\n2008-01-09\n5435.419922\n5463.069824\n5420.060059\n5459.399902\n198041500\n-0.011024"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.1. Statistiques descriptives",
    "text": "II.1. Statistiques descriptives\nNous constatons que dans la p√©riode d‚Äôentrainement est plus volatile (std=1.39%). Cela s‚Äôexplique par le fait que la p√©riode d‚Äôentrainement prend en compte deux crises majeures : la crise des subprimes et la crise du Covid-19. Cela peut √©galement se voir √† travers les clusters de volatilit√© observables √† la suite de ces crises. Dans la p√©riode de test, aucun √©v√®nement majeur n‚Äôest observ√©, avec une plus faible volatilit√© observ√©e (std=0.08%).\nOn s‚Äôattend √† ce que la VaR entrain√©e sur la p√©riode d‚Äôentrainement performe tr√®s bien sur la p√©riode de test, mais on s‚Äôattend √©galement √† ce que la VaR entrain√©e sur la p√©riode de test performe moins bien sur la p√©riode d‚Äôentrainement.\n\nplt.figure(figsize=(10, 4))\nplt.plot(data_train, label='Train', color='grey')\nplt.plot(data_test, label='Test', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Donn√©es d'entrainement et de test\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_train.describe()\n\ncount    3523.000000\nmean        0.000153\nstd         0.013953\nmin        -0.130983\n25%        -0.006099\n50%         0.000580\n75%         0.006855\nmax         0.096169\nName: log_return, dtype: float64\n\n\n\ndata_test.describe()\n\ncount    586.000000\nmean       0.000292\nstd        0.008947\nmin       -0.036484\n25%       -0.004763\n50%        0.000642\n75%        0.005612\nmax        0.041504\nName: log_return, dtype: float64"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.2. VaR non param√©trique",
    "text": "II.2. VaR non param√©trique\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de scaling. 2. Approche bootstrap : On tire al√©atoirement des √©chantillons de rendements pass√©s avec remise, puis on prend le quantile empirique d‚Äôordre \\(\\alpha\\) pour calculer la VaR de chaque √©chantillon. La VaR finale est la moyenne des VaR obtenues.\n\nII.2.1. Historique\n\n# Objectif : impl√©menter une fonction calculant la VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1- alpha))\n\n\n# Calcul de la VaR historique sur l'√©chantillon d'entrainement pour h=1j et alpha=0.99\nalpha = 0.99\nvar_hist_train = historical_var(data_train, alpha=alpha)\nprint(f\"La VaR historique pour h=1j et alpha=0.99 est : {var_hist_train:.4%}\")\n\nLa VaR historique pour h=1j et alpha=0.99 est : 4.0850%\n\n\nOn constate que la VaR historique pour une horizon de 1 jour et un niveau de confiance de 99% est de -4,09%. De ce fait, la perte maximale que l‚Äôon peut subir avec un niveau de confiance de 99% sur un jour est de 4,09%. Autrement dit, il y a 1 chance sur 100 que la perte soit sup√©rieure √† 4,09%. Cette perte peut se produire 2 √† 3 ans fois en une ann√©e (252 jours de trading).\n\n\nII.2.2. Bootstrap\nPour l‚Äôimpl√©mentation de la VaR bootstrap, nous faisons le choix de faire un tirage de taille n=la taille de la s√©rie des rendements, avec remise. Ce choix est fait pour des raisons de simplicit√©. En ce qui concerne le choix du nombre d‚Äô√©chantillons, nous allons observer l‚Äô√©volution de de l‚Äôestimation de la VaR en fonction du nombre d‚Äô√©chantillons. Nous limiterons √† des √©chantillons compris entre 1000 et 10000, pour des raisons de temps computationnels, en ayant conscience que plus le nombre d‚Äô√©chantillons est grand, plus l‚Äôestimation de la VaR sera pr√©cise.\n\n# Objectif : impl√©menter une fonction calculant la VaR bootstrap et un IC\n\ndef bootstrap_var(data, alpha=0.99, M=1000, seuil=0.05):\n    \"\"\"\n    Calcul de la VaR bootstrap\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance de la VaR\n    n : le nombre de simulations\n    seuil : le seuil de l'intervalle de confiance\n    \"\"\"\n    # set seed\n    np.random.seed(42)\n\n    # Initialisation du vecteur des VaR\n    var = np.zeros(M)\n\n    # Calcul de la VaR bootstrap\n    for i in range(M):\n        sample = np.random.choice(data, size=len(data), replace=True)\n        var[i] = -np.percentile(sample, 100*(1- alpha))\n\n    # Calcul de l'intervalle de confiance\n    lower = np.percentile(var, 100*(1-seuil)/2)\n    upper = np.percentile(var, 100*(seuil + (1-seuil)/2))\n\n    return np.mean(var), lower, upper\n\n\n# Observer la variation de la VaR en fonction de M\nM_values = np.arange(1000, 10000, 10)\nvar_bs_values = []\n\nfor M in tqdm(M_values):\n    var_bs_train, _, _ = bootstrap_var(data_train, alpha=alpha, M=M)\n    var_bs_values.append(var_bs_train)\n\n  0%|          | 0/900 [00:00&lt;?, ?it/s]  0%|          | 2/900 [00:00&lt;00:57, 15.54it/s]  0%|          | 4/900 [00:00&lt;00:58, 15.20it/s]  1%|          | 6/900 [00:00&lt;00:58, 15.25it/s]  1%|          | 8/900 [00:00&lt;00:58, 15.17it/s]  1%|          | 10/900 [00:00&lt;00:59, 15.02it/s]  1%|‚ñè         | 12/900 [00:00&lt;01:00, 14.72it/s]  2%|‚ñè         | 14/900 [00:00&lt;01:01, 14.52it/s]  2%|‚ñè         | 16/900 [00:01&lt;01:01, 14.34it/s]  2%|‚ñè         | 18/900 [00:01&lt;01:02, 14.16it/s]  2%|‚ñè         | 20/900 [00:01&lt;01:03, 13.92it/s]  2%|‚ñè         | 22/900 [00:01&lt;01:03, 13.73it/s]  3%|‚ñé         | 24/900 [00:01&lt;01:05, 13.47it/s]  3%|‚ñé         | 26/900 [00:01&lt;01:05, 13.29it/s]  3%|‚ñé         | 28/900 [00:01&lt;01:06, 13.10it/s]  3%|‚ñé         | 30/900 [00:02&lt;01:07, 12.88it/s]  4%|‚ñé         | 32/900 [00:02&lt;01:08, 12.70it/s]  4%|‚ñç         | 34/900 [00:02&lt;01:09, 12.52it/s]  4%|‚ñç         | 36/900 [00:02&lt;01:10, 12.26it/s]  4%|‚ñç         | 38/900 [00:02&lt;01:11, 12.07it/s]  4%|‚ñç         | 40/900 [00:03&lt;01:12, 11.91it/s]  5%|‚ñç         | 42/900 [00:03&lt;01:13, 11.72it/s]  5%|‚ñç         | 44/900 [00:03&lt;01:14, 11.57it/s]  5%|‚ñå         | 46/900 [00:03&lt;01:15, 11.38it/s]  5%|‚ñå         | 48/900 [00:03&lt;01:15, 11.22it/s]  6%|‚ñå         | 50/900 [00:03&lt;01:16, 11.05it/s]  6%|‚ñå         | 52/900 [00:04&lt;01:17, 10.90it/s]  6%|‚ñå         | 54/900 [00:04&lt;01:18, 10.75it/s]  6%|‚ñå         | 56/900 [00:04&lt;01:19, 10.62it/s]  6%|‚ñã         | 58/900 [00:04&lt;01:20, 10.47it/s]  7%|‚ñã         | 60/900 [00:04&lt;01:21, 10.32it/s]  7%|‚ñã         | 62/900 [00:05&lt;01:22, 10.18it/s]  7%|‚ñã         | 64/900 [00:05&lt;01:23, 10.04it/s]  7%|‚ñã         | 66/900 [00:05&lt;01:24,  9.92it/s]  7%|‚ñã         | 67/900 [00:05&lt;01:24,  9.87it/s]  8%|‚ñä         | 68/900 [00:05&lt;01:25,  9.78it/s]  8%|‚ñä         | 69/900 [00:05&lt;01:25,  9.72it/s]  8%|‚ñä         | 70/900 [00:05&lt;01:26,  9.63it/s]  8%|‚ñä         | 71/900 [00:06&lt;01:26,  9.56it/s]  8%|‚ñä         | 72/900 [00:06&lt;01:27,  9.50it/s]  8%|‚ñä         | 73/900 [00:06&lt;01:27,  9.41it/s]  8%|‚ñä         | 74/900 [00:06&lt;01:28,  9.36it/s]  8%|‚ñä         | 75/900 [00:06&lt;01:28,  9.31it/s]  8%|‚ñä         | 76/900 [00:06&lt;01:29,  9.24it/s]  9%|‚ñä         | 77/900 [00:06&lt;01:29,  9.17it/s]  9%|‚ñä         | 78/900 [00:06&lt;01:30,  9.12it/s]  9%|‚ñâ         | 79/900 [00:06&lt;01:30,  9.08it/s]  9%|‚ñâ         | 80/900 [00:07&lt;01:30,  9.01it/s]  9%|‚ñâ         | 81/900 [00:07&lt;01:31,  8.98it/s]  9%|‚ñâ         | 82/900 [00:07&lt;01:31,  8.90it/s]  9%|‚ñâ         | 83/900 [00:07&lt;01:32,  8.87it/s]  9%|‚ñâ         | 84/900 [00:07&lt;01:32,  8.82it/s]  9%|‚ñâ         | 85/900 [00:07&lt;01:32,  8.77it/s] 10%|‚ñâ         | 86/900 [00:07&lt;01:33,  8.70it/s] 10%|‚ñâ         | 87/900 [00:07&lt;01:34,  8.64it/s] 10%|‚ñâ         | 88/900 [00:07&lt;01:34,  8.59it/s] 10%|‚ñâ         | 89/900 [00:08&lt;01:34,  8.54it/s] 10%|‚ñà         | 90/900 [00:08&lt;01:35,  8.49it/s] 10%|‚ñà         | 91/900 [00:08&lt;01:35,  8.47it/s] 10%|‚ñà         | 92/900 [00:08&lt;01:35,  8.45it/s] 10%|‚ñà         | 93/900 [00:08&lt;01:35,  8.41it/s] 10%|‚ñà         | 94/900 [00:08&lt;01:36,  8.37it/s] 11%|‚ñà         | 95/900 [00:08&lt;01:36,  8.32it/s] 11%|‚ñà         | 96/900 [00:08&lt;01:37,  8.25it/s] 11%|‚ñà         | 97/900 [00:09&lt;01:37,  8.20it/s] 11%|‚ñà         | 98/900 [00:09&lt;01:38,  8.17it/s] 11%|‚ñà         | 99/900 [00:09&lt;01:38,  8.10it/s] 11%|‚ñà         | 100/900 [00:09&lt;01:38,  8.08it/s] 11%|‚ñà         | 101/900 [00:09&lt;01:39,  8.03it/s] 11%|‚ñà‚ñè        | 102/900 [00:09&lt;01:39,  8.01it/s] 11%|‚ñà‚ñè        | 103/900 [00:09&lt;01:39,  7.98it/s] 12%|‚ñà‚ñè        | 104/900 [00:09&lt;01:40,  7.95it/s] 12%|‚ñà‚ñè        | 105/900 [00:10&lt;01:40,  7.90it/s] 12%|‚ñà‚ñè        | 106/900 [00:10&lt;01:40,  7.87it/s] 12%|‚ñà‚ñè        | 107/900 [00:10&lt;01:41,  7.81it/s] 12%|‚ñà‚ñè        | 108/900 [00:10&lt;01:42,  7.70it/s] 12%|‚ñà‚ñè        | 109/900 [00:10&lt;01:43,  7.66it/s] 12%|‚ñà‚ñè        | 110/900 [00:10&lt;01:43,  7.63it/s] 12%|‚ñà‚ñè        | 111/900 [00:10&lt;01:44,  7.58it/s] 12%|‚ñà‚ñè        | 112/900 [00:10&lt;01:44,  7.56it/s] 13%|‚ñà‚ñé        | 113/900 [00:11&lt;01:45,  7.48it/s] 13%|‚ñà‚ñé        | 114/900 [00:11&lt;01:45,  7.43it/s] 13%|‚ñà‚ñé        | 115/900 [00:11&lt;01:49,  7.18it/s] 13%|‚ñà‚ñé        | 116/900 [00:11&lt;01:48,  7.23it/s] 13%|‚ñà‚ñé        | 117/900 [00:11&lt;01:47,  7.28it/s] 13%|‚ñà‚ñé        | 118/900 [00:11&lt;01:48,  7.19it/s] 13%|‚ñà‚ñé        | 119/900 [00:11&lt;01:48,  7.22it/s] 13%|‚ñà‚ñé        | 120/900 [00:12&lt;01:48,  7.18it/s] 13%|‚ñà‚ñé        | 121/900 [00:12&lt;01:48,  7.20it/s] 14%|‚ñà‚ñé        | 122/900 [00:12&lt;01:48,  7.16it/s] 14%|‚ñà‚ñé        | 123/900 [00:12&lt;01:49,  7.09it/s] 14%|‚ñà‚ñç        | 124/900 [00:12&lt;01:49,  7.11it/s] 14%|‚ñà‚ñç        | 125/900 [00:12&lt;01:50,  6.98it/s] 14%|‚ñà‚ñç        | 126/900 [00:12&lt;01:50,  7.01it/s] 14%|‚ñà‚ñç        | 127/900 [00:13&lt;01:51,  6.95it/s] 14%|‚ñà‚ñç        | 128/900 [00:13&lt;01:51,  6.92it/s] 14%|‚ñà‚ñç        | 129/900 [00:13&lt;01:53,  6.82it/s] 14%|‚ñà‚ñç        | 130/900 [00:13&lt;01:53,  6.81it/s] 15%|‚ñà‚ñç        | 131/900 [00:13&lt;01:52,  6.81it/s] 15%|‚ñà‚ñç        | 132/900 [00:13&lt;01:53,  6.79it/s] 15%|‚ñà‚ñç        | 133/900 [00:13&lt;01:52,  6.81it/s] 15%|‚ñà‚ñç        | 134/900 [00:14&lt;01:53,  6.75it/s] 15%|‚ñà‚ñå        | 135/900 [00:14&lt;01:53,  6.72it/s] 15%|‚ñà‚ñå        | 136/900 [00:14&lt;01:54,  6.66it/s] 15%|‚ñà‚ñå        | 137/900 [00:14&lt;01:56,  6.56it/s] 15%|‚ñà‚ñå        | 138/900 [00:14&lt;01:55,  6.61it/s] 15%|‚ñà‚ñå        | 139/900 [00:14&lt;01:56,  6.53it/s] 16%|‚ñà‚ñå        | 140/900 [00:15&lt;01:56,  6.53it/s] 16%|‚ñà‚ñå        | 141/900 [00:15&lt;01:55,  6.56it/s] 16%|‚ñà‚ñå        | 142/900 [00:15&lt;01:56,  6.48it/s] 16%|‚ñà‚ñå        | 143/900 [00:15&lt;01:56,  6.49it/s] 16%|‚ñà‚ñå        | 144/900 [00:15&lt;01:57,  6.44it/s] 16%|‚ñà‚ñå        | 145/900 [00:15&lt;01:57,  6.43it/s] 16%|‚ñà‚ñå        | 146/900 [00:15&lt;01:57,  6.44it/s] 16%|‚ñà‚ñã        | 147/900 [00:16&lt;01:58,  6.37it/s] 16%|‚ñà‚ñã        | 148/900 [00:16&lt;01:58,  6.33it/s] 17%|‚ñà‚ñã        | 149/900 [00:16&lt;02:02,  6.11it/s] 17%|‚ñà‚ñã        | 150/900 [00:16&lt;02:03,  6.09it/s] 17%|‚ñà‚ñã        | 151/900 [00:16&lt;02:01,  6.15it/s] 17%|‚ñà‚ñã        | 152/900 [00:16&lt;02:00,  6.21it/s] 17%|‚ñà‚ñã        | 153/900 [00:17&lt;02:01,  6.12it/s] 17%|‚ñà‚ñã        | 154/900 [00:17&lt;02:01,  6.14it/s] 17%|‚ñà‚ñã        | 155/900 [00:17&lt;02:00,  6.19it/s] 17%|‚ñà‚ñã        | 156/900 [00:17&lt;02:00,  6.20it/s] 17%|‚ñà‚ñã        | 157/900 [00:17&lt;02:01,  6.13it/s] 18%|‚ñà‚ñä        | 158/900 [00:17&lt;02:00,  6.14it/s] 18%|‚ñà‚ñä        | 159/900 [00:18&lt;02:00,  6.16it/s] 18%|‚ñà‚ñä        | 160/900 [00:18&lt;02:03,  5.99it/s] 18%|‚ñà‚ñä        | 161/900 [00:18&lt;02:03,  5.97it/s] 18%|‚ñà‚ñä        | 162/900 [00:18&lt;02:03,  5.98it/s] 18%|‚ñà‚ñä        | 163/900 [00:18&lt;02:02,  6.02it/s] 18%|‚ñà‚ñä        | 164/900 [00:18&lt;02:01,  6.05it/s] 18%|‚ñà‚ñä        | 165/900 [00:19&lt;02:02,  6.02it/s] 18%|‚ñà‚ñä        | 166/900 [00:19&lt;02:02,  5.99it/s] 19%|‚ñà‚ñä        | 167/900 [00:19&lt;02:02,  6.00it/s] 19%|‚ñà‚ñä        | 168/900 [00:19&lt;02:02,  6.00it/s] 19%|‚ñà‚ñâ        | 169/900 [00:19&lt;02:02,  5.99it/s] 19%|‚ñà‚ñâ        | 170/900 [00:19&lt;02:02,  5.98it/s] 19%|‚ñà‚ñâ        | 171/900 [00:20&lt;02:02,  5.97it/s] 19%|‚ñà‚ñâ        | 172/900 [00:20&lt;02:03,  5.92it/s] 19%|‚ñà‚ñâ        | 173/900 [00:20&lt;02:03,  5.89it/s] 19%|‚ñà‚ñâ        | 174/900 [00:20&lt;02:04,  5.85it/s] 19%|‚ñà‚ñâ        | 175/900 [00:20&lt;02:03,  5.85it/s] 20%|‚ñà‚ñâ        | 176/900 [00:21&lt;02:13,  5.42it/s] 20%|‚ñà‚ñâ        | 177/900 [00:21&lt;02:11,  5.49it/s] 20%|‚ñà‚ñâ        | 178/900 [00:21&lt;02:09,  5.56it/s] 20%|‚ñà‚ñâ        | 179/900 [00:21&lt;02:08,  5.60it/s] 20%|‚ñà‚ñà        | 180/900 [00:21&lt;02:10,  5.53it/s] 20%|‚ñà‚ñà        | 181/900 [00:21&lt;02:16,  5.27it/s] 20%|‚ñà‚ñà        | 182/900 [00:22&lt;02:13,  5.38it/s] 20%|‚ñà‚ñà        | 183/900 [00:22&lt;02:11,  5.46it/s] 20%|‚ñà‚ñà        | 184/900 [00:22&lt;02:09,  5.52it/s] 21%|‚ñà‚ñà        | 185/900 [00:22&lt;02:09,  5.51it/s] 21%|‚ñà‚ñà        | 186/900 [00:22&lt;02:09,  5.51it/s] 21%|‚ñà‚ñà        | 187/900 [00:23&lt;02:09,  5.51it/s] 21%|‚ñà‚ñà        | 188/900 [00:23&lt;02:08,  5.53it/s] 21%|‚ñà‚ñà        | 189/900 [00:23&lt;02:08,  5.53it/s] 21%|‚ñà‚ñà        | 190/900 [00:23&lt;02:08,  5.53it/s] 21%|‚ñà‚ñà        | 191/900 [00:23&lt;02:08,  5.53it/s] 21%|‚ñà‚ñà‚ñè       | 192/900 [00:23&lt;02:08,  5.49it/s] 21%|‚ñà‚ñà‚ñè       | 193/900 [00:24&lt;02:08,  5.48it/s] 22%|‚ñà‚ñà‚ñè       | 194/900 [00:24&lt;02:08,  5.48it/s] 22%|‚ñà‚ñà‚ñè       | 195/900 [00:24&lt;02:08,  5.47it/s] 22%|‚ñà‚ñà‚ñè       | 196/900 [00:24&lt;02:11,  5.35it/s] 22%|‚ñà‚ñà‚ñè       | 197/900 [00:24&lt;02:11,  5.35it/s] 22%|‚ñà‚ñà‚ñè       | 198/900 [00:25&lt;02:11,  5.35it/s] 22%|‚ñà‚ñà‚ñè       | 199/900 [00:25&lt;02:11,  5.35it/s] 22%|‚ñà‚ñà‚ñè       | 200/900 [00:25&lt;02:10,  5.36it/s] 22%|‚ñà‚ñà‚ñè       | 201/900 [00:25&lt;02:10,  5.36it/s] 22%|‚ñà‚ñà‚ñè       | 202/900 [00:25&lt;02:10,  5.35it/s] 23%|‚ñà‚ñà‚ñé       | 203/900 [00:25&lt;02:10,  5.34it/s] 23%|‚ñà‚ñà‚ñé       | 204/900 [00:26&lt;02:10,  5.33it/s] 23%|‚ñà‚ñà‚ñé       | 205/900 [00:26&lt;02:14,  5.16it/s] 23%|‚ñà‚ñà‚ñé       | 206/900 [00:26&lt;02:17,  5.05it/s] 23%|‚ñà‚ñà‚ñé       | 207/900 [00:26&lt;02:16,  5.07it/s] 23%|‚ñà‚ñà‚ñé       | 208/900 [00:26&lt;02:15,  5.11it/s] 23%|‚ñà‚ñà‚ñé       | 209/900 [00:27&lt;02:15,  5.12it/s] 23%|‚ñà‚ñà‚ñé       | 210/900 [00:27&lt;02:14,  5.11it/s] 23%|‚ñà‚ñà‚ñé       | 211/900 [00:27&lt;02:14,  5.13it/s] 24%|‚ñà‚ñà‚ñé       | 212/900 [00:27&lt;02:22,  4.83it/s] 24%|‚ñà‚ñà‚ñé       | 213/900 [00:27&lt;02:22,  4.84it/s] 24%|‚ñà‚ñà‚ñç       | 214/900 [00:28&lt;02:19,  4.91it/s] 24%|‚ñà‚ñà‚ñç       | 215/900 [00:28&lt;02:23,  4.77it/s] 24%|‚ñà‚ñà‚ñç       | 216/900 [00:28&lt;02:25,  4.69it/s] 24%|‚ñà‚ñà‚ñç       | 217/900 [00:28&lt;02:23,  4.75it/s] 24%|‚ñà‚ñà‚ñç       | 218/900 [00:29&lt;02:24,  4.72it/s] 24%|‚ñà‚ñà‚ñç       | 219/900 [00:29&lt;02:25,  4.67it/s] 24%|‚ñà‚ñà‚ñç       | 220/900 [00:29&lt;02:23,  4.74it/s] 25%|‚ñà‚ñà‚ñç       | 221/900 [00:29&lt;02:20,  4.82it/s] 25%|‚ñà‚ñà‚ñç       | 222/900 [00:29&lt;02:31,  4.47it/s] 25%|‚ñà‚ñà‚ñç       | 223/900 [00:30&lt;02:28,  4.55it/s] 25%|‚ñà‚ñà‚ñç       | 224/900 [00:30&lt;02:25,  4.63it/s] 25%|‚ñà‚ñà‚ñå       | 225/900 [00:30&lt;02:33,  4.41it/s] 25%|‚ñà‚ñà‚ñå       | 226/900 [00:30&lt;02:31,  4.44it/s] 25%|‚ñà‚ñà‚ñå       | 227/900 [00:31&lt;02:27,  4.55it/s] 25%|‚ñà‚ñà‚ñå       | 228/900 [00:31&lt;02:25,  4.63it/s] 25%|‚ñà‚ñà‚ñå       | 229/900 [00:31&lt;02:23,  4.69it/s] 26%|‚ñà‚ñà‚ñå       | 230/900 [00:31&lt;02:29,  4.48it/s] 26%|‚ñà‚ñà‚ñå       | 231/900 [00:31&lt;02:28,  4.51it/s] 26%|‚ñà‚ñà‚ñå       | 232/900 [00:32&lt;02:25,  4.59it/s] 26%|‚ñà‚ñà‚ñå       | 233/900 [00:32&lt;02:23,  4.65it/s] 26%|‚ñà‚ñà‚ñå       | 234/900 [00:32&lt;02:21,  4.70it/s] 26%|‚ñà‚ñà‚ñå       | 235/900 [00:32&lt;02:20,  4.73it/s] 26%|‚ñà‚ñà‚ñå       | 236/900 [00:32&lt;02:19,  4.75it/s] 26%|‚ñà‚ñà‚ñã       | 237/900 [00:33&lt;02:20,  4.72it/s] 26%|‚ñà‚ñà‚ñã       | 238/900 [00:33&lt;02:28,  4.45it/s] 27%|‚ñà‚ñà‚ñã       | 239/900 [00:33&lt;02:27,  4.48it/s] 27%|‚ñà‚ñà‚ñã       | 240/900 [00:33&lt;02:25,  4.55it/s] 27%|‚ñà‚ñà‚ñã       | 241/900 [00:34&lt;02:30,  4.36it/s] 27%|‚ñà‚ñà‚ñã       | 242/900 [00:34&lt;02:30,  4.38it/s] 27%|‚ñà‚ñà‚ñã       | 243/900 [00:34&lt;02:27,  4.44it/s] 27%|‚ñà‚ñà‚ñã       | 244/900 [00:34&lt;02:25,  4.51it/s] 27%|‚ñà‚ñà‚ñã       | 245/900 [00:34&lt;02:24,  4.53it/s] 27%|‚ñà‚ñà‚ñã       | 246/900 [00:35&lt;02:24,  4.54it/s] 27%|‚ñà‚ñà‚ñã       | 247/900 [00:35&lt;02:22,  4.57it/s] 28%|‚ñà‚ñà‚ñä       | 248/900 [00:35&lt;02:22,  4.59it/s] 28%|‚ñà‚ñà‚ñä       | 249/900 [00:35&lt;02:21,  4.59it/s] 28%|‚ñà‚ñà‚ñä       | 250/900 [00:36&lt;02:21,  4.59it/s] 28%|‚ñà‚ñà‚ñä       | 251/900 [00:36&lt;02:25,  4.47it/s] 28%|‚ñà‚ñà‚ñä       | 252/900 [00:36&lt;02:33,  4.21it/s] 28%|‚ñà‚ñà‚ñä       | 253/900 [00:36&lt;02:32,  4.23it/s] 28%|‚ñà‚ñà‚ñä       | 254/900 [00:37&lt;02:29,  4.32it/s] 28%|‚ñà‚ñà‚ñä       | 255/900 [00:37&lt;02:28,  4.36it/s] 28%|‚ñà‚ñà‚ñä       | 256/900 [00:37&lt;02:27,  4.38it/s] 29%|‚ñà‚ñà‚ñä       | 257/900 [00:37&lt;02:26,  4.39it/s] 29%|‚ñà‚ñà‚ñä       | 258/900 [00:37&lt;02:32,  4.22it/s] 29%|‚ñà‚ñà‚ñâ       | 259/900 [00:38&lt;02:37,  4.07it/s] 29%|‚ñà‚ñà‚ñâ       | 260/900 [00:38&lt;02:34,  4.15it/s] 29%|‚ñà‚ñà‚ñâ       | 261/900 [00:38&lt;02:30,  4.24it/s] 29%|‚ñà‚ñà‚ñâ       | 262/900 [00:38&lt;02:35,  4.10it/s] 29%|‚ñà‚ñà‚ñâ       | 263/900 [00:39&lt;02:37,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 264/900 [00:39&lt;02:34,  4.11it/s] 29%|‚ñà‚ñà‚ñâ       | 265/900 [00:39&lt;02:37,  4.03it/s] 30%|‚ñà‚ñà‚ñâ       | 266/900 [00:39&lt;02:36,  4.06it/s] 30%|‚ñà‚ñà‚ñâ       | 267/900 [00:40&lt;02:32,  4.14it/s] 30%|‚ñà‚ñà‚ñâ       | 268/900 [00:40&lt;02:34,  4.09it/s] 30%|‚ñà‚ñà‚ñâ       | 269/900 [00:40&lt;02:39,  3.97it/s] 30%|‚ñà‚ñà‚ñà       | 270/900 [00:40&lt;02:36,  4.02it/s] 30%|‚ñà‚ñà‚ñà       | 271/900 [00:41&lt;02:38,  3.96it/s] 30%|‚ñà‚ñà‚ñà       | 272/900 [00:41&lt;02:40,  3.92it/s] 30%|‚ñà‚ñà‚ñà       | 273/900 [00:41&lt;02:35,  4.02it/s] 30%|‚ñà‚ñà‚ñà       | 274/900 [00:41&lt;02:33,  4.07it/s] 31%|‚ñà‚ñà‚ñà       | 275/900 [00:42&lt;02:31,  4.13it/s] 31%|‚ñà‚ñà‚ñà       | 276/900 [00:42&lt;02:30,  4.16it/s] 31%|‚ñà‚ñà‚ñà       | 277/900 [00:42&lt;02:39,  3.90it/s] 31%|‚ñà‚ñà‚ñà       | 278/900 [00:42&lt;02:37,  3.94it/s] 31%|‚ñà‚ñà‚ñà       | 279/900 [00:43&lt;02:34,  4.01it/s] 31%|‚ñà‚ñà‚ñà       | 280/900 [00:43&lt;02:32,  4.06it/s] 31%|‚ñà‚ñà‚ñà       | 281/900 [00:43&lt;02:30,  4.10it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 282/900 [00:43&lt;02:29,  4.13it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 283/900 [00:44&lt;02:37,  3.91it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 284/900 [00:44&lt;02:36,  3.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 285/900 [00:44&lt;02:33,  4.01it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 286/900 [00:44&lt;02:31,  4.04it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 287/900 [00:45&lt;02:33,  3.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 288/900 [00:45&lt;02:36,  3.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 289/900 [00:45&lt;02:34,  3.94it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 290/900 [00:45&lt;02:32,  3.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 291/900 [00:46&lt;02:45,  3.67it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 292/900 [00:46&lt;02:41,  3.76it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 293/900 [00:46&lt;02:38,  3.83it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 294/900 [00:47&lt;02:39,  3.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 295/900 [00:47&lt;02:41,  3.75it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 296/900 [00:47&lt;02:38,  3.82it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 297/900 [00:47&lt;02:35,  3.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 298/900 [00:48&lt;02:33,  3.92it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 299/900 [00:48&lt;02:41,  3.73it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 300/900 [00:48&lt;02:39,  3.75it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 301/900 [00:48&lt;02:36,  3.83it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 302/900 [00:49&lt;02:44,  3.65it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 303/900 [00:49&lt;02:43,  3.65it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 304/900 [00:49&lt;02:39,  3.73it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 305/900 [00:49&lt;02:40,  3.70it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 306/900 [00:50&lt;02:37,  3.76it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 307/900 [00:50&lt;02:39,  3.72it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 308/900 [00:50&lt;02:43,  3.62it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 309/900 [00:51&lt;02:40,  3.69it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 310/900 [00:51&lt;02:37,  3.75it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 311/900 [00:51&lt;02:38,  3.73it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 312/900 [00:51&lt;02:36,  3.76it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 313/900 [00:52&lt;02:35,  3.78it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 314/900 [00:52&lt;02:40,  3.65it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 315/900 [00:52&lt;02:42,  3.60it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 316/900 [00:52&lt;02:39,  3.66it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 317/900 [00:53&lt;02:42,  3.59it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 318/900 [00:53&lt;02:41,  3.60it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 319/900 [00:53&lt;02:38,  3.67it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 320/900 [00:54&lt;02:37,  3.68it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 321/900 [00:54&lt;02:35,  3.72it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 322/900 [00:54&lt;02:34,  3.75it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 323/900 [00:54&lt;02:39,  3.62it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 324/900 [00:55&lt;02:40,  3.59it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 325/900 [00:55&lt;02:38,  3.64it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 326/900 [00:55&lt;02:36,  3.66it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 327/900 [00:55&lt;02:35,  3.67it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 328/900 [00:56&lt;02:34,  3.69it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 329/900 [00:56&lt;02:34,  3.70it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 330/900 [00:56&lt;02:33,  3.71it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 331/900 [00:57&lt;02:35,  3.67it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 332/900 [00:57&lt;02:34,  3.68it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 333/900 [00:57&lt;02:34,  3.67it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 334/900 [00:57&lt;02:41,  3.50it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 335/900 [00:58&lt;02:39,  3.53it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 336/900 [00:58&lt;02:38,  3.56it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 337/900 [00:58&lt;02:37,  3.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 338/900 [00:58&lt;02:37,  3.57it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 339/900 [00:59&lt;02:41,  3.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 340/900 [00:59&lt;02:39,  3.52it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 341/900 [00:59&lt;02:44,  3.41it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 342/900 [01:00&lt;02:41,  3.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 343/900 [01:00&lt;02:46,  3.35it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 344/900 [01:00&lt;02:46,  3.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 345/900 [01:01&lt;02:42,  3.41it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 346/900 [01:01&lt;02:50,  3.25it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 347/900 [01:01&lt;02:48,  3.28it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 348/900 [01:01&lt;02:44,  3.36it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 349/900 [01:02&lt;02:49,  3.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 350/900 [01:02&lt;02:46,  3.31it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 351/900 [01:02&lt;02:42,  3.38it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 352/900 [01:03&lt;02:40,  3.42it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 353/900 [01:03&lt;02:38,  3.45it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 354/900 [01:03&lt;02:37,  3.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 355/900 [01:04&lt;02:36,  3.48it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 356/900 [01:04&lt;02:35,  3.49it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 357/900 [01:04&lt;02:38,  3.43it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 358/900 [01:04&lt;02:37,  3.43it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 359/900 [01:05&lt;02:43,  3.31it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 360/900 [01:05&lt;02:44,  3.28it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 361/900 [01:05&lt;02:41,  3.34it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 362/900 [01:06&lt;02:39,  3.37it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 363/900 [01:06&lt;02:45,  3.24it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 364/900 [01:06&lt;02:47,  3.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 365/900 [01:07&lt;02:43,  3.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 366/900 [01:07&lt;02:41,  3.31it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 367/900 [01:07&lt;02:39,  3.35it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 368/900 [01:08&lt;02:47,  3.18it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 369/900 [01:08&lt;02:43,  3.24it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 370/900 [01:08&lt;02:49,  3.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 371/900 [01:08&lt;02:44,  3.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 372/900 [01:09&lt;02:42,  3.25it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 373/900 [01:09&lt;02:49,  3.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 374/900 [01:09&lt;02:47,  3.15it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 375/900 [01:10&lt;02:43,  3.21it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 376/900 [01:10&lt;02:41,  3.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 377/900 [01:10&lt;02:39,  3.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 378/900 [01:11&lt;02:38,  3.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 379/900 [01:11&lt;02:36,  3.32it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 380/900 [01:11&lt;02:38,  3.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 381/900 [01:12&lt;02:37,  3.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 382/900 [01:12&lt;02:36,  3.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 383/900 [01:12&lt;02:36,  3.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 384/900 [01:12&lt;02:35,  3.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 385/900 [01:13&lt;02:36,  3.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 386/900 [01:13&lt;02:35,  3.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 387/900 [01:13&lt;02:35,  3.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 388/900 [01:14&lt;02:35,  3.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 389/900 [01:14&lt;02:35,  3.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 390/900 [01:14&lt;02:34,  3.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 391/900 [01:15&lt;02:41,  3.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 392/900 [01:15&lt;02:40,  3.16it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 393/900 [01:15&lt;02:39,  3.18it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 394/900 [01:16&lt;02:38,  3.19it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 395/900 [01:16&lt;02:38,  3.19it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 396/900 [01:16&lt;02:44,  3.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 397/900 [01:17&lt;02:44,  3.07it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 398/900 [01:17&lt;02:41,  3.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 399/900 [01:17&lt;02:49,  2.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 400/900 [01:18&lt;02:46,  3.00it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 401/900 [01:18&lt;02:49,  2.95it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 402/900 [01:18&lt;02:46,  3.00it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 403/900 [01:19&lt;02:43,  3.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 404/900 [01:19&lt;02:50,  2.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 405/900 [01:19&lt;02:46,  2.97it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 406/900 [01:20&lt;02:43,  3.02it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 407/900 [01:20&lt;02:41,  3.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 408/900 [01:20&lt;02:40,  3.07it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 409/900 [01:20&lt;02:38,  3.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 410/900 [01:21&lt;02:39,  3.07it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 411/900 [01:21&lt;02:38,  3.09it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 412/900 [01:21&lt;02:37,  3.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 413/900 [01:22&lt;02:37,  3.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 414/900 [01:22&lt;02:36,  3.11it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 415/900 [01:22&lt;02:35,  3.11it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 416/900 [01:23&lt;02:35,  3.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 417/900 [01:23&lt;02:35,  3.11it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 418/900 [01:23&lt;02:35,  3.10it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 419/900 [01:24&lt;02:35,  3.09it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 420/900 [01:24&lt;02:35,  3.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 421/900 [01:24&lt;02:35,  3.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 422/900 [01:25&lt;02:34,  3.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 423/900 [01:25&lt;02:35,  3.07it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 424/900 [01:25&lt;02:35,  3.06it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 425/900 [01:26&lt;02:36,  3.04it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 426/900 [01:26&lt;02:36,  3.03it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 427/900 [01:26&lt;02:36,  3.03it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 428/900 [01:27&lt;02:36,  3.02it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 429/900 [01:27&lt;02:37,  3.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 430/900 [01:27&lt;02:36,  3.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 431/900 [01:28&lt;02:36,  3.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 432/900 [01:28&lt;02:36,  3.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 433/900 [01:28&lt;02:35,  3.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 434/900 [01:29&lt;02:35,  2.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 435/900 [01:29&lt;02:35,  2.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 436/900 [01:29&lt;02:35,  2.98it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 437/900 [01:30&lt;02:34,  2.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 438/900 [01:30&lt;02:35,  2.97it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 439/900 [01:30&lt;02:35,  2.96it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 440/900 [01:31&lt;02:35,  2.96it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 441/900 [01:31&lt;02:35,  2.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 442/900 [01:31&lt;02:35,  2.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 443/900 [01:32&lt;02:35,  2.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 444/900 [01:32&lt;02:35,  2.93it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 445/900 [01:32&lt;02:35,  2.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 446/900 [01:33&lt;02:34,  2.93it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 447/900 [01:33&lt;02:35,  2.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 448/900 [01:33&lt;02:36,  2.89it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 449/900 [01:34&lt;02:38,  2.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 450/900 [01:34&lt;02:39,  2.83it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 451/900 [01:35&lt;02:39,  2.82it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 452/900 [01:35&lt;02:37,  2.84it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 453/900 [01:35&lt;02:36,  2.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 454/900 [01:36&lt;02:36,  2.86it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 455/900 [01:36&lt;02:35,  2.87it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 456/900 [01:36&lt;02:36,  2.84it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 457/900 [01:37&lt;02:35,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 458/900 [01:37&lt;02:35,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 459/900 [01:37&lt;02:34,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 460/900 [01:38&lt;02:34,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 461/900 [01:38&lt;02:34,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 462/900 [01:38&lt;02:33,  2.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 463/900 [01:39&lt;02:33,  2.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 464/900 [01:39&lt;02:32,  2.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 465/900 [01:39&lt;02:33,  2.84it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 466/900 [01:40&lt;02:35,  2.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 467/900 [01:40&lt;02:35,  2.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 468/900 [01:41&lt;02:34,  2.79it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 469/900 [01:41&lt;02:43,  2.63it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 470/900 [01:41&lt;02:42,  2.65it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 471/900 [01:42&lt;02:39,  2.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 472/900 [01:42&lt;02:41,  2.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 473/900 [01:42&lt;02:40,  2.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 474/900 [01:43&lt;02:43,  2.61it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 475/900 [01:43&lt;02:40,  2.64it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 476/900 [01:44&lt;02:44,  2.58it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 477/900 [01:44&lt;02:41,  2.62it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 478/900 [01:44&lt;02:43,  2.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 479/900 [01:45&lt;02:44,  2.56it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 480/900 [01:45&lt;02:41,  2.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 481/900 [01:46&lt;02:46,  2.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 482/900 [01:46&lt;02:47,  2.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 483/900 [01:46&lt;02:42,  2.56it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 484/900 [01:47&lt;02:44,  2.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 485/900 [01:47&lt;02:42,  2.56it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 486/900 [01:48&lt;02:39,  2.60it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 487/900 [01:48&lt;02:36,  2.63it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 488/900 [01:48&lt;02:37,  2.61it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 489/900 [01:49&lt;02:35,  2.64it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 490/900 [01:49&lt;02:34,  2.66it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 491/900 [01:49&lt;02:33,  2.67it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 492/900 [01:50&lt;02:32,  2.68it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 493/900 [01:50&lt;02:35,  2.61it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 494/900 [01:51&lt;02:34,  2.63it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 495/900 [01:51&lt;02:33,  2.64it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 496/900 [01:51&lt;02:32,  2.65it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 497/900 [01:52&lt;02:31,  2.66it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 498/900 [01:52&lt;02:30,  2.67it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 499/900 [01:52&lt;02:30,  2.67it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 500/900 [01:53&lt;02:30,  2.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 501/900 [01:53&lt;02:29,  2.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 502/900 [01:54&lt;02:29,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 503/900 [01:54&lt;02:29,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 504/900 [01:54&lt;02:29,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 505/900 [01:55&lt;02:28,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 506/900 [01:55&lt;02:28,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 507/900 [01:55&lt;02:28,  2.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 508/900 [01:56&lt;02:28,  2.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 509/900 [01:56&lt;02:28,  2.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 510/900 [01:57&lt;02:27,  2.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 511/900 [01:57&lt;02:28,  2.63it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 512/900 [01:57&lt;02:27,  2.63it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 513/900 [01:58&lt;02:27,  2.62it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 514/900 [01:58&lt;02:27,  2.62it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 515/900 [01:58&lt;02:27,  2.62it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 516/900 [01:59&lt;02:27,  2.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 517/900 [01:59&lt;02:27,  2.60it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 518/900 [02:00&lt;02:27,  2.59it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 519/900 [02:00&lt;02:27,  2.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 520/900 [02:00&lt;02:27,  2.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 521/900 [02:01&lt;02:26,  2.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 522/900 [02:01&lt;02:26,  2.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 523/900 [02:02&lt;02:26,  2.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 524/900 [02:02&lt;02:26,  2.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 525/900 [02:02&lt;02:26,  2.56it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 526/900 [02:03&lt;02:26,  2.56it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 527/900 [02:03&lt;02:25,  2.56it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 528/900 [02:04&lt;02:26,  2.55it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 529/900 [02:04&lt;02:25,  2.54it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 530/900 [02:04&lt;02:25,  2.54it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 531/900 [02:05&lt;02:25,  2.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 532/900 [02:05&lt;02:25,  2.54it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 533/900 [02:06&lt;02:25,  2.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 534/900 [02:06&lt;02:24,  2.52it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 535/900 [02:06&lt;02:24,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 536/900 [02:07&lt;02:24,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 537/900 [02:07&lt;02:24,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 538/900 [02:08&lt;02:23,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 539/900 [02:08&lt;02:23,  2.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 540/900 [02:08&lt;02:23,  2.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 541/900 [02:09&lt;02:22,  2.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 542/900 [02:09&lt;02:22,  2.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 543/900 [02:10&lt;02:22,  2.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 544/900 [02:10&lt;02:22,  2.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 545/900 [02:10&lt;02:23,  2.48it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 546/900 [02:11&lt;02:23,  2.47it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 547/900 [02:11&lt;02:25,  2.43it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 548/900 [02:12&lt;02:29,  2.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 549/900 [02:12&lt;02:27,  2.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 550/900 [02:12&lt;02:25,  2.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 551/900 [02:13&lt;02:25,  2.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 552/900 [02:13&lt;02:24,  2.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 553/900 [02:14&lt;02:23,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 554/900 [02:14&lt;02:23,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 555/900 [02:15&lt;02:22,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 556/900 [02:15&lt;02:22,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 557/900 [02:15&lt;02:22,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 558/900 [02:16&lt;02:22,  2.40it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 559/900 [02:16&lt;02:22,  2.40it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 560/900 [02:17&lt;02:22,  2.39it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 561/900 [02:17&lt;02:21,  2.39it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 562/900 [02:17&lt;02:21,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 563/900 [02:18&lt;02:21,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 564/900 [02:18&lt;02:20,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 565/900 [02:19&lt;02:20,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 566/900 [02:19&lt;02:19,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 567/900 [02:20&lt;02:19,  2.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 568/900 [02:20&lt;02:19,  2.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 569/900 [02:20&lt;02:19,  2.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 570/900 [02:21&lt;02:19,  2.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 571/900 [02:21&lt;02:19,  2.36it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 572/900 [02:22&lt;02:19,  2.36it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 573/900 [02:22&lt;02:19,  2.35it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 574/900 [02:23&lt;02:22,  2.29it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 575/900 [02:23&lt;02:21,  2.29it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 576/900 [02:23&lt;02:20,  2.31it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 577/900 [02:24&lt;02:18,  2.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 578/900 [02:24&lt;02:18,  2.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 579/900 [02:25&lt;02:18,  2.31it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 580/900 [02:25&lt;02:18,  2.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 581/900 [02:26&lt;02:17,  2.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 582/900 [02:26&lt;02:17,  2.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 583/900 [02:26&lt;02:16,  2.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 584/900 [02:27&lt;02:16,  2.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 585/900 [02:27&lt;02:16,  2.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 586/900 [02:28&lt;02:17,  2.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 587/900 [02:28&lt;02:16,  2.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 588/900 [02:29&lt;02:15,  2.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 589/900 [02:29&lt;02:15,  2.30it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 590/900 [02:29&lt;02:15,  2.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 591/900 [02:30&lt;02:14,  2.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 592/900 [02:30&lt;02:14,  2.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 593/900 [02:31&lt;02:15,  2.26it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 594/900 [02:31&lt;02:14,  2.27it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 595/900 [02:32&lt;02:14,  2.27it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 596/900 [02:32&lt;02:14,  2.27it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 597/900 [02:33&lt;02:15,  2.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 598/900 [02:33&lt;02:14,  2.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 599/900 [02:33&lt;02:13,  2.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 600/900 [02:34&lt;02:15,  2.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 601/900 [02:34&lt;02:13,  2.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 602/900 [02:35&lt;02:12,  2.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 603/900 [02:35&lt;02:16,  2.18it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 604/900 [02:36&lt;02:14,  2.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 605/900 [02:36&lt;02:15,  2.18it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 606/900 [02:37&lt;02:17,  2.14it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 607/900 [02:37&lt;02:16,  2.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 608/900 [02:38&lt;02:13,  2.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 609/900 [02:38&lt;02:11,  2.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 610/900 [02:39&lt;02:12,  2.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 611/900 [02:39&lt;02:11,  2.20it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 612/900 [02:40&lt;02:18,  2.07it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 613/900 [02:40&lt;02:18,  2.07it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 614/900 [02:40&lt;02:18,  2.07it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 615/900 [02:41&lt;02:14,  2.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 616/900 [02:41&lt;02:14,  2.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 617/900 [02:42&lt;02:12,  2.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 618/900 [02:42&lt;02:10,  2.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 619/900 [02:43&lt;02:09,  2.17it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 620/900 [02:43&lt;02:09,  2.17it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 621/900 [02:44&lt;02:08,  2.18it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 622/900 [02:44&lt;02:07,  2.18it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 623/900 [02:45&lt;02:07,  2.17it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 624/900 [02:45&lt;02:06,  2.18it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 625/900 [02:46&lt;02:05,  2.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 626/900 [02:46&lt;02:05,  2.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 627/900 [02:46&lt;02:04,  2.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 628/900 [02:47&lt;02:04,  2.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 629/900 [02:47&lt;02:03,  2.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 630/900 [02:48&lt;02:04,  2.17it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 631/900 [02:48&lt;02:03,  2.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 632/900 [02:49&lt;02:03,  2.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 633/900 [02:49&lt;02:02,  2.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 634/900 [02:50&lt;02:01,  2.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 635/900 [02:50&lt;02:01,  2.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 636/900 [02:51&lt;02:01,  2.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 637/900 [02:51&lt;02:01,  2.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 638/900 [02:52&lt;02:00,  2.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 639/900 [02:52&lt;02:00,  2.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 640/900 [02:52&lt;02:00,  2.16it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 641/900 [02:53&lt;02:00,  2.16it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 642/900 [02:53&lt;02:00,  2.15it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 643/900 [02:54&lt;02:00,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 644/900 [02:54&lt;01:59,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 645/900 [02:55&lt;01:59,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 646/900 [02:55&lt;01:58,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 647/900 [02:56&lt;01:58,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 648/900 [02:56&lt;01:58,  2.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 649/900 [02:57&lt;01:57,  2.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 650/900 [02:57&lt;01:57,  2.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 651/900 [02:58&lt;01:56,  2.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 652/900 [02:58&lt;01:56,  2.13it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 653/900 [02:59&lt;01:56,  2.13it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 654/900 [02:59&lt;01:56,  2.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 655/900 [02:59&lt;01:55,  2.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 656/900 [03:00&lt;01:55,  2.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 657/900 [03:00&lt;01:54,  2.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 658/900 [03:01&lt;01:54,  2.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 659/900 [03:01&lt;01:54,  2.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 660/900 [03:02&lt;01:53,  2.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 661/900 [03:02&lt;01:53,  2.11it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 662/900 [03:03&lt;01:53,  2.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 663/900 [03:03&lt;01:52,  2.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 664/900 [03:04&lt;01:52,  2.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 665/900 [03:04&lt;01:52,  2.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 666/900 [03:05&lt;01:52,  2.09it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 667/900 [03:05&lt;01:51,  2.08it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 668/900 [03:06&lt;01:51,  2.08it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 669/900 [03:06&lt;01:51,  2.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 670/900 [03:07&lt;01:51,  2.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 671/900 [03:07&lt;01:50,  2.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 672/900 [03:08&lt;01:50,  2.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 673/900 [03:08&lt;01:49,  2.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 674/900 [03:09&lt;01:49,  2.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 675/900 [03:09&lt;01:49,  2.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 676/900 [03:10&lt;01:49,  2.05it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 677/900 [03:10&lt;01:48,  2.05it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 678/900 [03:11&lt;01:48,  2.05it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 679/900 [03:11&lt;01:47,  2.05it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 680/900 [03:12&lt;01:47,  2.05it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 681/900 [03:12&lt;01:46,  2.05it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 682/900 [03:13&lt;01:46,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 683/900 [03:13&lt;01:46,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 684/900 [03:13&lt;01:45,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 685/900 [03:14&lt;01:45,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 686/900 [03:14&lt;01:45,  2.03it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 687/900 [03:15&lt;01:44,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 688/900 [03:15&lt;01:44,  2.04it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 689/900 [03:16&lt;01:44,  2.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 690/900 [03:16&lt;01:43,  2.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 691/900 [03:17&lt;01:43,  2.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 692/900 [03:17&lt;01:43,  2.02it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 693/900 [03:18&lt;01:43,  2.01it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 694/900 [03:18&lt;01:42,  2.01it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 695/900 [03:19&lt;01:42,  2.00it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 696/900 [03:19&lt;01:41,  2.00it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 697/900 [03:20&lt;01:41,  2.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 698/900 [03:20&lt;01:41,  2.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 699/900 [03:21&lt;01:40,  2.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/900 [03:21&lt;01:40,  2.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 701/900 [03:22&lt;01:39,  1.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 702/900 [03:22&lt;01:39,  1.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 703/900 [03:23&lt;01:38,  1.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 704/900 [03:23&lt;01:38,  1.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 705/900 [03:24&lt;01:38,  1.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 706/900 [03:24&lt;01:38,  1.98it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 707/900 [03:25&lt;01:37,  1.98it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 708/900 [03:25&lt;01:37,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 709/900 [03:26&lt;01:36,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 710/900 [03:27&lt;01:36,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 711/900 [03:27&lt;01:35,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 712/900 [03:28&lt;01:35,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 713/900 [03:28&lt;01:34,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 714/900 [03:29&lt;01:34,  1.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 715/900 [03:29&lt;01:34,  1.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 716/900 [03:30&lt;01:33,  1.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 717/900 [03:30&lt;01:33,  1.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 718/900 [03:31&lt;01:33,  1.95it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 719/900 [03:31&lt;01:32,  1.95it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 720/900 [03:32&lt;01:32,  1.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 721/900 [03:32&lt;01:32,  1.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 722/900 [03:33&lt;01:31,  1.95it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 723/900 [03:33&lt;01:30,  1.95it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 724/900 [03:34&lt;01:30,  1.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 725/900 [03:34&lt;01:30,  1.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 726/900 [03:35&lt;01:29,  1.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 727/900 [03:35&lt;01:29,  1.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 728/900 [03:36&lt;01:28,  1.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 729/900 [03:36&lt;01:28,  1.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 730/900 [03:37&lt;01:28,  1.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 731/900 [03:37&lt;01:27,  1.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 732/900 [03:38&lt;01:27,  1.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 733/900 [03:38&lt;01:27,  1.91it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 734/900 [03:39&lt;01:27,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 735/900 [03:39&lt;01:26,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 736/900 [03:40&lt;01:26,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 737/900 [03:40&lt;01:25,  1.91it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 738/900 [03:41&lt;01:25,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 739/900 [03:42&lt;01:24,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 740/900 [03:42&lt;01:24,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 741/900 [03:43&lt;01:23,  1.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 742/900 [03:43&lt;01:23,  1.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 743/900 [03:44&lt;01:22,  1.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 744/900 [03:44&lt;01:22,  1.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 745/900 [03:45&lt;01:21,  1.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 746/900 [03:45&lt;01:21,  1.89it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 747/900 [03:46&lt;01:21,  1.89it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 748/900 [03:46&lt;01:21,  1.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 749/900 [03:47&lt;01:20,  1.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 750/900 [03:47&lt;01:20,  1.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 751/900 [03:48&lt;01:19,  1.87it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 752/900 [03:48&lt;01:19,  1.87it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 753/900 [03:49&lt;01:18,  1.87it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 754/900 [03:50&lt;01:18,  1.87it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 755/900 [03:50&lt;01:17,  1.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 756/900 [03:51&lt;01:17,  1.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 757/900 [03:51&lt;01:16,  1.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 758/900 [03:52&lt;01:16,  1.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 759/900 [03:52&lt;01:15,  1.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 760/900 [03:53&lt;01:15,  1.86it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 761/900 [03:53&lt;01:14,  1.86it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 762/900 [03:54&lt;01:14,  1.85it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 763/900 [03:54&lt;01:13,  1.85it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 764/900 [03:55&lt;01:13,  1.85it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 765/900 [03:55&lt;01:13,  1.83it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 766/900 [03:56&lt;01:12,  1.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 767/900 [03:57&lt;01:12,  1.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 768/900 [03:57&lt;01:11,  1.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 769/900 [03:58&lt;01:12,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 770/900 [03:58&lt;01:11,  1.82it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 771/900 [03:59&lt;01:11,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 772/900 [03:59&lt;01:10,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 773/900 [04:00&lt;01:10,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 774/900 [04:00&lt;01:09,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 775/900 [04:01&lt;01:08,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 776/900 [04:02&lt;01:08,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 777/900 [04:02&lt;01:07,  1.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 778/900 [04:03&lt;01:07,  1.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 779/900 [04:03&lt;01:06,  1.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 780/900 [04:04&lt;01:06,  1.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 781/900 [04:04&lt;01:05,  1.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 782/900 [04:05&lt;01:05,  1.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 783/900 [04:05&lt;01:04,  1.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 784/900 [04:06&lt;01:04,  1.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 785/900 [04:07&lt;01:03,  1.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 786/900 [04:07&lt;01:03,  1.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 787/900 [04:08&lt;01:02,  1.81it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 788/900 [04:08&lt;01:02,  1.80it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 789/900 [04:09&lt;01:01,  1.80it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 790/900 [04:09&lt;01:01,  1.80it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 791/900 [04:10&lt;01:00,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 792/900 [04:10&lt;01:00,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 793/900 [04:11&lt;00:59,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 794/900 [04:12&lt;00:59,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 795/900 [04:12&lt;00:58,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 796/900 [04:13&lt;00:58,  1.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 797/900 [04:13&lt;00:57,  1.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 798/900 [04:14&lt;00:57,  1.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 799/900 [04:14&lt;00:56,  1.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 800/900 [04:15&lt;00:56,  1.78it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 801/900 [04:15&lt;00:55,  1.77it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 802/900 [04:16&lt;00:55,  1.77it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 803/900 [04:17&lt;00:54,  1.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 804/900 [04:17&lt;00:54,  1.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 805/900 [04:18&lt;00:53,  1.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 806/900 [04:18&lt;00:53,  1.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 807/900 [04:19&lt;00:52,  1.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 808/900 [04:19&lt;00:52,  1.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 809/900 [04:20&lt;00:51,  1.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 810/900 [04:21&lt;00:51,  1.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 811/900 [04:21&lt;00:50,  1.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 812/900 [04:22&lt;00:50,  1.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 813/900 [04:22&lt;00:49,  1.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 814/900 [04:23&lt;00:49,  1.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 815/900 [04:23&lt;00:48,  1.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 816/900 [04:24&lt;00:48,  1.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 817/900 [04:25&lt;00:47,  1.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 818/900 [04:25&lt;00:46,  1.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 819/900 [04:26&lt;00:46,  1.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 820/900 [04:26&lt;00:45,  1.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 821/900 [04:27&lt;00:45,  1.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 822/900 [04:27&lt;00:45,  1.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 823/900 [04:28&lt;00:44,  1.73it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 824/900 [04:29&lt;00:43,  1.73it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 825/900 [04:29&lt;00:43,  1.73it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 826/900 [04:30&lt;00:42,  1.73it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 827/900 [04:30&lt;00:42,  1.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 828/900 [04:31&lt;00:41,  1.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 829/900 [04:32&lt;00:41,  1.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 830/900 [04:32&lt;00:40,  1.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 831/900 [04:33&lt;00:40,  1.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 832/900 [04:33&lt;00:39,  1.72it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 833/900 [04:34&lt;00:39,  1.72it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 834/900 [04:34&lt;00:38,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 835/900 [04:35&lt;00:37,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 836/900 [04:36&lt;00:37,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 837/900 [04:36&lt;00:36,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 838/900 [04:37&lt;00:36,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 839/900 [04:37&lt;00:35,  1.70it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 840/900 [04:38&lt;00:35,  1.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 841/900 [04:39&lt;00:34,  1.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 842/900 [04:39&lt;00:33,  1.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 843/900 [04:40&lt;00:33,  1.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 844/900 [04:40&lt;00:32,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 845/900 [04:41&lt;00:32,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 846/900 [04:41&lt;00:31,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 847/900 [04:42&lt;00:31,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 848/900 [04:43&lt;00:30,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 849/900 [04:43&lt;00:30,  1.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 850/900 [04:44&lt;00:29,  1.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 851/900 [04:44&lt;00:29,  1.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 852/900 [04:45&lt;00:28,  1.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 853/900 [04:46&lt;00:28,  1.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 854/900 [04:46&lt;00:27,  1.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 855/900 [04:47&lt;00:27,  1.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 856/900 [04:47&lt;00:26,  1.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 857/900 [04:48&lt;00:25,  1.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 858/900 [04:49&lt;00:25,  1.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 859/900 [04:49&lt;00:24,  1.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 860/900 [04:50&lt;00:23,  1.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 861/900 [04:50&lt;00:23,  1.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 862/900 [04:51&lt;00:22,  1.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 863/900 [04:52&lt;00:22,  1.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 864/900 [04:52&lt;00:21,  1.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 865/900 [04:53&lt;00:21,  1.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 866/900 [04:53&lt;00:20,  1.65it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 867/900 [04:54&lt;00:19,  1.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 868/900 [04:55&lt;00:19,  1.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 869/900 [04:55&lt;00:18,  1.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 870/900 [04:56&lt;00:18,  1.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 871/900 [04:57&lt;00:17,  1.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 872/900 [04:57&lt;00:17,  1.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 873/900 [04:58&lt;00:16,  1.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 874/900 [04:58&lt;00:15,  1.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 875/900 [04:59&lt;00:15,  1.63it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 876/900 [05:00&lt;00:14,  1.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 877/900 [05:00&lt;00:14,  1.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 878/900 [05:01&lt;00:13,  1.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 879/900 [05:01&lt;00:12,  1.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 880/900 [05:02&lt;00:12,  1.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 881/900 [05:03&lt;00:11,  1.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 882/900 [05:03&lt;00:11,  1.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 883/900 [05:04&lt;00:10,  1.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 884/900 [05:05&lt;00:09,  1.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 885/900 [05:05&lt;00:09,  1.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 886/900 [05:06&lt;00:08,  1.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 887/900 [05:06&lt;00:08,  1.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 888/900 [05:07&lt;00:07,  1.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 889/900 [05:08&lt;00:06,  1.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 890/900 [05:08&lt;00:06,  1.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 891/900 [05:09&lt;00:05,  1.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 892/900 [05:09&lt;00:04,  1.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 893/900 [05:10&lt;00:04,  1.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 894/900 [05:11&lt;00:03,  1.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 895/900 [05:11&lt;00:03,  1.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 896/900 [05:12&lt;00:02,  1.61it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 897/900 [05:13&lt;00:01,  1.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 898/900 [05:13&lt;00:01,  1.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 899/900 [05:14&lt;00:00,  1.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:14&lt;00:00,  1.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:14&lt;00:00,  2.86it/s]\n\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(M_values, var_bs_values, marker='o', color='red')\nplt.xlabel('Nombre de simulations')\nplt.ylabel('VaR bootstrap')\nplt.title(\"Variation de la VaR bootstrap en fonction du nombre de simulations\")\n\nText(0.5, 1.0, 'Variation de la VaR bootstrap en fonction du nombre de simulations')\n\n\n\n\n\n\n\n\n\nPour la taille de l‚Äôechantillon bootstrap, nous allons prendre M=8000 √©tant donn√© que la courbe semble se stabiliser √† partir de cette valeur. Avec ce choix, la VaR estim√© est de 4.11%% avec un intervalle de confiance √† 5% de [4.05%, 4.09%]. De plus, en ce qui concerne la VaR historique, nous constatons que l‚Äôestimation est contenu dans l‚Äôintervalle de confiance.\n\nM=8000\nvar_bs_train, lower_ic,upper_ic = bootstrap_var(data_train, alpha=alpha, M=M)\nprint(f\"La VaR bootstrap pour h=1j et alpha=0.99 est : {var_bs_train:.4%}\")\nprint(f\"L'intervalle de confiance est : [{lower_ic:.4%}, {upper_ic:.4%}]\")\n\nLa VaR bootstrap pour h=1j et alpha=0.99 est : 4.1065%\nL'intervalle de confiance est : [4.0536%, 4.0939%]\n\n\n\n\nII.2.3. Backtest\nPour l‚Äôexercice de backtest, il s‚Äôagit de : 1. D√©terminer si la proportion \\(p\\) de violations de la VaR est coh√©rente avec le niveau de confiance, i.e.¬†√©gale √† \\(1-\\alpha\\). Cela permet de v√©rifier si la mesure de risque est bien calibr√©e. Pour cela, nous pouvons avoir recours √† un test de proportion ou un test de ratio de vraisemblance.\n**Unconditional coverage test** :\nSoit I la variable indicatrice de violation de la VaR, i.e. $I=1$ si la perte est sup√©rieure √† la VaR, et $I=0$ sinon. La proportion de violations de la VaR est donn√©e par :\n\n$$p = \\frac{1}{n} \\sum_{i=1}^{n} I_i = \\frac{Z}{n}$$\n\nSous H0, i.e. p=1-$\\alpha$, Z $\\sim$ Binomiale(n, 1-$\\alpha$). En supposant que n est suffisamment grand, on peut approximer Z par une loi normale. Ainsi donc :\n$$\\frac{Z - n (1-\\alpha)}{\\sqrt{\\alpha (1-\\alpha) n}} \\sim \\mathcal{N}(n(1-\\alpha), n\\alpha(1-\\alpha))$$\n\nSous cette hypoth√®se asymptotique, on peut calculer la statistique du ratio de vraisemblance suivant :\n$$LR = -2 ln \\left( \\frac{L(H1)}{L(H0)} \\right) =-2 ln \\left( 1- (1-\\alpha))^{n-e}(1-\\alpha)^e \\right) + 2 ln \\left( (1-\\frac{e}{n})^{n-e} (\\frac{e}{n})^e  \\right)  \\sim \\chi^2(1)$$\n\no√π e est le nombre de violations de la VaR. On rejette H0 si LR &gt; $\\chi^2(1-\\alpha)$.\n\n**Test de proportion** :\n$$\nH_0 : p = p_0 = 1-\\alpha \\\\ H_1 : p &gt; 1-\\alpha\n$$\nOn peut √©galement utiliser un test binomial pour tester si la proportion de violations de la VaR est √©gale √† $1-\\alpha$. On peut calculer la statistique du test suivant :\n$$Z = \\frac{p - p_0}{\\sqrt{p_0 (1-p_0) / n}} \\sim \\mathcal{N}(0,1)$$\n\nOn rejette H0 si Z &gt; $p_0+ \\phi^{-1}(1-\\alpha) \\sqrt{p_0 (1-p_0)/n}$, o√π $\\phi$ est la quantile de la loi normale standard.\n\nD√©terminer si, lorsqu‚Äôil y en a, les violations de VaR √† deux diff√©rents jours sont ind√©pendantes. Cela permet si la mesure de risque est capable de r√©agir aux chocs de march√© affectant la volatilit√© des rendements. Pour cela, nous utilisons un conditional coverage test.\nConditional coverage test : y revenir\n\n\nimport scipy.stats as stats\n\n# Objectif : impl√©menter une fonction calculant le nombre d'exception sur l'√©chantillon test\ndef exceptions(data, var):\n    \"\"\"\n    Calcul du nombre d'exception\n    data : les rendements logarithmiques\n    var : la VaR\n    \"\"\"\n    return np.sum(data &lt; -var)\n\n\n# Objectif : test de proportion binomiale\n\ndef binomial_test(n, p, p0 = 0.01, alpha=0.05):\n    \"\"\"\n    Test de proportion binomiale\n    H0 : p = p0\n    H1 : p &gt; p0\n    n : le nombre d'essais\n    p : la proportion\n    alpha : le niveau de confiance\n    \"\"\"\n\n    z = (p - p0) / np.sqrt(p0 * (1 - p0) / n)\n    #reject_zone = p0 + stats.norm.ppf(1 - alpha) * np.sqrt(p0 * (1 - p0) / n)\n    p_value = 1 - stats.norm.cdf(z)\n    reject = p_value &lt; alpha\n\n    # Calcul des IC\n    lower = p - stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n    upper = p + stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n\n    return p_value, reject, lower, upper\n\n# Unconditionnal coverage test ==&gt; to do.\n\n\n# Backtest de la VaR historique\n\nexceptions_test = exceptions(data_test, var_hist_train)\nprint(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {exceptions_test}\")\n\nprint(\"=\"*80)\nn = len(data_test)\np = exceptions_test / n\np_value, reject, lower,upper = binomial_test(n, p)\nprint(f\"H0 : le nombre d'exceptions est inf√©rieur ou √©gale √† {1-alpha:.2%}\")\nprint(f\"IC : [{lower:.2%},{upper:.2%}]\")\nprint(f\"La p-value du test de proportion binomiale est : {p_value:.4f}\")\nprint(f\"Rejet de l'hypoth√®se nulle : {reject}\")\nprint(\"=\"*80)\n\nLe nombre d'exceptions sur l'√©chantillon de test est : 0\n================================================================================\nH0 : le nombre d'exceptions est inf√©rieur ou √©gale √† 1.00%\nIC : [0.00%,0.00%]\nLa p-value du test de proportion binomiale est : 0.9925\nRejet de l'hypoth√®se nulle : False\n================================================================================"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.3. VaR param√©trique",
    "text": "II.3. VaR param√©trique\n\nII.3.1. Validation ex-ante\nVisuellement, les donn√©es ne semblent pas suivre une loi normale. En effet, les quantiles th√©oriques d‚Äôune loi normale ne collent pas avec les quantiles empiriques des rendements. Cela peut √™tre d√ª √† la pr√©sence de queues √©paisses observables sur l‚Äôestimation de la densit√© des rendements sur l‚Äô√©chantillon d‚Äôapprentissage, de pics, de clusters de volatilit√© que nous avons observ√©es plus haut. De plus, le skewness est n√©gatif ce qui indique une asym√©trie n√©gative des rendements. Enfin, le kurtosis est sup√©rieur √† 3, ce qui indique une distribution leptokurtique des rendements.\nNous allons tout de m√™me impl√©menter une VaR gaussienne pour voir comment elle se comporte dans le backtest.\n\n# Test visuel d'ad√©quation de la loi normale\n\n# Cr√©er un Q-Q plot\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(data_train, dist=\"norm\", plot=ax)\n\n# Personnalisation du graphique\nax.set_title(\"Q-Q Plot (Normal Distribution)\")\nax.set_xlabel(\"Theoretical Quantiles\")\nax.set_ylabel(\"Sample Quantiles\")\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\n# Densit√© de l'echantillon train et l'√©chantillon de test\n\nplt.figure(figsize=(10, 4))\ndata_train.plot(kind='kde', label='Train', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Densit√© de l'√©chantillon d'entrainement\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Skewness et kurtosis\nprint(\"=\"*80)\nprint(\"Skewness de l'√©chantillon d'entrainement : \", data_train.skew())\nprint(\"Kurtosis de l'√©chantillon d'entrainement : \", data_train.kurt())\nprint(\"=\"*80)\n\n================================================================================\nSkewness de l'√©chantillon d'entrainement :  -0.2981820421484688\nKurtosis de l'√©chantillon d'entrainement :  7.353960005618779\n================================================================================\n\n\n\nfrom scipy.stats import kstest\n\n# Test de Kolmogorov-Smirnov\nks_stat, ks_p_value = kstest(data_train, 'norm')\nprint(\"=\"*80)\nprint(\"H0 : Les donn√©es suivent une loi normale\")\nprint(f\"Statistique de test : {ks_stat:.4f}\")\nprint(f\"P-value : {ks_p_value:.4f}\")\nprint(\"=\"*80)\n\n================================================================================\nH0 : Les donn√©es suivent une loi normale\nStatistique de test : 0.4775\nP-value : 0.0000\n================================================================================\n\n\n\n\nII.3.2. Impl√©mentation de la VaR\n\na. M√©thode scaling\n\n# Objectif : √©crire une fonction qui calcule la VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n\nvar_gauss_train = gaussian_var(data_train, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=1j et alpha={alpha} est : {var_gauss_train:.4%}\")\nprint(f\"La VaR historique pour h=10j et alpha={alpha} est : {np.sqrt(10)*var_gauss_train:.4%}\")\n\nLa VaR gaussienne pour h=1j et alpha=0.99 est : 3.2302%\nLa VaR historique pour h=10j et alpha=0.99 est : 10.2148%\n\n\n\n\nb. M√©thode de diffusion d‚Äôun actif\nPour calculer la VaR gaussienne √† 10 jours par m√©thode de diffusion d‚Äôun actif, nous allons suivre les √©tapes suivantes :\nL‚Äô√©volution du prix d‚Äôun actif suit un processus de type mouvement brownien g√©om√©trique : \\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t &lt;=&gt; S_t = S_{t-1} e^{(\\mu - \\frac{1}{2} \\sigma^2) + \\sigma W_t}\n\\] o√π $ S_t$ est le prix de l‚Äôactif √† l‚Äôinstant$ t$, $ $ est le rendement moyen estim√© (drift), $ $ est la volatilit√© du rendement, $ dW_t$ est un mouvement brownien standard.\nOn peut de ce fait calculer plusieurs trajectoires de rendements de \\(S_0\\) et \\(S_{10}\\), puis calculer la VaR √† partir de la s√©rie des rendements $ r_{10j} = (S_{10} / S_{0}) $ obtenus avec ces trajectoires.\nEn utilisant la m√©thode de scaling et la m√©thode de diffusion, nous obtenons sensiblement la m√™me VaR.\n\nimport numpy as np\n\nmu = np.mean(data_train)\nsigma = np.std(data_train)\n\nreturn_sim = []\nT = 10\nM = 1000\nS0 = train['Close'].iloc[-1]\n\nfor i in range(M):\n    S = np.zeros(T+1)\n    S[0] = S0\n    for t in range(1,T+1):\n        Wt = np.random.normal()\n        S[t] = S[t-1] * np.exp((mu - 0.5 * sigma**2) + sigma * Wt)\n\n    return_sim.append(np.log(S[T]/S0))\n\nvar_gauss_diffusion = gaussian_var(return_sim, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=10j et alpha={alpha} est : {var_gauss_diffusion:.4%}\")\n\nLa VaR gaussienne pour h=10j et alpha=0.99 est : 10.0053%\n\n\n\n\nc.¬†M√©thode EWMA\nLa VaR EWMA est une m√©thode qui permet de calculer la VaR en surpond√©rant les rendements les plus r√©cents. Cela permet de donner plus de poids aux rendements les plus r√©cents, et donc de mieux capturer les changements de volatilit√©. La VaR EWMA est donn√©e par la formule suivante :\n\\[\nVaR_{t+1} = \\mu + \\sigma \\times z_{\\alpha}\n\\]\n\n# VaR la m√©thode EWMA (Exponential Weighting Moving Average)\n\ndef gaussian_var_ewma(data, alpha, lambda_=0.94):\n    \"\"\"\n    Calcul de la VaR gaussienne EWMA\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    lambda_ : le param√®tre de lissage\n    \"\"\"\n\n    weights = np.array([(1-lambda_)*(lambda_**i) for i in range(len(data))])\n    weights = weights / np.sum(weights)\n\n    mu = np.sum(data[::-1] * weights)\n    sigma = np.sqrt(np.sum(weights * (data[::-1] - mu)**2))\n\n    return -(mu + sigma * stats.norm.ppf(1 - alpha)), mu, sigma\n\n# y revenir\n\n\nlambdas = [0.9, 0.95, 0.99]\nalpha = 0.99\nimport scipy.stats as stats\nfor l in lambdas:\n    print(\"=\"*80)\n    print(\"Lambda : \", l)\n    print(\"-\"*15)\n    var_gauss_emwa, mu, sigma = gaussian_var_ewma(data_train, alpha=alpha, lambda_=l)\n    print(f\"La VaR gaussienne EWMA pour h=1j et alpha={alpha} est : {var_gauss_emwa:.4%}\")\n    print(f\"La moyenne est : {mu:.4%}\")\n    print(f\"L'√©cart-type est : {sigma:.4%}\")\n\n    n_exceptions = exceptions(data_test, var_gauss_emwa)\n    print(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {n_exceptions}\")\n\n================================================================================\nLambda :  0.9\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.3751%\nLa moyenne est : 0.1898%\nL'√©cart-type est : 1.1025%\nLe nombre d'exceptions sur l'√©chantillon de test est : 5\n================================================================================\nLambda :  0.95\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.8969%\nLa moyenne est : 0.0735%\nL'√©cart-type est : 1.2768%\nLe nombre d'exceptions sur l'√©chantillon de test est : 4\n================================================================================\nLambda :  0.99\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 3.3511%\nLa moyenne est : -0.0321%\nL'√©cart-type est : 1.4267%\nLe nombre d'exceptions sur l'√©chantillon de test est : 1\n\n\nAvec la m√©thode EWMA, nous observons une que la VaR diminue plus \\(\\lambda\\) augmente. Cela est d√ª au fait que plus \\(\\lambda\\) est grand, plus les rendements les plus r√©cents sont surpond√©r√©s, et donc la volatilit√© est plus faible, en raison de la fin de la p√©riode d‚Äôapprentissage."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.4. VaR skew-student",
    "text": "II.4. VaR skew-student\n\nII.4.1. Validation ex-ante\n\n# Ecrire une fonction permettant d‚Äôestimer les param√®tres d‚Äôune loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n\n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n\n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(data_train)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0],\n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.0023247899830732325\nSigma :  0.008823850410822238\nGamma :  -0.23183614178494735\nNu :  2.9618421112435653\n================================================================================\n\n\n\n# Superposition de la densit√© th√©orique et des donn√©es\n\nx_values = np.linspace(min(data_train), max(data_train), 1000)\n\ntheoretical_density = skew_student_pdf(x_values, **params_sstd)\nplt.figure(figsize = (10,4))\nplt.hist(data_train, bins=30, density=True, alpha=0.5, label='Donn√©es empiriques')\nplt.plot(x_values, theoretical_density, label='Densit√© Skew Student', color='red')\n# Densit√© normale\nplt.plot(x_values, stats.norm.pdf(x_values, np.mean(data_train), np.std(data_train)), label='Densit√© normale', color='blue')\nplt.xlabel('Rendements logarithmiques')\nplt.ylabel('Densit√©')\nplt.title(\"Comparaison entre les donn√©es et la densit√© th√©orique d'une loi de Skew Student\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densit√© th√©orique de la skew-student semble bien s‚Äôajuster aux donn√©es. La loi skew-student de param√®tres (\\(\\mu = 0.002, \\sigma = 0.009, \\gamma = -0.23, \\nu = 2.96\\)). Le \\(\\mu\\) est le rendement moyen, \\(\\sigma\\) est l‚Äô√©cart-type, \\(\\gamma\\) est le coefficient d‚Äôasym√©trie et \\(\\nu\\) est le degr√© de libert√©. Le skewness est n√©gatif, ce qui indique une asym√©trie n√©gative des rendements, comme ce qu‚Äôon a observ√© plus haut.\nNous allons appuyer cette validation en utilisant le QQ-plot. La fonction quantile d‚Äôune loi skew-student n‚Äôest pas analytique. Pour ce faire, nous allons construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l‚Äôinverse de cette fonction de repartition. Nous allons ensuite comparer les quantiles th√©oriques de la skew-student avec les quantiles empiriques des rendements.\nEn observant le QQ-plot, on constate que les quantiles th√©oriques de la skew-student collent bien avec les quantiles empiriques des rendements. Cela confirme que la skew-student est une bonne approximation de la distribution des rendements. Pour une validation plus rigoureuse, on peut utiliser un test de Kolmogorov-Smirnov pour tester si les rendements suivent une loi skew-student.\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\nquantiles_empiriques = np.quantile(data_train, niveaux_quantiles)\nquantiles_theoriques = [skew_student_quantile(alpha,**params_sstd) for alpha in tqdm(niveaux_quantiles)]\n\n  0%|          | 0/999 [00:00&lt;?, ?it/s]  0%|          | 1/999 [00:00&lt;04:09,  3.99it/s]  0%|          | 2/999 [00:00&lt;04:01,  4.13it/s]  0%|          | 3/999 [00:00&lt;04:07,  4.02it/s]  0%|          | 4/999 [00:01&lt;04:47,  3.47it/s]  1%|          | 5/999 [00:01&lt;05:03,  3.28it/s]  1%|          | 6/999 [00:01&lt;05:04,  3.26it/s]  1%|          | 7/999 [00:02&lt;05:03,  3.26it/s]  1%|          | 8/999 [00:02&lt;05:09,  3.21it/s]  1%|          | 9/999 [00:02&lt;05:07,  3.22it/s]  1%|          | 10/999 [00:03&lt;05:12,  3.16it/s]  1%|          | 11/999 [00:03&lt;05:03,  3.25it/s]  1%|          | 12/999 [00:03&lt;05:08,  3.19it/s]  1%|‚ñè         | 13/999 [00:03&lt;05:15,  3.13it/s]  1%|‚ñè         | 14/999 [00:04&lt;05:22,  3.05it/s]  2%|‚ñè         | 15/999 [00:04&lt;05:07,  3.20it/s]  2%|‚ñè         | 16/999 [00:04&lt;05:09,  3.18it/s]  2%|‚ñè         | 17/999 [00:05&lt;05:10,  3.17it/s]  2%|‚ñè         | 18/999 [00:05&lt;05:13,  3.13it/s]  2%|‚ñè         | 19/999 [00:05&lt;05:18,  3.08it/s]  2%|‚ñè         | 20/999 [00:06&lt;05:10,  3.15it/s]  2%|‚ñè         | 21/999 [00:06&lt;05:10,  3.15it/s]  2%|‚ñè         | 22/999 [00:06&lt;05:17,  3.08it/s]  2%|‚ñè         | 23/999 [00:07&lt;05:10,  3.14it/s]  2%|‚ñè         | 24/999 [00:07&lt;05:07,  3.17it/s]  3%|‚ñé         | 25/999 [00:07&lt;05:08,  3.15it/s]  3%|‚ñé         | 26/999 [00:08&lt;05:14,  3.09it/s]  3%|‚ñé         | 27/999 [00:08&lt;05:12,  3.11it/s]  3%|‚ñé         | 28/999 [00:08&lt;05:12,  3.11it/s]  3%|‚ñé         | 29/999 [00:09&lt;05:19,  3.04it/s]  3%|‚ñé         | 30/999 [00:09&lt;05:15,  3.07it/s]  3%|‚ñé         | 31/999 [00:09&lt;05:23,  2.99it/s]  3%|‚ñé         | 32/999 [00:10&lt;05:31,  2.91it/s]  3%|‚ñé         | 33/999 [00:10&lt;05:36,  2.87it/s]  3%|‚ñé         | 34/999 [00:10&lt;05:40,  2.83it/s]  4%|‚ñé         | 35/999 [00:11&lt;05:44,  2.80it/s]  4%|‚ñé         | 36/999 [00:11&lt;05:56,  2.70it/s]  4%|‚ñé         | 37/999 [00:12&lt;06:03,  2.65it/s]  4%|‚ñç         | 38/999 [00:12&lt;06:13,  2.58it/s]  4%|‚ñç         | 39/999 [00:12&lt;06:19,  2.53it/s]  4%|‚ñç         | 40/999 [00:13&lt;06:07,  2.61it/s]  4%|‚ñç         | 41/999 [00:13&lt;06:08,  2.60it/s]  4%|‚ñç         | 42/999 [00:13&lt;06:12,  2.57it/s]  4%|‚ñç         | 43/999 [00:14&lt;06:13,  2.56it/s]  4%|‚ñç         | 44/999 [00:14&lt;06:09,  2.59it/s]  5%|‚ñç         | 45/999 [00:15&lt;06:03,  2.63it/s]  5%|‚ñç         | 46/999 [00:15&lt;05:58,  2.66it/s]  5%|‚ñç         | 47/999 [00:15&lt;05:45,  2.75it/s]  5%|‚ñç         | 48/999 [00:16&lt;05:49,  2.72it/s]  5%|‚ñç         | 49/999 [00:16&lt;06:01,  2.62it/s]  5%|‚ñå         | 50/999 [00:16&lt;05:54,  2.68it/s]  5%|‚ñå         | 51/999 [00:17&lt;05:52,  2.69it/s]  5%|‚ñå         | 52/999 [00:17&lt;05:54,  2.67it/s]  5%|‚ñå         | 53/999 [00:18&lt;05:54,  2.67it/s]  5%|‚ñå         | 54/999 [00:18&lt;05:42,  2.76it/s]  6%|‚ñå         | 55/999 [00:18&lt;05:40,  2.78it/s]  6%|‚ñå         | 56/999 [00:19&lt;05:44,  2.73it/s]  6%|‚ñå         | 57/999 [00:19&lt;05:54,  2.66it/s]  6%|‚ñå         | 58/999 [00:19&lt;05:51,  2.68it/s]  6%|‚ñå         | 59/999 [00:20&lt;05:42,  2.75it/s]  6%|‚ñå         | 60/999 [00:20&lt;05:45,  2.71it/s]  6%|‚ñå         | 61/999 [00:21&lt;05:42,  2.74it/s]  6%|‚ñå         | 62/999 [00:21&lt;05:49,  2.68it/s]  6%|‚ñã         | 63/999 [00:21&lt;05:50,  2.67it/s]  6%|‚ñã         | 64/999 [00:22&lt;05:54,  2.64it/s]  7%|‚ñã         | 65/999 [00:22&lt;05:47,  2.69it/s]  7%|‚ñã         | 66/999 [00:22&lt;05:45,  2.70it/s]  7%|‚ñã         | 67/999 [00:23&lt;05:49,  2.66it/s]  7%|‚ñã         | 68/999 [00:23&lt;05:48,  2.67it/s]  7%|‚ñã         | 69/999 [00:24&lt;05:58,  2.59it/s]  7%|‚ñã         | 70/999 [00:24&lt;05:54,  2.62it/s]  7%|‚ñã         | 71/999 [00:24&lt;06:05,  2.54it/s]  7%|‚ñã         | 72/999 [00:25&lt;05:56,  2.60it/s]  7%|‚ñã         | 73/999 [00:25&lt;05:58,  2.59it/s]  7%|‚ñã         | 74/999 [00:25&lt;05:55,  2.60it/s]  8%|‚ñä         | 75/999 [00:26&lt;05:56,  2.59it/s]  8%|‚ñä         | 76/999 [00:26&lt;06:00,  2.56it/s]  8%|‚ñä         | 77/999 [00:27&lt;05:56,  2.59it/s]  8%|‚ñä         | 78/999 [00:27&lt;05:59,  2.56it/s]  8%|‚ñä         | 79/999 [00:27&lt;06:04,  2.53it/s]  8%|‚ñä         | 80/999 [00:28&lt;06:01,  2.54it/s]  8%|‚ñä         | 81/999 [00:28&lt;05:58,  2.56it/s]  8%|‚ñä         | 82/999 [00:29&lt;05:57,  2.56it/s]  8%|‚ñä         | 83/999 [00:29&lt;05:51,  2.61it/s]  8%|‚ñä         | 84/999 [00:29&lt;05:48,  2.63it/s]  9%|‚ñä         | 85/999 [00:30&lt;05:41,  2.67it/s]  9%|‚ñä         | 86/999 [00:30&lt;05:41,  2.67it/s]  9%|‚ñä         | 87/999 [00:30&lt;05:47,  2.63it/s]  9%|‚ñâ         | 88/999 [00:31&lt;05:48,  2.62it/s]  9%|‚ñâ         | 89/999 [00:31&lt;05:43,  2.65it/s]  9%|‚ñâ         | 90/999 [00:32&lt;05:34,  2.72it/s]  9%|‚ñâ         | 91/999 [00:32&lt;05:35,  2.70it/s]  9%|‚ñâ         | 92/999 [00:32&lt;05:31,  2.73it/s]  9%|‚ñâ         | 93/999 [00:33&lt;05:22,  2.81it/s]  9%|‚ñâ         | 94/999 [00:33&lt;05:24,  2.79it/s] 10%|‚ñâ         | 95/999 [00:33&lt;05:35,  2.70it/s] 10%|‚ñâ         | 96/999 [00:34&lt;05:36,  2.69it/s] 10%|‚ñâ         | 97/999 [00:34&lt;05:44,  2.62it/s] 10%|‚ñâ         | 98/999 [00:35&lt;05:42,  2.63it/s] 10%|‚ñâ         | 99/999 [00:35&lt;05:38,  2.66it/s] 10%|‚ñà         | 100/999 [00:35&lt;05:27,  2.74it/s] 10%|‚ñà         | 101/999 [00:36&lt;05:27,  2.74it/s] 10%|‚ñà         | 102/999 [00:36&lt;05:24,  2.77it/s] 10%|‚ñà         | 103/999 [00:36&lt;05:26,  2.74it/s] 10%|‚ñà         | 104/999 [00:37&lt;05:23,  2.77it/s] 11%|‚ñà         | 105/999 [00:37&lt;05:20,  2.79it/s] 11%|‚ñà         | 106/999 [00:37&lt;05:28,  2.72it/s] 11%|‚ñà         | 107/999 [00:38&lt;05:30,  2.70it/s] 11%|‚ñà         | 108/999 [00:38&lt;05:37,  2.64it/s] 11%|‚ñà         | 109/999 [00:39&lt;05:36,  2.64it/s] 11%|‚ñà         | 110/999 [00:39&lt;05:40,  2.61it/s] 11%|‚ñà         | 111/999 [00:39&lt;05:45,  2.57it/s] 11%|‚ñà         | 112/999 [00:40&lt;05:50,  2.53it/s] 11%|‚ñà‚ñè        | 113/999 [00:40&lt;05:56,  2.49it/s] 11%|‚ñà‚ñè        | 114/999 [00:41&lt;05:52,  2.51it/s] 12%|‚ñà‚ñè        | 115/999 [00:41&lt;05:52,  2.51it/s] 12%|‚ñà‚ñè        | 116/999 [00:41&lt;05:49,  2.52it/s] 12%|‚ñà‚ñè        | 117/999 [00:42&lt;05:52,  2.50it/s] 12%|‚ñà‚ñè        | 118/999 [00:42&lt;05:49,  2.52it/s] 12%|‚ñà‚ñè        | 119/999 [00:43&lt;05:52,  2.49it/s] 12%|‚ñà‚ñè        | 120/999 [00:43&lt;05:48,  2.52it/s] 12%|‚ñà‚ñè        | 121/999 [00:43&lt;05:50,  2.50it/s] 12%|‚ñà‚ñè        | 122/999 [00:44&lt;06:05,  2.40it/s] 12%|‚ñà‚ñè        | 123/999 [00:44&lt;05:59,  2.44it/s] 12%|‚ñà‚ñè        | 124/999 [00:45&lt;05:57,  2.45it/s] 13%|‚ñà‚ñé        | 125/999 [00:45&lt;05:58,  2.44it/s] 13%|‚ñà‚ñé        | 126/999 [00:46&lt;05:58,  2.44it/s] 13%|‚ñà‚ñé        | 127/999 [00:46&lt;05:57,  2.44it/s] 13%|‚ñà‚ñé        | 128/999 [00:46&lt;05:54,  2.45it/s] 13%|‚ñà‚ñé        | 129/999 [00:47&lt;05:55,  2.44it/s] 13%|‚ñà‚ñé        | 130/999 [00:47&lt;05:53,  2.46it/s] 13%|‚ñà‚ñé        | 131/999 [00:48&lt;05:48,  2.49it/s] 13%|‚ñà‚ñé        | 132/999 [00:48&lt;05:40,  2.55it/s] 13%|‚ñà‚ñé        | 133/999 [00:48&lt;05:36,  2.58it/s] 13%|‚ñà‚ñé        | 134/999 [00:49&lt;05:29,  2.63it/s] 14%|‚ñà‚ñé        | 135/999 [00:49&lt;05:30,  2.61it/s] 14%|‚ñà‚ñé        | 136/999 [00:49&lt;05:32,  2.59it/s] 14%|‚ñà‚ñé        | 137/999 [00:50&lt;05:36,  2.56it/s] 14%|‚ñà‚ñç        | 138/999 [00:50&lt;05:34,  2.57it/s] 14%|‚ñà‚ñç        | 139/999 [00:51&lt;05:45,  2.49it/s] 14%|‚ñà‚ñç        | 140/999 [00:51&lt;05:50,  2.45it/s] 14%|‚ñà‚ñç        | 141/999 [00:51&lt;05:40,  2.52it/s] 14%|‚ñà‚ñç        | 142/999 [00:52&lt;05:34,  2.56it/s] 14%|‚ñà‚ñç        | 143/999 [00:52&lt;05:40,  2.51it/s] 14%|‚ñà‚ñç        | 144/999 [00:53&lt;05:31,  2.58it/s] 15%|‚ñà‚ñç        | 145/999 [00:53&lt;05:25,  2.62it/s] 15%|‚ñà‚ñç        | 146/999 [00:53&lt;05:19,  2.67it/s] 15%|‚ñà‚ñç        | 147/999 [00:54&lt;05:25,  2.62it/s] 15%|‚ñà‚ñç        | 148/999 [00:54&lt;05:46,  2.46it/s] 15%|‚ñà‚ñç        | 149/999 [00:55&lt;05:50,  2.43it/s] 15%|‚ñà‚ñå        | 150/999 [00:55&lt;05:54,  2.40it/s] 15%|‚ñà‚ñå        | 151/999 [00:55&lt;05:46,  2.45it/s] 15%|‚ñà‚ñå        | 152/999 [00:56&lt;05:38,  2.50it/s] 15%|‚ñà‚ñå        | 153/999 [00:56&lt;05:42,  2.47it/s] 15%|‚ñà‚ñå        | 154/999 [00:57&lt;05:47,  2.43it/s] 16%|‚ñà‚ñå        | 155/999 [00:57&lt;05:50,  2.41it/s] 16%|‚ñà‚ñå        | 156/999 [00:57&lt;05:53,  2.39it/s] 16%|‚ñà‚ñå        | 157/999 [00:58&lt;05:53,  2.38it/s] 16%|‚ñà‚ñå        | 158/999 [00:58&lt;05:47,  2.42it/s] 16%|‚ñà‚ñå        | 159/999 [00:59&lt;05:57,  2.35it/s] 16%|‚ñà‚ñå        | 160/999 [00:59&lt;06:03,  2.31it/s] 16%|‚ñà‚ñå        | 161/999 [01:00&lt;05:58,  2.34it/s] 16%|‚ñà‚ñå        | 162/999 [01:00&lt;06:08,  2.27it/s] 16%|‚ñà‚ñã        | 163/999 [01:01&lt;06:10,  2.25it/s] 16%|‚ñà‚ñã        | 164/999 [01:01&lt;06:17,  2.21it/s] 17%|‚ñà‚ñã        | 165/999 [01:01&lt;06:13,  2.23it/s] 17%|‚ñà‚ñã        | 166/999 [01:02&lt;06:08,  2.26it/s] 17%|‚ñà‚ñã        | 167/999 [01:02&lt;06:04,  2.28it/s] 17%|‚ñà‚ñã        | 168/999 [01:03&lt;06:09,  2.25it/s] 17%|‚ñà‚ñã        | 169/999 [01:03&lt;06:07,  2.26it/s] 17%|‚ñà‚ñã        | 170/999 [01:04&lt;06:06,  2.26it/s] 17%|‚ñà‚ñã        | 171/999 [01:04&lt;05:58,  2.31it/s] 17%|‚ñà‚ñã        | 172/999 [01:05&lt;06:04,  2.27it/s] 17%|‚ñà‚ñã        | 173/999 [01:05&lt;06:04,  2.27it/s] 17%|‚ñà‚ñã        | 174/999 [01:05&lt;05:58,  2.30it/s] 18%|‚ñà‚ñä        | 175/999 [01:06&lt;05:55,  2.32it/s] 18%|‚ñà‚ñä        | 176/999 [01:06&lt;06:04,  2.26it/s] 18%|‚ñà‚ñä        | 177/999 [01:07&lt;06:04,  2.25it/s] 18%|‚ñà‚ñä        | 178/999 [01:07&lt;05:53,  2.32it/s] 18%|‚ñà‚ñä        | 179/999 [01:08&lt;05:45,  2.37it/s] 18%|‚ñà‚ñä        | 180/999 [01:08&lt;05:50,  2.34it/s] 18%|‚ñà‚ñä        | 181/999 [01:08&lt;05:52,  2.32it/s] 18%|‚ñà‚ñä        | 182/999 [01:09&lt;06:00,  2.26it/s] 18%|‚ñà‚ñä        | 183/999 [01:09&lt;06:10,  2.20it/s] 18%|‚ñà‚ñä        | 184/999 [01:10&lt;06:03,  2.24it/s] 19%|‚ñà‚ñä        | 185/999 [01:10&lt;06:11,  2.19it/s] 19%|‚ñà‚ñä        | 186/999 [01:11&lt;06:19,  2.14it/s] 19%|‚ñà‚ñä        | 187/999 [01:11&lt;06:18,  2.14it/s] 19%|‚ñà‚ñâ        | 188/999 [01:12&lt;06:17,  2.15it/s] 19%|‚ñà‚ñâ        | 189/999 [01:12&lt;06:02,  2.23it/s] 19%|‚ñà‚ñâ        | 190/999 [01:13&lt;06:02,  2.23it/s] 19%|‚ñà‚ñâ        | 191/999 [01:13&lt;05:41,  2.37it/s] 19%|‚ñà‚ñâ        | 192/999 [01:13&lt;05:49,  2.31it/s] 19%|‚ñà‚ñâ        | 193/999 [01:14&lt;05:46,  2.32it/s] 19%|‚ñà‚ñâ        | 194/999 [01:14&lt;05:45,  2.33it/s] 20%|‚ñà‚ñâ        | 195/999 [01:15&lt;05:42,  2.35it/s] 20%|‚ñà‚ñâ        | 196/999 [01:15&lt;05:42,  2.35it/s] 20%|‚ñà‚ñâ        | 197/999 [01:15&lt;05:45,  2.32it/s] 20%|‚ñà‚ñâ        | 198/999 [01:16&lt;05:42,  2.34it/s] 20%|‚ñà‚ñâ        | 199/999 [01:16&lt;05:47,  2.30it/s] 20%|‚ñà‚ñà        | 200/999 [01:17&lt;05:49,  2.28it/s] 20%|‚ñà‚ñà        | 201/999 [01:17&lt;05:45,  2.31it/s] 20%|‚ñà‚ñà        | 202/999 [01:18&lt;05:50,  2.27it/s] 20%|‚ñà‚ñà        | 203/999 [01:18&lt;05:47,  2.29it/s] 20%|‚ñà‚ñà        | 204/999 [01:19&lt;05:39,  2.34it/s] 21%|‚ñà‚ñà        | 205/999 [01:19&lt;05:35,  2.37it/s] 21%|‚ñà‚ñà        | 206/999 [01:19&lt;05:29,  2.41it/s] 21%|‚ñà‚ñà        | 207/999 [01:20&lt;05:36,  2.35it/s] 21%|‚ñà‚ñà        | 208/999 [01:20&lt;05:40,  2.32it/s] 21%|‚ñà‚ñà        | 209/999 [01:21&lt;05:47,  2.28it/s] 21%|‚ñà‚ñà        | 210/999 [01:21&lt;05:45,  2.28it/s] 21%|‚ñà‚ñà        | 211/999 [01:22&lt;05:58,  2.20it/s] 21%|‚ñà‚ñà        | 212/999 [01:22&lt;05:53,  2.23it/s] 21%|‚ñà‚ñà‚ñè       | 213/999 [01:22&lt;05:43,  2.29it/s] 21%|‚ñà‚ñà‚ñè       | 214/999 [01:23&lt;05:46,  2.26it/s] 22%|‚ñà‚ñà‚ñè       | 215/999 [01:23&lt;05:49,  2.24it/s] 22%|‚ñà‚ñà‚ñè       | 216/999 [01:24&lt;05:43,  2.28it/s] 22%|‚ñà‚ñà‚ñè       | 217/999 [01:24&lt;05:43,  2.28it/s] 22%|‚ñà‚ñà‚ñè       | 218/999 [01:25&lt;05:43,  2.28it/s] 22%|‚ñà‚ñà‚ñè       | 219/999 [01:25&lt;05:25,  2.39it/s] 22%|‚ñà‚ñà‚ñè       | 220/999 [01:25&lt;05:27,  2.38it/s] 22%|‚ñà‚ñà‚ñè       | 221/999 [01:26&lt;05:31,  2.35it/s] 22%|‚ñà‚ñà‚ñè       | 222/999 [01:26&lt;05:40,  2.28it/s] 22%|‚ñà‚ñà‚ñè       | 223/999 [01:27&lt;05:46,  2.24it/s] 22%|‚ñà‚ñà‚ñè       | 224/999 [01:27&lt;05:44,  2.25it/s] 23%|‚ñà‚ñà‚ñé       | 225/999 [01:28&lt;05:50,  2.21it/s] 23%|‚ñà‚ñà‚ñé       | 226/999 [01:28&lt;05:42,  2.25it/s] 23%|‚ñà‚ñà‚ñé       | 227/999 [01:29&lt;05:42,  2.26it/s] 23%|‚ñà‚ñà‚ñé       | 228/999 [01:29&lt;05:44,  2.24it/s] 23%|‚ñà‚ñà‚ñé       | 229/999 [01:30&lt;05:55,  2.17it/s] 23%|‚ñà‚ñà‚ñé       | 230/999 [01:30&lt;06:04,  2.11it/s] 23%|‚ñà‚ñà‚ñé       | 231/999 [01:31&lt;06:10,  2.07it/s] 23%|‚ñà‚ñà‚ñé       | 232/999 [01:31&lt;06:01,  2.12it/s] 23%|‚ñà‚ñà‚ñé       | 233/999 [01:31&lt;05:50,  2.19it/s] 23%|‚ñà‚ñà‚ñé       | 234/999 [01:32&lt;05:43,  2.22it/s] 24%|‚ñà‚ñà‚ñé       | 235/999 [01:32&lt;05:39,  2.25it/s] 24%|‚ñà‚ñà‚ñé       | 236/999 [01:33&lt;05:49,  2.18it/s] 24%|‚ñà‚ñà‚ñé       | 237/999 [01:33&lt;05:58,  2.13it/s] 24%|‚ñà‚ñà‚ñç       | 238/999 [01:34&lt;06:08,  2.06it/s] 24%|‚ñà‚ñà‚ñç       | 239/999 [01:34&lt;06:19,  2.00it/s] 24%|‚ñà‚ñà‚ñç       | 240/999 [01:35&lt;06:23,  1.98it/s] 24%|‚ñà‚ñà‚ñç       | 241/999 [01:35&lt;06:23,  1.98it/s] 24%|‚ñà‚ñà‚ñç       | 242/999 [01:36&lt;06:25,  1.96it/s] 24%|‚ñà‚ñà‚ñç       | 243/999 [01:36&lt;06:32,  1.92it/s] 24%|‚ñà‚ñà‚ñç       | 244/999 [01:37&lt;06:36,  1.90it/s] 25%|‚ñà‚ñà‚ñç       | 245/999 [01:37&lt;06:31,  1.92it/s] 25%|‚ñà‚ñà‚ñç       | 246/999 [01:38&lt;06:34,  1.91it/s] 25%|‚ñà‚ñà‚ñç       | 247/999 [01:39&lt;06:32,  1.92it/s] 25%|‚ñà‚ñà‚ñç       | 248/999 [01:39&lt;06:45,  1.85it/s] 25%|‚ñà‚ñà‚ñç       | 249/999 [01:40&lt;06:53,  1.81it/s] 25%|‚ñà‚ñà‚ñå       | 250/999 [01:40&lt;06:50,  1.82it/s] 25%|‚ñà‚ñà‚ñå       | 251/999 [01:41&lt;06:54,  1.81it/s] 25%|‚ñà‚ñà‚ñå       | 252/999 [01:41&lt;06:45,  1.84it/s] 25%|‚ñà‚ñà‚ñå       | 253/999 [01:42&lt;06:51,  1.81it/s] 25%|‚ñà‚ñà‚ñå       | 254/999 [01:42&lt;06:58,  1.78it/s] 26%|‚ñà‚ñà‚ñå       | 255/999 [01:43&lt;06:59,  1.77it/s] 26%|‚ñà‚ñà‚ñå       | 256/999 [01:44&lt;06:55,  1.79it/s] 26%|‚ñà‚ñà‚ñå       | 257/999 [01:44&lt;06:35,  1.87it/s] 26%|‚ñà‚ñà‚ñå       | 258/999 [01:45&lt;06:27,  1.91it/s] 26%|‚ñà‚ñà‚ñå       | 259/999 [01:45&lt;06:28,  1.90it/s] 26%|‚ñà‚ñà‚ñå       | 260/999 [01:46&lt;06:20,  1.94it/s] 26%|‚ñà‚ñà‚ñå       | 261/999 [01:46&lt;06:12,  1.98it/s] 26%|‚ñà‚ñà‚ñå       | 262/999 [01:47&lt;05:59,  2.05it/s] 26%|‚ñà‚ñà‚ñã       | 263/999 [01:47&lt;05:55,  2.07it/s] 26%|‚ñà‚ñà‚ñã       | 264/999 [01:47&lt;05:50,  2.10it/s] 27%|‚ñà‚ñà‚ñã       | 265/999 [01:48&lt;05:47,  2.11it/s] 27%|‚ñà‚ñà‚ñã       | 266/999 [01:48&lt;05:46,  2.12it/s] 27%|‚ñà‚ñà‚ñã       | 267/999 [01:49&lt;05:42,  2.13it/s] 27%|‚ñà‚ñà‚ñã       | 268/999 [01:49&lt;05:49,  2.09it/s] 27%|‚ñà‚ñà‚ñã       | 269/999 [01:50&lt;05:48,  2.10it/s] 27%|‚ñà‚ñà‚ñã       | 270/999 [01:50&lt;05:52,  2.07it/s] 27%|‚ñà‚ñà‚ñã       | 271/999 [01:51&lt;05:49,  2.08it/s] 27%|‚ñà‚ñà‚ñã       | 272/999 [01:51&lt;05:58,  2.03it/s] 27%|‚ñà‚ñà‚ñã       | 273/999 [01:52&lt;05:59,  2.02it/s] 27%|‚ñà‚ñà‚ñã       | 274/999 [01:52&lt;05:55,  2.04it/s] 28%|‚ñà‚ñà‚ñä       | 275/999 [01:53&lt;05:55,  2.03it/s] 28%|‚ñà‚ñà‚ñä       | 276/999 [01:53&lt;06:01,  2.00it/s] 28%|‚ñà‚ñà‚ñä       | 277/999 [01:54&lt;05:53,  2.04it/s] 28%|‚ñà‚ñà‚ñä       | 278/999 [01:54&lt;05:59,  2.01it/s] 28%|‚ñà‚ñà‚ñä       | 279/999 [01:55&lt;06:01,  1.99it/s] 28%|‚ñà‚ñà‚ñä       | 280/999 [01:55&lt;06:07,  1.96it/s] 28%|‚ñà‚ñà‚ñä       | 281/999 [01:56&lt;06:07,  1.95it/s] 28%|‚ñà‚ñà‚ñä       | 282/999 [01:56&lt;06:14,  1.91it/s] 28%|‚ñà‚ñà‚ñä       | 283/999 [01:57&lt;06:15,  1.91it/s] 28%|‚ñà‚ñà‚ñä       | 284/999 [01:58&lt;06:32,  1.82it/s] 29%|‚ñà‚ñà‚ñä       | 285/999 [01:58&lt;06:44,  1.76it/s] 29%|‚ñà‚ñà‚ñä       | 286/999 [01:59&lt;06:34,  1.81it/s] 29%|‚ñà‚ñà‚ñä       | 287/999 [01:59&lt;06:41,  1.78it/s] 29%|‚ñà‚ñà‚ñâ       | 288/999 [02:00&lt;06:31,  1.81it/s] 29%|‚ñà‚ñà‚ñâ       | 289/999 [02:00&lt;06:35,  1.80it/s] 29%|‚ñà‚ñà‚ñâ       | 290/999 [02:01&lt;06:26,  1.83it/s] 29%|‚ñà‚ñà‚ñâ       | 291/999 [02:01&lt;06:34,  1.79it/s] 29%|‚ñà‚ñà‚ñâ       | 292/999 [02:02&lt;06:23,  1.84it/s] 29%|‚ñà‚ñà‚ñâ       | 293/999 [02:02&lt;06:15,  1.88it/s] 29%|‚ñà‚ñà‚ñâ       | 294/999 [02:03&lt;06:25,  1.83it/s] 30%|‚ñà‚ñà‚ñâ       | 295/999 [02:04&lt;06:16,  1.87it/s] 30%|‚ñà‚ñà‚ñâ       | 296/999 [02:04&lt;06:26,  1.82it/s] 30%|‚ñà‚ñà‚ñâ       | 297/999 [02:05&lt;06:19,  1.85it/s] 30%|‚ñà‚ñà‚ñâ       | 298/999 [02:05&lt;06:23,  1.83it/s] 30%|‚ñà‚ñà‚ñâ       | 299/999 [02:06&lt;06:19,  1.85it/s] 30%|‚ñà‚ñà‚ñà       | 300/999 [02:06&lt;06:27,  1.80it/s] 30%|‚ñà‚ñà‚ñà       | 301/999 [02:07&lt;06:31,  1.78it/s] 30%|‚ñà‚ñà‚ñà       | 302/999 [02:07&lt;06:30,  1.78it/s] 30%|‚ñà‚ñà‚ñà       | 303/999 [02:08&lt;06:39,  1.74it/s] 30%|‚ñà‚ñà‚ñà       | 304/999 [02:09&lt;06:29,  1.79it/s] 31%|‚ñà‚ñà‚ñà       | 305/999 [02:09&lt;06:18,  1.83it/s] 31%|‚ñà‚ñà‚ñà       | 306/999 [02:10&lt;06:08,  1.88it/s] 31%|‚ñà‚ñà‚ñà       | 307/999 [02:10&lt;06:05,  1.89it/s] 31%|‚ñà‚ñà‚ñà       | 308/999 [02:11&lt;05:53,  1.95it/s] 31%|‚ñà‚ñà‚ñà       | 309/999 [02:11&lt;05:59,  1.92it/s] 31%|‚ñà‚ñà‚ñà       | 310/999 [02:12&lt;06:05,  1.89it/s] 31%|‚ñà‚ñà‚ñà       | 311/999 [02:12&lt;06:11,  1.85it/s] 31%|‚ñà‚ñà‚ñà       | 312/999 [02:13&lt;06:12,  1.84it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 313/999 [02:13&lt;06:14,  1.83it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 314/999 [02:14&lt;06:08,  1.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 315/999 [02:14&lt;05:59,  1.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 316/999 [02:15&lt;05:59,  1.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 317/999 [02:15&lt;05:59,  1.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 318/999 [02:16&lt;05:55,  1.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 319/999 [02:16&lt;06:00,  1.89it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 320/999 [02:17&lt;06:01,  1.88it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 321/999 [02:18&lt;06:20,  1.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 322/999 [02:18&lt;06:07,  1.84it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 323/999 [02:19&lt;06:21,  1.77it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 324/999 [02:19&lt;06:19,  1.78it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 325/999 [02:20&lt;06:11,  1.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 326/999 [02:20&lt;06:06,  1.84it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 327/999 [02:21&lt;06:21,  1.76it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 328/999 [02:22&lt;06:11,  1.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 329/999 [02:22&lt;06:22,  1.75it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 330/999 [02:23&lt;06:13,  1.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 331/999 [02:23&lt;06:11,  1.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 332/999 [02:24&lt;06:11,  1.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 333/999 [02:24&lt;06:11,  1.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 334/999 [02:25&lt;06:02,  1.84it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 335/999 [02:25&lt;06:01,  1.83it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 336/999 [02:26&lt;06:03,  1.82it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 337/999 [02:26&lt;06:02,  1.83it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 338/999 [02:27&lt;06:04,  1.81it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 339/999 [02:28&lt;05:54,  1.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 340/999 [02:28&lt;05:47,  1.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 341/999 [02:29&lt;05:49,  1.88it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 342/999 [02:29&lt;05:48,  1.88it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 343/999 [02:30&lt;05:48,  1.88it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 344/999 [02:30&lt;05:50,  1.87it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 345/999 [02:31&lt;05:45,  1.89it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 346/999 [02:31&lt;05:45,  1.89it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 347/999 [02:32&lt;05:47,  1.88it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 348/999 [02:32&lt;05:46,  1.88it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 349/999 [02:33&lt;05:40,  1.91it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 350/999 [02:33&lt;05:38,  1.92it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 351/999 [02:34&lt;05:44,  1.88it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 352/999 [02:34&lt;05:46,  1.87it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 353/999 [02:35&lt;05:47,  1.86it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 354/999 [02:36&lt;05:43,  1.88it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 355/999 [02:36&lt;05:40,  1.89it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 356/999 [02:37&lt;05:40,  1.89it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 357/999 [02:37&lt;05:37,  1.90it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 358/999 [02:38&lt;05:43,  1.86it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 359/999 [02:38&lt;05:47,  1.84it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 360/999 [02:39&lt;05:40,  1.87it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 361/999 [02:39&lt;05:40,  1.87it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 362/999 [02:40&lt;05:36,  1.89it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 363/999 [02:40&lt;05:29,  1.93it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 364/999 [02:41&lt;05:23,  1.97it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 365/999 [02:41&lt;05:30,  1.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 366/999 [02:42&lt;05:29,  1.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 367/999 [02:42&lt;05:26,  1.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 368/999 [02:43&lt;05:23,  1.95it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 369/999 [02:43&lt;05:18,  1.98it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 370/999 [02:44&lt;05:17,  1.98it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 371/999 [02:44&lt;05:14,  2.00it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 372/999 [02:45&lt;05:17,  1.98it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 373/999 [02:45&lt;05:17,  1.97it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 374/999 [02:46&lt;05:16,  1.98it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 375/999 [02:46&lt;05:07,  2.03it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 376/999 [02:47&lt;05:04,  2.05it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 377/999 [02:47&lt;05:09,  2.01it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 378/999 [02:48&lt;05:05,  2.03it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 379/999 [02:48&lt;05:08,  2.01it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 380/999 [02:49&lt;05:14,  1.97it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 381/999 [02:49&lt;05:13,  1.97it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 382/999 [02:50&lt;05:07,  2.01it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 383/999 [02:50&lt;05:02,  2.04it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 384/999 [02:51&lt;05:07,  2.00it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 385/999 [02:51&lt;05:08,  1.99it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 386/999 [02:52&lt;05:11,  1.97it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 387/999 [02:52&lt;04:56,  2.06it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 388/999 [02:53&lt;05:05,  2.00it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 389/999 [02:53&lt;05:03,  2.01it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 390/999 [02:54&lt;05:04,  2.00it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 391/999 [02:54&lt;05:11,  1.95it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 392/999 [02:55&lt;05:14,  1.93it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 393/999 [02:55&lt;05:16,  1.91it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 394/999 [02:56&lt;05:13,  1.93it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 395/999 [02:56&lt;05:09,  1.95it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 396/999 [02:57&lt;05:13,  1.92it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 397/999 [02:57&lt;05:14,  1.91it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 398/999 [02:58&lt;05:12,  1.92it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 399/999 [02:59&lt;05:21,  1.87it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 400/999 [02:59&lt;05:18,  1.88it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 401/999 [03:00&lt;05:18,  1.88it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 402/999 [03:00&lt;05:15,  1.89it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 403/999 [03:01&lt;05:14,  1.90it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 404/999 [03:01&lt;05:10,  1.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 405/999 [03:02&lt;05:04,  1.95it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 406/999 [03:02&lt;05:02,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 407/999 [03:03&lt;05:02,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 408/999 [03:03&lt;04:57,  1.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 409/999 [03:04&lt;04:57,  1.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 410/999 [03:04&lt;05:00,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 411/999 [03:05&lt;05:04,  1.93it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 412/999 [03:05&lt;04:59,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 413/999 [03:06&lt;04:59,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 414/999 [03:06&lt;04:56,  1.97it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 415/999 [03:07&lt;05:01,  1.93it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/999 [03:07&lt;05:05,  1.91it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/999 [03:08&lt;05:09,  1.88it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 418/999 [03:08&lt;05:03,  1.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 419/999 [03:09&lt;05:07,  1.89it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/999 [03:09&lt;05:08,  1.88it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 421/999 [03:10&lt;05:14,  1.84it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 422/999 [03:11&lt;05:17,  1.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/999 [03:11&lt;05:19,  1.80it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/999 [03:12&lt;05:13,  1.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/999 [03:12&lt;05:13,  1.83it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 426/999 [03:13&lt;05:11,  1.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/999 [03:13&lt;05:13,  1.82it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 428/999 [03:14&lt;05:11,  1.83it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/999 [03:14&lt;05:23,  1.76it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 430/999 [03:15&lt;05:17,  1.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 431/999 [03:16&lt;05:08,  1.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/999 [03:16&lt;05:06,  1.85it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433/999 [03:17&lt;05:00,  1.88it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 434/999 [03:17&lt;04:56,  1.91it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 435/999 [03:18&lt;05:06,  1.84it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 436/999 [03:18&lt;05:14,  1.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/999 [03:19&lt;05:20,  1.75it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/999 [03:19&lt;05:18,  1.76it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 439/999 [03:20&lt;05:10,  1.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/999 [03:20&lt;05:10,  1.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 441/999 [03:21&lt;05:00,  1.86it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 442/999 [03:22&lt;04:57,  1.87it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 443/999 [03:22&lt;04:58,  1.86it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/999 [03:23&lt;04:53,  1.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445/999 [03:23&lt;04:54,  1.88it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/999 [03:24&lt;04:49,  1.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 447/999 [03:24&lt;04:54,  1.87it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/999 [03:25&lt;04:52,  1.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/999 [03:25&lt;04:52,  1.88it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 450/999 [03:26&lt;04:57,  1.84it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/999 [03:26&lt;05:00,  1.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/999 [03:27&lt;05:03,  1.80it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/999 [03:27&lt;05:04,  1.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/999 [03:28&lt;05:02,  1.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/999 [03:29&lt;05:00,  1.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 456/999 [03:29&lt;04:54,  1.84it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 457/999 [03:30&lt;04:53,  1.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/999 [03:30&lt;04:52,  1.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 459/999 [03:31&lt;05:03,  1.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/999 [03:31&lt;05:03,  1.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/999 [03:32&lt;05:02,  1.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 462/999 [03:32&lt;05:00,  1.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 463/999 [03:33&lt;05:00,  1.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/999 [03:34&lt;05:04,  1.76it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/999 [03:34&lt;04:55,  1.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/999 [03:35&lt;04:57,  1.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467/999 [03:35&lt;04:48,  1.84it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/999 [03:36&lt;04:43,  1.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/999 [03:36&lt;04:45,  1.86it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/999 [03:37&lt;04:46,  1.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/999 [03:37&lt;04:48,  1.83it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/999 [03:38&lt;04:55,  1.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/999 [03:39&lt;04:57,  1.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 474/999 [03:39&lt;04:44,  1.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475/999 [03:40&lt;04:42,  1.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/999 [03:40&lt;04:39,  1.87it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/999 [03:41&lt;04:42,  1.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/999 [03:41&lt;04:47,  1.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 479/999 [03:42&lt;04:51,  1.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/999 [03:42&lt;04:41,  1.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481/999 [03:43&lt;04:43,  1.83it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/999 [03:43&lt;04:39,  1.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 483/999 [03:44&lt;04:46,  1.80it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 484/999 [03:45&lt;04:39,  1.84it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 485/999 [03:45&lt;04:35,  1.87it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/999 [03:46&lt;04:31,  1.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/999 [03:46&lt;04:30,  1.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/999 [03:47&lt;04:26,  1.91it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 489/999 [03:47&lt;04:22,  1.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/999 [03:48&lt;04:21,  1.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491/999 [03:48&lt;04:15,  1.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 492/999 [03:49&lt;04:15,  1.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/999 [03:49&lt;04:19,  1.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 494/999 [03:50&lt;04:22,  1.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 495/999 [03:50&lt;04:22,  1.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/999 [03:51&lt;04:20,  1.93it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/999 [03:51&lt;04:24,  1.90it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 498/999 [03:52&lt;04:24,  1.89it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/999 [03:52&lt;04:14,  1.97it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/999 [03:53&lt;04:14,  1.96it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/999 [03:53&lt;04:05,  2.03it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 502/999 [03:54&lt;04:13,  1.96it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/999 [03:54&lt;04:11,  1.97it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/999 [03:55&lt;04:09,  1.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/999 [03:55&lt;04:10,  1.97it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 506/999 [03:56&lt;04:15,  1.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/999 [03:56&lt;04:12,  1.95it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/999 [03:57&lt;03:59,  2.05it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 509/999 [03:57&lt;03:57,  2.06it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/999 [03:58&lt;04:00,  2.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 511/999 [03:58&lt;03:56,  2.06it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 512/999 [03:59&lt;03:52,  2.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 513/999 [03:59&lt;03:52,  2.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 514/999 [04:00&lt;03:54,  2.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 515/999 [04:00&lt;04:03,  1.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/999 [04:01&lt;04:00,  2.01it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/999 [04:01&lt;04:01,  2.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 518/999 [04:02&lt;04:03,  1.98it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 519/999 [04:02&lt;04:00,  1.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/999 [04:03&lt;04:00,  1.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/999 [04:03&lt;03:58,  2.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 522/999 [04:04&lt;03:56,  2.02it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 523/999 [04:04&lt;04:01,  1.97it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/999 [04:05&lt;04:03,  1.95it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/999 [04:05&lt;04:09,  1.90it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 526/999 [04:06&lt;04:08,  1.90it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 527/999 [04:06&lt;04:01,  1.95it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/999 [04:07&lt;04:06,  1.91it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/999 [04:07&lt;03:55,  2.00it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 530/999 [04:08&lt;04:03,  1.93it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/999 [04:08&lt;04:03,  1.92it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532/999 [04:09&lt;04:05,  1.90it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/999 [04:09&lt;04:04,  1.91it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 534/999 [04:10&lt;04:00,  1.93it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 535/999 [04:10&lt;03:57,  1.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 536/999 [04:11&lt;03:58,  1.94it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 537/999 [04:11&lt;03:55,  1.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/999 [04:12&lt;03:54,  1.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 539/999 [04:12&lt;03:56,  1.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/999 [04:13&lt;03:54,  1.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/999 [04:13&lt;03:53,  1.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 542/999 [04:14&lt;03:53,  1.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 543/999 [04:14&lt;03:52,  1.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/999 [04:15&lt;03:57,  1.91it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/999 [04:16&lt;04:00,  1.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 546/999 [04:16&lt;03:57,  1.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/999 [04:17&lt;03:56,  1.91it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/999 [04:17&lt;03:58,  1.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/999 [04:18&lt;03:54,  1.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 550/999 [04:18&lt;03:50,  1.95it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/999 [04:19&lt;03:55,  1.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/999 [04:19&lt;03:51,  1.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 553/999 [04:20&lt;03:53,  1.91it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 554/999 [04:20&lt;03:51,  1.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/999 [04:21&lt;03:52,  1.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/999 [04:21&lt;03:52,  1.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/999 [04:22&lt;03:50,  1.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 558/999 [04:22&lt;03:49,  1.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/999 [04:23&lt;03:47,  1.93it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/999 [04:23&lt;03:48,  1.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/999 [04:24&lt;03:46,  1.93it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 562/999 [04:24&lt;03:43,  1.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563/999 [04:25&lt;03:52,  1.88it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/999 [04:26&lt;03:55,  1.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 565/999 [04:26&lt;04:02,  1.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/999 [04:27&lt;03:58,  1.82it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/999 [04:27&lt;03:52,  1.86it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/999 [04:28&lt;03:59,  1.80it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/999 [04:28&lt;04:08,  1.73it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 570/999 [04:29&lt;04:03,  1.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 571/999 [04:29&lt;03:56,  1.81it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 572/999 [04:30&lt;03:53,  1.83it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 573/999 [04:31&lt;03:54,  1.82it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/999 [04:31&lt;03:54,  1.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 575/999 [04:32&lt;03:47,  1.86it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/999 [04:32&lt;03:42,  1.90it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 577/999 [04:33&lt;03:44,  1.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/999 [04:33&lt;03:42,  1.89it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579/999 [04:34&lt;03:35,  1.94it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/999 [04:34&lt;03:37,  1.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 581/999 [04:35&lt;03:43,  1.87it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/999 [04:35&lt;03:35,  1.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583/999 [04:36&lt;03:35,  1.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 584/999 [04:36&lt;03:35,  1.92it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/999 [04:37&lt;03:36,  1.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/999 [04:37&lt;03:40,  1.87it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 587/999 [04:38&lt;03:41,  1.86it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/999 [04:38&lt;03:40,  1.87it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/999 [04:39&lt;03:34,  1.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/999 [04:39&lt;03:35,  1.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/999 [04:40&lt;03:40,  1.85it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/999 [04:41&lt;03:33,  1.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 593/999 [04:41&lt;03:36,  1.88it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/999 [04:42&lt;03:39,  1.84it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/999 [04:42&lt;03:42,  1.82it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 596/999 [04:43&lt;03:43,  1.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 597/999 [04:43&lt;03:47,  1.77it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/999 [04:44&lt;03:44,  1.79it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/999 [04:45&lt;03:44,  1.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/999 [04:45&lt;03:42,  1.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/999 [04:46&lt;03:37,  1.83it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/999 [04:46&lt;03:34,  1.85it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 603/999 [04:47&lt;03:33,  1.86it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/999 [04:47&lt;03:31,  1.87it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/999 [04:48&lt;03:30,  1.87it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/999 [04:48&lt;03:36,  1.82it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/999 [04:49&lt;03:31,  1.86it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/999 [04:49&lt;03:26,  1.89it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/999 [04:50&lt;03:23,  1.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/999 [04:50&lt;03:20,  1.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/999 [04:51&lt;03:18,  1.96it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 612/999 [04:51&lt;03:21,  1.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 613/999 [04:52&lt;03:17,  1.96it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/999 [04:52&lt;03:20,  1.92it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 615/999 [04:53&lt;03:11,  2.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 616/999 [04:53&lt;03:18,  1.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 617/999 [04:54&lt;03:17,  1.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 618/999 [04:54&lt;03:18,  1.92it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/999 [04:55&lt;03:13,  1.96it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/999 [04:55&lt;03:16,  1.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 621/999 [04:56&lt;03:13,  1.96it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 622/999 [04:56&lt;03:11,  1.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 623/999 [04:57&lt;03:12,  1.96it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/999 [04:57&lt;03:10,  1.97it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/999 [04:58&lt;03:12,  1.94it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/999 [04:59&lt;03:12,  1.94it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/999 [04:59&lt;03:10,  1.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/999 [05:00&lt;03:11,  1.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 629/999 [05:00&lt;03:12,  1.92it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/999 [05:01&lt;03:12,  1.91it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/999 [05:01&lt;03:10,  1.94it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/999 [05:02&lt;03:02,  2.01it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/999 [05:02&lt;03:02,  2.00it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 634/999 [05:03&lt;03:02,  2.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/999 [05:03&lt;03:01,  2.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/999 [05:04&lt;03:03,  1.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 637/999 [05:04&lt;02:59,  2.01it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 638/999 [05:05&lt;03:02,  1.98it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 639/999 [05:05&lt;03:02,  1.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/999 [05:06&lt;03:02,  1.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 641/999 [05:06&lt;03:07,  1.91it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 642/999 [05:07&lt;03:08,  1.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/999 [05:07&lt;03:06,  1.91it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 644/999 [05:08&lt;03:05,  1.92it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/999 [05:08&lt;03:01,  1.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 646/999 [05:09&lt;03:04,  1.91it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 647/999 [05:09&lt;03:03,  1.92it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 648/999 [05:10&lt;02:56,  1.98it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 649/999 [05:10&lt;02:53,  2.01it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/999 [05:11&lt;02:48,  2.07it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/999 [05:11&lt;02:50,  2.04it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 652/999 [05:12&lt;02:50,  2.03it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 653/999 [05:12&lt;02:53,  1.99it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/999 [05:13&lt;02:48,  2.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/999 [05:13&lt;02:44,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/999 [05:14&lt;02:48,  2.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 657/999 [05:14&lt;02:49,  2.01it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 658/999 [05:15&lt;02:46,  2.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 659/999 [05:15&lt;02:44,  2.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/999 [05:16&lt;02:46,  2.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 661/999 [05:16&lt;02:44,  2.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 662/999 [05:17&lt;02:45,  2.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/999 [05:17&lt;02:47,  2.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/999 [05:18&lt;02:49,  1.97it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 665/999 [05:18&lt;02:47,  2.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/999 [05:19&lt;02:46,  1.99it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/999 [05:19&lt;02:43,  2.03it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/999 [05:20&lt;02:43,  2.02it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 669/999 [05:20&lt;02:46,  1.98it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/999 [05:21&lt;02:46,  1.97it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 671/999 [05:21&lt;02:43,  2.01it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/999 [05:22&lt;02:38,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 673/999 [05:22&lt;02:36,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/999 [05:23&lt;02:35,  2.09it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/999 [05:23&lt;02:34,  2.09it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/999 [05:23&lt;02:35,  2.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 677/999 [05:24&lt;02:36,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 678/999 [05:24&lt;02:37,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/999 [05:25&lt;02:36,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/999 [05:25&lt;02:38,  2.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 681/999 [05:26&lt;02:35,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/999 [05:26&lt;02:32,  2.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 683/999 [05:27&lt;02:35,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 684/999 [05:27&lt;02:35,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/999 [05:28&lt;02:34,  2.04it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/999 [05:28&lt;02:34,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 687/999 [05:29&lt;02:38,  1.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/999 [05:29&lt;02:39,  1.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 689/999 [05:30&lt;02:37,  1.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/999 [05:30&lt;02:38,  1.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691/999 [05:31&lt;02:37,  1.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 692/999 [05:32&lt;02:36,  1.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 693/999 [05:32&lt;02:36,  1.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/999 [05:32&lt;02:32,  1.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 695/999 [05:33&lt;02:35,  1.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/999 [05:34&lt;02:38,  1.91it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/999 [05:34&lt;02:39,  1.90it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 698/999 [05:35&lt;02:38,  1.90it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/999 [05:35&lt;02:39,  1.88it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/999 [05:36&lt;02:37,  1.90it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/999 [05:36&lt;02:36,  1.91it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702/999 [05:37&lt;02:40,  1.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/999 [05:37&lt;02:39,  1.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/999 [05:38&lt;02:37,  1.87it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 705/999 [05:38&lt;02:45,  1.77it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/999 [05:39&lt;02:39,  1.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/999 [05:40&lt;02:37,  1.85it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 708/999 [05:40&lt;02:33,  1.89it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/999 [05:41&lt;02:34,  1.87it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/999 [05:41&lt;02:37,  1.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 711/999 [05:42&lt;02:37,  1.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 712/999 [05:42&lt;02:35,  1.85it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 713/999 [05:43&lt;02:30,  1.90it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 714/999 [05:43&lt;02:32,  1.87it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 715/999 [05:44&lt;02:31,  1.87it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/999 [05:44&lt;02:33,  1.84it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/999 [05:45&lt;02:38,  1.78it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 718/999 [05:46&lt;02:37,  1.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/999 [05:46&lt;02:39,  1.75it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/999 [05:47&lt;02:43,  1.70it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/999 [05:47&lt;02:47,  1.66it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/999 [05:48&lt;02:50,  1.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 723/999 [05:49&lt;02:50,  1.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 724/999 [05:49&lt;02:48,  1.63it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/999 [05:50&lt;02:46,  1.65it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/999 [05:50&lt;02:40,  1.70it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 727/999 [05:51&lt;02:41,  1.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/999 [05:52&lt;02:41,  1.67it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 729/999 [05:52&lt;02:41,  1.67it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 730/999 [05:53&lt;02:43,  1.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/999 [05:53&lt;02:41,  1.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/999 [05:54&lt;02:43,  1.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/999 [05:55&lt;02:41,  1.65it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 734/999 [05:55&lt;02:37,  1.69it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 735/999 [05:56&lt;02:34,  1.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/999 [05:56&lt;02:40,  1.64it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 737/999 [05:57&lt;02:40,  1.63it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 738/999 [05:58&lt;02:54,  1.49it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739/999 [05:59&lt;02:56,  1.47it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/999 [05:59&lt;02:56,  1.47it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 741/999 [06:00&lt;02:56,  1.46it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 742/999 [06:01&lt;02:53,  1.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 743/999 [06:01&lt;02:48,  1.52it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/999 [06:02&lt;02:49,  1.51it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/999 [06:03&lt;02:44,  1.54it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/999 [06:03&lt;02:41,  1.57it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 747/999 [06:04&lt;02:40,  1.57it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/999 [06:04&lt;02:44,  1.53it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 749/999 [06:05&lt;02:41,  1.55it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750/999 [06:06&lt;02:43,  1.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/999 [06:06&lt;02:43,  1.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/999 [06:07&lt;02:39,  1.54it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 753/999 [06:08&lt;02:40,  1.53it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/999 [06:08&lt;02:41,  1.52it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/999 [06:09&lt;02:40,  1.52it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 756/999 [06:10&lt;02:37,  1.55it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/999 [06:10&lt;02:42,  1.49it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 758/999 [06:11&lt;02:42,  1.48it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 759/999 [06:12&lt;02:38,  1.51it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 760/999 [06:12&lt;02:40,  1.49it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/999 [06:13&lt;02:43,  1.46it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 762/999 [06:14&lt;02:43,  1.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 763/999 [06:15&lt;02:46,  1.42it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/999 [06:15&lt;02:49,  1.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/999 [06:16&lt;02:45,  1.41it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/999 [06:17&lt;02:43,  1.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/999 [06:17&lt;02:43,  1.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/999 [06:18&lt;02:42,  1.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/999 [06:19&lt;02:41,  1.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 770/999 [06:20&lt;02:41,  1.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/999 [06:20&lt;02:35,  1.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 772/999 [06:21&lt;02:32,  1.49it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 773/999 [06:21&lt;02:31,  1.50it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/999 [06:22&lt;02:32,  1.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/999 [06:23&lt;02:30,  1.49it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 776/999 [06:23&lt;02:31,  1.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 777/999 [06:24&lt;02:31,  1.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/999 [06:25&lt;02:33,  1.44it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/999 [06:26&lt;02:29,  1.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 780/999 [06:26&lt;02:27,  1.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/999 [06:27&lt;02:30,  1.44it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 782/999 [06:28&lt;02:29,  1.46it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/999 [06:28&lt;02:29,  1.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/999 [06:29&lt;02:28,  1.45it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 785/999 [06:30&lt;02:32,  1.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 786/999 [06:31&lt;02:35,  1.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 787/999 [06:31&lt;02:32,  1.39it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/999 [06:32&lt;02:31,  1.39it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/999 [06:33&lt;02:30,  1.39it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 790/999 [06:33&lt;02:29,  1.39it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 791/999 [06:34&lt;02:25,  1.43it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 792/999 [06:35&lt;02:22,  1.45it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 793/999 [06:35&lt;02:20,  1.47it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 794/999 [06:36&lt;02:21,  1.45it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/999 [06:37&lt;02:17,  1.48it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/999 [06:37&lt;02:17,  1.48it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 797/999 [06:38&lt;02:17,  1.47it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 798/999 [06:39&lt;02:20,  1.43it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/999 [06:40&lt;02:24,  1.38it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/999 [06:40&lt;02:26,  1.36it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/999 [06:41&lt;02:24,  1.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/999 [06:42&lt;02:23,  1.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/999 [06:43&lt;02:24,  1.36it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/999 [06:43&lt;02:25,  1.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/999 [06:44&lt;02:24,  1.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 806/999 [06:45&lt;02:23,  1.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 807/999 [06:46&lt;02:20,  1.37it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 808/999 [06:46&lt;02:17,  1.38it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 809/999 [06:47&lt;02:15,  1.40it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/999 [06:48&lt;02:11,  1.44it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 811/999 [06:48&lt;02:11,  1.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 812/999 [06:49&lt;02:10,  1.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 813/999 [06:50&lt;02:10,  1.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/999 [06:50&lt;02:13,  1.39it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/999 [06:51&lt;02:15,  1.36it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/999 [06:52&lt;02:16,  1.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/999 [06:53&lt;02:16,  1.33it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/999 [06:54&lt;02:14,  1.35it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 819/999 [06:54&lt;02:17,  1.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/999 [06:55&lt;02:15,  1.32it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 821/999 [06:56&lt;02:13,  1.33it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/999 [06:57&lt;02:10,  1.35it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/999 [06:57&lt;02:09,  1.35it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 824/999 [06:58&lt;02:09,  1.35it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/999 [06:59&lt;02:10,  1.33it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 826/999 [06:59&lt;02:06,  1.37it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/999 [07:00&lt;02:01,  1.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/999 [07:01&lt;02:00,  1.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 829/999 [07:02&lt;02:01,  1.39it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 830/999 [07:02&lt;01:59,  1.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/999 [07:03&lt;01:56,  1.44it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/999 [07:04&lt;01:56,  1.43it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 833/999 [07:04&lt;01:51,  1.48it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/999 [07:05&lt;01:49,  1.51it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/999 [07:06&lt;01:50,  1.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/999 [07:06&lt;01:50,  1.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 837/999 [07:07&lt;01:51,  1.45it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 838/999 [07:08&lt;01:51,  1.44it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 839/999 [07:08&lt;01:51,  1.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/999 [07:09&lt;01:50,  1.44it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/999 [07:10&lt;01:48,  1.46it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/999 [07:10&lt;01:47,  1.46it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/999 [07:11&lt;01:45,  1.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 844/999 [07:12&lt;01:43,  1.50it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 845/999 [07:12&lt;01:40,  1.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/999 [07:13&lt;01:43,  1.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 847/999 [07:14&lt;01:42,  1.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/999 [07:14&lt;01:41,  1.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/999 [07:15&lt;01:42,  1.46it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 850/999 [07:16&lt;01:40,  1.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/999 [07:16&lt;01:41,  1.46it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 852/999 [07:17&lt;01:40,  1.46it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 853/999 [07:18&lt;01:38,  1.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 854/999 [07:18&lt;01:34,  1.53it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/999 [07:19&lt;01:34,  1.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 856/999 [07:20&lt;01:32,  1.55it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 857/999 [07:20&lt;01:29,  1.58it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 858/999 [07:21&lt;01:29,  1.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 859/999 [07:22&lt;01:29,  1.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/999 [07:22&lt;01:31,  1.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 861/999 [07:23&lt;01:32,  1.50it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 862/999 [07:24&lt;01:29,  1.54it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/999 [07:24&lt;01:29,  1.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 864/999 [07:25&lt;01:30,  1.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/999 [07:26&lt;01:31,  1.47it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 866/999 [07:26&lt;01:30,  1.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/999 [07:27&lt;01:30,  1.46it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 868/999 [07:28&lt;01:27,  1.50it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 869/999 [07:28&lt;01:26,  1.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/999 [07:29&lt;01:24,  1.53it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 871/999 [07:30&lt;01:24,  1.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/999 [07:30&lt;01:24,  1.50it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 873/999 [07:31&lt;01:24,  1.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/999 [07:32&lt;01:23,  1.50it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/999 [07:32&lt;01:23,  1.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 876/999 [07:33&lt;01:22,  1.48it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877/999 [07:34&lt;01:23,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/999 [07:34&lt;01:22,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 879/999 [07:35&lt;01:21,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 880/999 [07:36&lt;01:20,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/999 [07:36&lt;01:20,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 882/999 [07:37&lt;01:20,  1.46it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/999 [07:38&lt;01:18,  1.47it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 884/999 [07:38&lt;01:17,  1.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 885/999 [07:39&lt;01:15,  1.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/999 [07:40&lt;01:14,  1.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 887/999 [07:40&lt;01:14,  1.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/999 [07:41&lt;01:16,  1.46it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 889/999 [07:42&lt;01:14,  1.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 890/999 [07:42&lt;01:12,  1.50it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/999 [07:43&lt;01:10,  1.54it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 892/999 [07:44&lt;01:08,  1.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 893/999 [07:44&lt;01:07,  1.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/999 [07:45&lt;01:07,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/999 [07:46&lt;01:06,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/999 [07:46&lt;01:05,  1.57it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 897/999 [07:47&lt;01:05,  1.57it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/999 [07:48&lt;01:04,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/999 [07:48&lt;01:03,  1.57it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/999 [07:49&lt;01:02,  1.57it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/999 [07:49&lt;01:02,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/999 [07:50&lt;01:02,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 903/999 [07:51&lt;01:00,  1.58it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 904/999 [07:51&lt;01:00,  1.58it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/999 [07:52&lt;01:00,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/999 [07:53&lt;00:59,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 907/999 [07:53&lt;00:58,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 908/999 [07:54&lt;00:57,  1.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/999 [07:55&lt;00:58,  1.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 910/999 [07:55&lt;00:59,  1.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/999 [07:56&lt;00:58,  1.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 912/999 [07:57&lt;00:58,  1.50it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/999 [07:57&lt;00:57,  1.49it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 914/999 [07:58&lt;00:56,  1.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 915/999 [07:59&lt;00:55,  1.51it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/999 [07:59&lt;00:55,  1.51it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/999 [08:00&lt;00:54,  1.49it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 918/999 [08:01&lt;00:54,  1.49it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/999 [08:01&lt;00:53,  1.48it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 920/999 [08:02&lt;00:52,  1.51it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 921/999 [08:03&lt;00:50,  1.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/999 [08:03&lt;00:49,  1.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/999 [08:04&lt;00:48,  1.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 924/999 [08:04&lt;00:47,  1.57it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 925/999 [08:05&lt;00:47,  1.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/999 [08:06&lt;00:47,  1.55it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/999 [08:06&lt;00:46,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/999 [08:07&lt;00:46,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 929/999 [08:08&lt;00:45,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 930/999 [08:08&lt;00:45,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 931/999 [08:09&lt;00:44,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/999 [08:10&lt;00:43,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 933/999 [08:10&lt;00:43,  1.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 934/999 [08:11&lt;00:42,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 935/999 [08:12&lt;00:41,  1.55it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/999 [08:12&lt;00:40,  1.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 937/999 [08:13&lt;00:39,  1.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/999 [08:13&lt;00:37,  1.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/999 [08:14&lt;00:37,  1.58it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 940/999 [08:15&lt;00:36,  1.60it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/999 [08:15&lt;00:35,  1.62it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/999 [08:16&lt;00:35,  1.60it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 943/999 [08:17&lt;00:35,  1.59it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/999 [08:17&lt;00:34,  1.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 945/999 [08:18&lt;00:34,  1.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/999 [08:19&lt;00:33,  1.59it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/999 [08:19&lt;00:30,  1.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 948/999 [08:20&lt;00:30,  1.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 949/999 [08:20&lt;00:30,  1.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/999 [08:21&lt;00:30,  1.62it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/999 [08:22&lt;00:30,  1.56it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 952/999 [08:22&lt;00:30,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/999 [08:23&lt;00:30,  1.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/999 [08:24&lt;00:30,  1.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 955/999 [08:24&lt;00:30,  1.45it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 956/999 [08:25&lt;00:29,  1.45it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/999 [08:26&lt;00:29,  1.43it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/999 [08:27&lt;00:28,  1.43it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 959/999 [08:27&lt;00:28,  1.42it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 960/999 [08:28&lt;00:26,  1.45it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/999 [08:29&lt;00:26,  1.44it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 962/999 [08:29&lt;00:25,  1.44it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/999 [08:30&lt;00:25,  1.41it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964/999 [08:31&lt;00:25,  1.37it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/999 [08:32&lt;00:24,  1.39it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 966/999 [08:32&lt;00:23,  1.41it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 967/999 [08:33&lt;00:22,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 968/999 [08:34&lt;00:21,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/999 [08:34&lt;00:21,  1.41it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 970/999 [08:35&lt;00:20,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 971/999 [08:36&lt;00:19,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/999 [08:36&lt;00:18,  1.42it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/999 [08:37&lt;00:18,  1.42it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 974/999 [08:38&lt;00:18,  1.35it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/999 [08:39&lt;00:17,  1.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 976/999 [08:39&lt;00:16,  1.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/999 [08:40&lt;00:16,  1.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 978/999 [08:41&lt;00:15,  1.34it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/999 [08:42&lt;00:14,  1.37it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 980/999 [08:42&lt;00:13,  1.40it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/999 [08:43&lt;00:13,  1.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 982/999 [08:44&lt;00:12,  1.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 983/999 [08:44&lt;00:11,  1.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 984/999 [08:45&lt;00:10,  1.40it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/999 [08:46&lt;00:10,  1.39it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/999 [08:46&lt;00:09,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 987/999 [08:47&lt;00:08,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 988/999 [08:48&lt;00:07,  1.40it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 989/999 [08:49&lt;00:07,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 990/999 [08:49&lt;00:06,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/999 [08:50&lt;00:05,  1.41it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 992/999 [08:51&lt;00:04,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/999 [08:51&lt;00:04,  1.38it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/999 [08:52&lt;00:03,  1.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 995/999 [08:53&lt;00:02,  1.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/999 [08:54&lt;00:02,  1.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 997/999 [08:54&lt;00:01,  1.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/999 [08:55&lt;00:00,  1.34it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [08:56&lt;00:00,  1.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [08:56&lt;00:00,  1.86it/s]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', label='Premi√®re bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles th√©oriques')\nplt.xlabel('Quantiles th√©oriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# # kstest y revenir\n# ks_stat, ks_p_value = kstest(data_train, skew_student_pdf, args=(**params_sstd,))\n\n# print(\"=\"*80)\n# print(\"H0 : Les donn√©es suivent une loi de Skew Student\")\n# print(f\"Statistique de test : {ks_stat:.4f}\")\n# print(f\"P-value : {ks_p_value:.4f}\")\n# print(\"=\"*80)\n# A revoir\n\n\n\nII.4.2. Calcul de la VaR Skew Student\n\n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var_fct(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\nsstd_var = sstd_var_fct(alpha, params_sstd)\nprint(f\"La VaR skew student pour h=1j et alpha={alpha} est : {sstd_var:.4%}\")\n\nLa VaR skew student pour h=1j et alpha=0.99 est : 4.2575%"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_def.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque r√©glementaire correspond √† la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s‚Äôagit dans cette section de d√©velopper la notion de VaR pour des portefeuilles lin√©aires et non lin√©aires."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_def.html#le-backtesting",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contr√¥le de la qualit√© de la VaR pour un horizon de 1 jour. Il permet de v√©rifier si la VaR est bien calibr√©e. Pour cela, on compare la VaR calcul√©e avec la perte r√©elle. Si la VaR est bien calibr√©e, la perte r√©elle ne doit pas d√©passer la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d‚Äôexception pour valider le mod√®le. Notons PnL le profit and loss du portefeuille et VaR la valeur √† risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsid√©rons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inf√©rieur √† l‚Äôoppos√© de la VaR avec probabilit√© \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une p√©riode ouvr√© comptant n jours, la probabilit√© d‚Äôavoir \\(i\\) exceptions est donn√©e par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir plus de \\(k\\) exceptions est donn√©e par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir au plus de \\(i\\) exceptions est donn√©e par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilit√© d‚Äôexception n‚Äôexc√®de pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d‚Äôexceptions empirique est sup√©rieur √† celui attendu, le mod√®le est rejet√© :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donn√©e par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d‚Äôexceptions observ√©e lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donn√©e par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) o√π \\(t\\) est la valeur observ√©e de la statistique de test et \\(\\phi\\) est la fonction de r√©partition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_def.html#cas-g√©n√©ral",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_def.html#cas-g√©n√©ral",
    "title": "La VaR",
    "section": "2.2 Cas g√©n√©ral",
    "text": "2.2 Cas g√©n√©ral\nDans cette approche, nous consid√©rons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en d√©duisons donc que la VaR est calcul√© comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction d√©croissante de l‚Äôesp√©rance de PnL et une fonction croissante de la volatilit√© du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de pr√©voir l‚Äôesp√©rance du PnL futur.\n\n2.2.1 Exemple\nNous consid√©rons une position courte de 1 million de dollars sur le contrat √† terme S&P 500. Nous estimons que la volatilit√© annualis√©e \\(\\sigma_{\\text{SPX}}\\) est √©gale √† 35%.\nLa perte du portefeuille est √©gale √† \\(L(w) = N \\times R_{\\text{SPX}}\\) o√π \\(N\\) est le montant de l‚Äôexposition (‚àí1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l‚Äôindice S&P 500. Nous d√©duisons que la volatilit√© de la perte annualis√©e est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur √† risque pour une p√©riode de d√©tention d‚Äôun an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l‚Äôinvestisseur sur un 1an s‚Äô√©l√®ve √† 815 500‚Ç¨ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre p√©riode de d√©tention, nous utilisons la r√®gle de la racine carr√© pour convertir la volatilit√© pour une fr√©quence donn√© \\(f_1\\) en une autre volatilit√© pour une autre fr√©quence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les r√©sultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calcul√© sur 1 jour, pour l‚Äôavoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Mod√®les lin√©aires de facteurs",
    "text": "2.3 Mod√®les lin√©aires de facteurs\nNous consid√©rons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est lin√©aire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La premi√®re id√©e est de choisir les facteurs comme √©tant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du march√© et chaque actif poss√®de son propre facteur de risque.\nLe probl√®me est que les prix sont loin d‚Äô√™tre stationnaires, ce qui nous am√®ne √† devoir affronter certains probl√®mes pour mod√©liser la distribution \\(F_t\\). Une autre id√©e est de r√©crire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] o√π \\(R_i(t;h)\\) est le retour de l‚Äôactif entre \\(t\\) et \\(t+h\\).\nNous d√©duisons que le PnL al√©atoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\no√π \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l‚Äôexposition nominale)dans l‚Äôactif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s‚Äôensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous d√©duisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l‚Äô√âquation (2.6), l‚Äôexpression de la valeur √† risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d‚Äôestimer la matrice de covariance des retours des actifs pour calculer la valeur √† risque. Cela explique la popularit√© de ce mod√®le, surtout lorsque le P&L du portefeuille est une fonction lin√©aire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsid√©rons l‚Äôexemple des entreprises d‚ÄôApple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l‚Äô√©cart type estim√© des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corr√©lation crois√©e est √©gale √† 12,0787 %. Il s‚Äôensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous d√©duisons que la valeur √† risque quotidienne √† 99% est de 41,21 $. Nous obtenons une figure inf√©rieure √† celle de la valeur √† risque historique, qui √©tait de 47,39 $. Nous expliquons ce r√©sultat par le fait que la distribution gaussienne sous-estime la probabilit√© des √©v√©nements extr√™mes et n‚Äôest donc pas adapt√©e √† des calculs pr√©cis de risque dans des situations de march√© volatiles.\n\n\n2.3.2 Exemple de portefeuille lin√©aire d‚Äôactifs\nConsid√©rons un portefeuille lin√©aire compos√© de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne √©gaux √† : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilit√©s journali√®res sont √©gales √† 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244‚Ç¨, 135‚Ç¨,315‚Ç¨. La matrice de corr√©lation est donn√©e par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journ√©e est donc de 18.42‚Ç¨ avec un risque 1% de se tromper.\n\n2.3.2.1 Impl√©mentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les √©carts-types et les corr√©lations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Mod√®les factoriels de risque",
    "text": "2.4 Mod√®les factoriels de risque\nNous supposons que la valeur du portefeuille d√©pend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille √† \\(t+h\\) d√©pend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\no√π g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit lin√©aire par rapport aux facteurs de risque, ainsi donc le retour des actifs √† l‚Äôhorizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] o√π \\(B\\) est la matrice des sensibilit√©s du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque √† l‚Äôhorizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables al√©atoires gaussiennes ind√©pendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable al√©atoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calcul√© comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette m√©thode repose sur 3 hypoth√®ses : l‚Äôind√©pendance temporelle des variations de la valeur du portefeuille, la normalit√© des facteurs et la relation lin√©aire entre les facteurs et la valeur du portefeuille. En g√©n√©ral, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons √† partir des donn√©es historiques des facteurs et \\(B\\) est le vecteur des sensibilit√©s du portefeuille aux facteurs de risque. La seuil difficult√© de cette m√©thode est l‚Äôestimation de la matrice de variance covariance.\n\n2.4.1 Exemple d‚Äôun portefeuille obligataire sans risque de cr√©dit\nNous consid√©rons une exposition sur une obligation am√©ricaine √† $t=$31 d√©cembre 2014. Le nominal de l‚Äôobligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont √©gaux √† 5, \\(t_m&gt;t\\). La maturit√© r√©siduelle est de cinq ans et les dates de fixation sont √† la fin de d√©cembre (\\(n_C=5\\). Le nombre d‚Äôobligations d√©tenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d‚Äôune obligation z√©ro coupon (montant qu‚Äôun investisseur serait pr√™t √† payer aujourd‚Äôhui pour recevoir un paiement fixe √† une date future : combien me rapport un euro √† maturit√© \\(T\\) aujourd‚Äôhui?) au temps \\(t\\) pour l‚Äô√©ch√©ance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) o√π \\(R_t(T)\\) est le taux de rendement z√©ro coupon.\nLa valeur de l‚Äôobligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en d√©duit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\no√π \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l‚Äôobligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l‚Äôobligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en d√©duisons que le prix de l‚Äôobligation est de \\(P(t)=115,47 \\$\\) et l‚Äôexposition totale est de 1 154 706 $. En utilisant la p√©riode historique de l‚Äôann√©e 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d‚Äôint√©r√™t √† coupon z√©ro sachant que l‚Äô√©cart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), o√π \\(h\\) correspond √† un jour de bourse. Pour la matrice de corr√©lation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en d√©duisons que la valeur √† risque √† 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur √† risque de 4970$ pour une p√©riode de d√©tention d‚Äôun jour.\n\n2.4.1.1 Impl√©mentation en R\n\n# D√©finition des √©carts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_application.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_application.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous int√©resser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d‚Äôestimer les pertes maximales potentielles d‚Äôun portefeuille d‚Äôactifs financiers sur un horizon de temps donn√©, √† un certain niveau de confiance. Elle est largement utilis√©e par les institutions financi√®res pour √©valuer et g√©rer les risques de march√©, de cr√©dit et de liquidit√©.\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-analytique",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est bas√©e sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR √† horizon 1 jour. La VaR √† horizon 1 jour est d√©finie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] o√π \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) √† \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l‚Äôhypoth√®se nulle est que la population √©tudi√©e suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\nnp.float64(1.0776562135811891e-41)\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l‚Äôhypoth√®se de la distribution normale de nos rendements. Cel√† est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(8, 6))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR √† horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR √† horizon 1 jour est de 0.0323\n\n\nLa VaR √† horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l‚Äô√©chantillon test les rendements avec la VaR √† horizon 1 jour. Si le rendement est inf√©rieur √† l‚Äôoppos√© de la VaR gaussienne, alors la VaR est viol√©e et cel√† correspond √† une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d‚Äôexceptions que nous comptabilisons sur nos donn√©es test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 30 et en d√©duisons que le taux d‚Äôexception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.27\n\n\nPour tester la pertinence de la VaR calcul√©e, il faudrait id√©alement que le taux d‚Äôexception soit inf√©rieur √† 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\nnp.float64(0.09676472990514096)\n\n\nLa pvaleur de ce test est 3.70%, cel√† est inf√©rieur √† 5% donc nous rejetons l‚Äôhypoth√®se nulle selon laquelle le taux d‚Äôexception est √©gale √† 0.01 au risque 5% de se tromper. Cel√† nous indique que la VaR gaussienne n‚Äôest pas performante. Ceci n‚Äôest pas surprenant √©tant donn√© que nous faisons une hypoth√®se sur la distribution des rendements qui n‚Äôest pas v√©rifi√©e."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-historique",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est bas√©e sur les rendements historiques. Elle est d√©finie comme l‚Äôoppos√© du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsid√©rons les mouvements de prix quotidiens pour l‚Äôindice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 sc√©narios ou cas qui serviront de guide pour les performances futures de l‚Äôindice, c‚Äôest-√†-dire que les 6513 derniers jours seront repr√©sentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour √† 99% correspond au 1er percentile de la distribution de probabilit√© des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique √† horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique √† horizon 1 jour est de 0.0396\n\n\nNous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que la VaR gaussienne. Le taux d‚Äôexception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 14 et en d√©duisons que le taux d‚Äôexception est 0.64%. Ce taux d‚Äôexception est statistiquement sup√©rieur √† 1% (car la pvaleur est d‚Äôenviron 0.95). Ainsi, la VaR historique est performante pour la p√©riode consid√©r√©e.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\nnp.float64(0.9682473859246779)"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-monte-carlo",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_application.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est bas√©e sur la simulation de trajectoires de rendements. Nous allons simuler jusqu‚Äô√† 10000 sc√©narios de rendements et calculer la VaR √† horizon 1 jour en posant une hypoth√®se de normalit√© sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise √† partir de 3000 sc√©narios. Nous utiliserons donc 3000 simulations de rendements. Nous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\nnp.float64(0.04399845832473067)\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que les deux autres VaRs. En effet, le taux d‚Äôexception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inf√©rieur √† 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\nnp.float64(0.9990895122494398)"
  },
  {
    "objectID": "posts/ensai/others/spearman-pearson.html",
    "href": "posts/ensai/others/spearman-pearson.html",
    "title": "Corr√©lation de Spearman vs corr√©lation de Pearson",
    "section": "",
    "text": "Corr√©lation de Spearman vs corr√©lation de Pearson\nLa corr√©lation de Spearman est une mesure de corr√©lation non param√©trique qui permet de mesurer la relation monotone entre deux variables. Elle est souvent utilis√©e pour mesurer la d√©pendance entre les variables al√©atoires. La corr√©lation de Spearman est bas√©e sur les rangs des observations et est moins sensible aux valeurs extr√™mes que la corr√©lation de Pearson. Elle est donc plus robuste et plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires.\nLa corr√©lation de Pearson, quant √† elle, ne permet de comparer les d√©pendances lin√©aires des variables. De plus, elle ne permet de comparer les corr√©lation que lorsque les variables al√©atoires sont normales. En effet, soit (\\(X_1,X_2,X_3\\)), si \\(\\rho(X_1,X_2) &gt; \\rho(X_1,X_3)\\), cela ne veut dire que la corr√©lation entre \\(X_1\\) et \\(X_2\\) est plus forte que celle entre \\(X_1\\) et \\(X_3\\) que si ces variables sont gaussiennes.\nNous verrons dans la suite de ce document que la corr√©lation de Spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires, car elle est une mesure de concordance. De ce fait, elle est d√©finit par une copule. Il n‚Äôen est pas ainsi pour la corr√©lation de Pearson.\n\nQu‚Äôest ce qu‚Äôune mesure de concordance ?\nUne mesure de concordance est une mesure qui permet de quantifier la relation entre deux variables al√©atoires. Cinq propri√©t√©s sont g√©n√©ralement attribu√©es √† une mesure de concordance :\n\nSym√©trie : la mesure de concordance entre X et Y est la m√™me que celle entre Y et X.\nNormalisation : la mesure de concordance est comprise entre -1 et 1.\n\\(\\delta(X_1, X_2) = 1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(U) \\right)\\)\n\\(\\delta(X_1, X_2) = -1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(1-U) \\right)\\)\n\\(\\delta(f(X_1), X_2) = \\delta(X_1, X_2)\\) si \\(f\\) est croissante; \\(\\delta(f(X_1), X_2) = -\\delta(X_1, X_2)\\) si \\(f\\) est d√©croissante.\n\nEn raison des propri√©t√©s 3 et 4, la corr√©lation de pearson n‚Äôest pas une mesure de concordance lorsque les variables al√©atoires ne sont pas gaussiennes. En effet, la corr√©lation de pearson ne v√©rifie pas la propri√©t√© 3. C‚Äôest pourquoi, la corr√©lation de spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires. Une mesure encore plus appropri√©e est la copule. La corr√©lation de spearman, quant √† elle, est une mesure de concordance.\n\n\nExemple :\nSoit un vecteur gaussien X = (\\(X_1, X_2, X_3\\)) suivant une loi N(0,\\(\\Sigma\\)) avec :\n\\[\n\\Sigma = \\begin{pmatrix}\n1 & 0.4 & 0.2 \\\\\n0.4 & 1 & -0.8 \\\\\n0.2 & -0.8 & 1\n\\end{pmatrix}\n\\]\n\nm &lt;- 3\nn &lt;- 2000\nsigma &lt;- matrix(c(1,0.4,0.2,0.4,1,-0.8,0.2,-0.8,1), nrow=3, ncol=3)\nX &lt;- mvrnorm(n,mu=rep(0,m),Sigma=sigma)\npairs.panels(X,method=\"spearman\")\n\n\n\n\n\n\n\n\nOn cr√©e ensuite un vecteur Z constitu√© des fonctions de r√©partition des √©l√©ments de X. On constate que la corr√©lation de Spearman entre les √©l√©ments de Z ne change pas. Cel√† s‚Äôexplique par le fait que nous appliquons une fonction croissante √† chaque √©l√©ment de X. Cependant, la distribution de Z est diff√©rente de celle de X. En effet, les composantes de Z suivent une loi uniforme \\(\\mathcal{U}(0,1)\\).\n\nZ &lt;- pnorm(X, 0,1)\npar(mfrow=c(1,2))\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"pearson\")\n\n\n\n\n\n\n\n\nA partir de Z, nous construisons un vecteur W tel que les composantes de W suivent des lois marginales diff√©rentes, resp \\(\\beta(0,1), \\gamma(2,1), \\beta(2,1)\\). La corr√©lation de Spearman entre les √©l√©ments de W ne change pas bien que la distribution des marginales ait chang√©, puisque nous appliquons une fonction croissante √† chaque √©l√©ment de Z.\n\nw1 &lt;- qbeta(Z[,1], 2, 1)\nw2 &lt;- qgamma(Z[,2], 2, 1)\nw3 &lt;- qbeta(Z[,3], 2,1)\nW &lt;- cbind(w1,w2,w3)\n\npar(mfrow=c(1,2))\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"pearson\")\n\n\n\n\n\n\n\n\nOn en conclut que la structure de d√©pendance d‚Äôun vecteur de variables al√©atoire peut √™tre isol√©e, caract√©ris√©ee, et mod√©lis√©e ind√©pendamment des lois marginales/distributions univari√©es des composantes du vecteur al√©atoire. Le concept de copule permet de mod√©liser cette structure de d√©pendance.\nSi l‚Äôon utilise la correlation de Pearson, on constate que la corr√©lation entre les √©l√©ments de W et Z change. En effet, la corr√©lation de Pearson est une mesure de corr√©lation lin√©aire et ne permet pas de comparer, xdans tous les cas, les d√©pendances entre les variables al√©atoires.\n\npairs.panels(X,method=\"pearson\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"spearman\")"
  },
  {
    "objectID": "posts/ensai/analyses_financieres/bilan_entreprise.html",
    "href": "posts/ensai/analyses_financieres/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "",
    "text": "L‚Äôanalyse financi√®re constitue l‚Äôensemble des outils permettant de donner un avis objectif d‚Äôune organisation (entreprises, fondations, etc.) sur la sant√© finani√®re et les risques financiers auxquels elle sera confront√©e. Il s‚Äôagit de determiner quels sont les crit√®res d‚Äôune sant√© financi√®re, qu‚Äôest le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le g√®re-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financi√®re d‚Äôune entreprise. Il s‚Äôagit du bilan et du compte de r√©sultat. Ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise. Comprendre comment ils fonctionnent permet de mieux appr√©hender la situation financi√®re d‚Äôune banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un √©tat des lieux de la situation patrimoniale de l‚Äôentreprise √† un moment donn√©. Il est compos√© de deux parties : l‚Äôactif et le passif. L‚Äôactif ou l‚Äôemploi regroupe l‚Äôensemble des biens et des droits de l‚Äôentreprise tandis que le passif regroupe l‚Äôensemble des ressources de l‚Äôentreprise (d‚Äôo√π vient l‚Äôargent et o√π peut-on s‚Äôen procurer). Le bilan est √©quilibr√© en valeur nette, c‚Äôest-√†-dire que l‚Äôactif est √©gal au passif.\nLe compte de r√©sultats, quant √† lui, est un document qui permet de faire un √©tat des lieux des performances de l‚Äôentreprise sur une p√©riode donn√©e (il r√©sume les b√©n√©fices ou pertes g√©n√©r√©es). Il est compos√© du d√©tail des produits et des charges de l‚Äôentreprise. Les produits sont les √©l√©ments qui g√©n√®rent des revenus pour l‚Äôentreprise tandis que les charges sont les √©l√©ments qui g√©n√®rent des d√©penses pour l‚Äôentreprise. Le compte de r√©sultat alimente par ailleurs la partie ‚Äúr√©sultat de l‚Äôexercice‚Äù du bilan comptable.\nLe coeur de l‚Äôentreprise √† analyser comme ressources suppl√©mentaires dans le compte de r√©sultat est l‚Äôensembles des charges financi√®res & exceptionnelles ainsi que l‚Äôensemble des produits d‚Äôexploitation et financiers. Ces √©l√©ments cl√©s permettent de d√©terminer la rentabilit√© de l‚Äôentreprise. En effet, si les charges sont sup√©rieures aux produits, l‚Äôentreprise est en perte. Si les produits sont sup√©rieurs aux charges, l‚Äôentreprise est en b√©n√©fice.\nIl est important de noter que ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "posts/ensai/analyses_financieres/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d‚Äôanalyser la situation financi√®re d‚Äôune entreprise, il faut donc le remodeler en un bilan ‚Äúfonctionnel‚Äù pour pouvoir l‚Äôanalyser. Le bilan fonctionnel est un document qui permet de faire un √©tat des lieux de la situation financi√®re de l‚Äôentreprise en fonction de son activit√©, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d‚Äôinvestissement √† long terme\nEmplois stables\n\nactifs immobilis√©s en valeur brute\n\nCycle de financement √† long terme\nRessources stables\n\nCapitaux propres,\nEmprunts √† long terme,\nAmortissements et d√©pr√©ciation,\nProvisions pour risques\n\n\n\nCycle d‚Äôexploitation\nEmplois d‚Äôexploitation\n\nStocks et encours\nCr√©ances\n\nCycle d‚Äôexploitation\nRessources d‚Äôexploitation\n\nDettes circulantes\n\n\n\nTr√©sorerie active\n\nDisponibilit√©s\n\nTr√©sorerie passive\n\nD√©couverts bancaires\n\n\n\n\nLes ressources stables font r√©f√©rence aux ressources saines du bilan etfont face aux emplois stables. La tr√©sorerie passive fait r√©f√©rence aux d√©couverts bancaires. Il est important de souligner qu‚Äôune tr√©sorerie passive est per√ßue n√©gativement dans le bilan fonctionnel. En effet, une tr√©sorerie passive signifie que l‚Äôentreprise a des dettes √† court terme qui ne sont pas couvertes par des actifs √† court terme d‚Äôo√π la n√©cessit√© d‚Äôavoir des d√©couverts bancaires.\nNb : La provision pour le risque peuve √™tre consid√©r√©e comme une ressource stable ou une ressource d‚Äôexploitation en fonction de l‚Äôentreprise. Tout d√©pend de la longevit√© des provisions.\n\nEquilibre financier\nNous dirons qu‚Äôil y a √©quilibre financier lorsque :\n\nLes emplois stables soient enti√®rement financ√©s par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a nec√©ssit√© d‚Äôun fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) d√©pend du cycle d‚Äôexploitation (entre autre, la rapidit√© de rotation des stocks et des cr√©ances). Il doit couvrir les besoins de financement du cycle d‚Äôexploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, cr√©ances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la tr√©sorerie (\\(\\text{Tr√©sorerie}=FDR-BFR\\)). Si la tr√©sorerie est positive, il y a √©quilibre financier. Cel√† signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d‚Äôexploitation. Lorsqu‚Äôil est n√©gatif, il faut trouver des ressources pour financer le cycle d‚Äôexploitation. Si la tr√©sorerie est nulle, il y a √©quilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-m√™me le cycle d‚Äôexploitation de l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le cr√©dit fournisseur. Il est important de noter que le cr√©dit fournisseur est une source de financement gratuite pour l‚Äôentreprise. C‚Äôest le cas des E-commerce o√π les acteurs encaissent leurs clients avant m√™me d‚Äôacheter les stocks aupr√®s des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transform√© en ressources en fonds de roulement, cel√† est une situation tr√®s favorable pour l‚Äôentreprise et est appel√©e ‚Äúcr√©dit inter-entreprises‚Äù."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "href": "posts/ensai/analyses_financieres/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Analyse du compte de r√©sultat",
    "text": "Analyse du compte de r√©sultat\nNous pouvons faire les m√™mes critiques faites au bilan comptable sur le compte de r√©sultat. En effet, le compte de r√©sultat est con√ßu de sorte √† fournir des informations au seul d√©tenteur du capital, √† savoir les actionnaires. Il fait apparaitre uniquement le b√©n√©fice ou la perte. C‚Äôest un document d‚Äôint√©r√™t pour l‚ÄôEtat pour d√©terminer si un pays est en croissance ou en r√©cession. Pour en faire un vrai diagnostic financier, il faut le d√©couper en sous-soldes appel√©s ‚Äúsoldes interm√©diaires de gestion‚Äù (SIG). Les SIG permettent de d√©terminer la rentabilit√© de l‚Äôentreprise, sa capacit√© d‚Äôautofinancement, sa capacit√© de remboursement, sa capacit√© de financement, etc.\nIl existe 9 soldes interm√©diaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajout√©e\nL‚Äôexc√©dent brut d‚Äôexploitation (EBE)\nLe r√©sultat d‚Äôexploitation\nLe r√©sultat courant avant imp√¥t\nLe r√©sultat exceptionnel\nLe r√©sultat net\nLa plus ou moins value de cession\n\nSelon la th√©orie de prise de d√©cisions, il y a deux grands types de d√©cisions : des d√©cisions qui permettent de cr√©er de la riches (Marge co., production et valeur ajout√©e) et des d√©cisions qui permettent de distribuer/d√©penser de la richesse (EBE, r√©sultat d‚Äôexploitation, r√©sultat courant avant imp√¥t, r√©sultat exceptionnel, r√©sultat net et plus ou moins value de cession). Lorsqu‚Äôon d√©pense la riches, il faudrait qu‚Äôelle soit bien d√©pens√©e.\n\nSoldes de cr√©ation de richesse\nLes soldes qui contribuent √† la cr√©ation de richesse sont la marge commerciale, la production et la valeur ajout√©e :\n\nLa marge commerciale est la diff√©rence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C‚Äôest un solde des entreprises commerciales (par exemple, les supermarch√©s). Pour une entreprise qui n‚Äôont pas de marchandises, le marge commerciale est nulle.\nLa production de l‚Äôexercice est la somme des produits vendus(\\(\\pm\\) les produits stock√©es) et des produits immobilis√©es par l‚Äôentreprise (certaines entreprises peuvent se vendre des produits √† elles-m√™mes). C‚Äôest un solde des entreprises industrielles.\nLa valeur ajout√©e est la richesse cr√©√©e par l‚Äôentreprise. C‚Äôest la somme des marges commerciales, de la production de l‚Äôexercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services ext√©rieurs).\n\nLa valeur ajout√© est un indicateur tr√®s suivi par l‚ÄôEtat pour d√©terminer le produit int√©rieur brut (PIB) afin de d√©terminer si un pays est en croissance ou en r√©cession. Par ailleurs, la valeur ajout√©e divis√©e par le nombre de salari√©s permet de d√©terminer le niveau de technicit√© de l‚Äôentreprise. Plus la valeur ajout√©e par salari√© est √©lev√©e, plus l‚Äôentreprise est techniquement avanc√©e.\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© √©conomique\nIl existe 5 tiers √† qui l‚Äôentreprise redistribue la VA (rang√©e par ordre de priorit√©) :\n\nLe personnel (√† travers les salaires),\nL‚ÄôEtat (√† travers les imp√¥ts),\nLe capital technique (via les amortissements),\nLes banques (via les int√©r√™ts),\nLes actionnaires ou les associ√©s (via le b√©n√©fice comptable)\n\nLes soldes qui permettent de financer l‚Äôactivit√© √©conomique (Etat, personnel, capital technique) sont l‚Äôexc√©dent brut d‚Äôexploitation et le r√©sultat d‚Äôexploitation :\n\nLe solde EBE r√©mun√®re le personnel et l‚ÄôEtat. Il repr√©sente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d‚Äôentreprise pour l‚ÄôEtat. Un EBE positif signifie que l‚Äôentreprise est capable de r√©mun√©rer le personnel et l‚ÄôEtat, et donc de financer l‚Äôemploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajout√©e} - \\text{Imp√¥ts, t√¢xes et versements assimil√©s} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de r√©sultat d‚Äôexploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de r√©mun√©rer le capital technique (machines etc.) et appartient √† tout ceux qui d√©pendent du capital financier et mesure les performances industrielles et commerciales de l‚Äôentreprise.\n\n\\[\\begin{align*}\n\\text{R√©sultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© financi√®re\nLes soldes qui permettent de financer l‚Äôactivit√© financi√®re (banques, actionnaires) sont le r√©sultat courant avant imp√¥t, le r√©sultat exceptionnel, le r√©sultat net et la plus ou moins value de cession :\n\nLe r√©sultat courant avant imp√¥t est le solde qui permet de r√©mun√©rer les banques. Il est un indicateur de la capacit√© de l‚Äôentreprise √† rembourser ses dettes et est un t√©moin de l‚Äôincidence de la politique financi√®re de l‚Äôentreprise sur son r√©sultat. Il faut distinguer les int√©r√™ts √† long terme et ceux de court terme. Plus ceux ci sont li√©s √† des dettes de court terme (ex. : d√©couverts), on peut dire que l‚Äôentreprise est en difficult√© financi√®re tandis que l‚Äôendettement √† long terme est un signe de bonne sant√© financi√®re, car il est voulu plut√¥t que subi. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat courant avant imp√¥t} &= \\text{R√©sultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financi√®res}\n\\end{align*}\\]\n\nLe r√©sultat exceptionnel est le solde qui est le moins analys√© car il est souvent li√© √† des √©v√®nements exceptionnels (ex. : vente d‚Äôun bien immobilier). Il est calcul√© comme √©tant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe r√©sultat net est le solde qui permet de r√©mun√©rer les actionnaires. C‚Äôest le solde en bas du compte de r√©sultat. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat net} &= \\text{R√©sultat courant avant imp√¥t} + \\text{R√©sultat exceptionnel} \\\\\n&- \\text{participations des salari√©s} - \\text{Imp√¥ts sur les b√©n√©fices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu‚Äôune entreprise vend une immobilisation. Ce ratio permet de d√©terminer si l‚Äôentreprise a vendu une immobilisation √† un prix sup√©rieur ou inf√©rieur √† sa valeur comptable. Cel√† constitue un temoin d‚Äôalerte sur la sant√© de l‚Äôentreprise et permet de d√©terminer si l‚Äôentreprise est en difficult√© financi√®re (car rien ne l‚Äôoblige √† vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "href": "posts/ensai/analyses_financieres/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "La capacit√© d‚Äôautofinancement",
    "text": "La capacit√© d‚Äôautofinancement\nLa capacit√© d‚Äôautofinancement (CAF) est un indicateur qui permet de d√©terminer si l‚Äôentreprise est capable de financer ses investissements sans recourir √† des financements ext√©rieurs. Elle regroupe la capacit√© √† d√©gager de la liquidit√©. Il n‚Äôy a pas de correspondance entre la tr√©sorerie et le b√©n√©fice. En effet, une entreprise peut √™tre en b√©n√©fice mais en difficult√© financi√®re. Pour la calculer, il faut √©liminer les sommes non encaissanles et non d√©caissables (ex. : Dotations, provision, reprise sur amortissements, les √©critures exceptionnelles).\nPour passer du b√©n√©fice √† la CAF, on ne conserve que les √©l√©ments qui sont encaissables et d√©caissables et est calcul√©e comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges d√©caissables (int√©r√™t bancaire, imp√¥t sur b√©n√©fice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu vers√© par l‚Äôentreprise √† ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas √† elle toute seule de d√©terminer l‚Äôautofinancement de l‚Äôentreprise. Dans le cadre l√©gal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu‚Äô√† 95% du b√©n√©fice comptable impos√© par l‚ÄôEtat et garder 5% √† l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende l√©gal‚Äù. Au del√† de 10%, les actionnaires peuvent retirer jusqu‚Äô√† 100% du b√©n√©fice comptable. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende statutaire‚Äù.\nAinsi, l‚Äôautofinancement est la somme qui reste de la CAF apr√®s le dividende l√©gal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaiss√©s par les actionnaires d√©termine la politique d‚Äôautofinancement de l‚Äôentreprise.\nL‚Äôautofinancement est essentiel pour l‚Äôentreprise car il permet de:\n\nrembourser les emprunts,\nam√©liorer la tr√©sorerie,\ncouvrir les risque de l‚Äôentreprise (provisions pour risque),\nfinancer l‚Äôexploitation (stocks & cr√©ances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-2.html",
    "href": "posts/ensai/gestion_actifs/TP-2.html",
    "title": "Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "",
    "text": "Le risque de liquidit√© est l‚Äôune des quatre grandes cat√©gories de risques auxquels une banque peut √™tre expos√©e. Il concerne √† la fois la liquidit√© du march√© et le risque op√©rationnel de financement.\nAinsi, le risque de liquidit√© est crucial dans la gestion d‚Äôun portefeuille, notamment en p√©riode de tensions sur les march√©s. Si un investisseur doit liquider un actif, il peut √™tre contraint de le vendre √† un prix inf√©rieur √† sa valeur fondamentale, entra√Ænant ainsi une perte importante.\nPour √©valuer ce risque, plusieurs indicateurs sont utilis√©s :\nDans le cadre de notre √©tude, nous nous concentrons sur le profil d‚Äô√©coulement d‚Äôun portefeuille compos√© de 10 actifs, et ce dans les quatre sc√©narios suivants :\n1. Conditions normales avec d√©formation : On consid√®re les volumes moyens de march√©, que l‚Äôon d√©forme pour obtenir les quantit√©s effectivement liquid√©es.\nLa liquidation d√©bute avec les actifs les plus liquides et se termine avec les plus illiquides.\nCe processus entra√Æne une d√©formation du portefeuille :\nles premiers investisseurs r√©cup√®rent une part plus liquide, tandis que ceux qui restent se retrouvent avec des actifs plus illiquides.\n2. Conditions normales sans d√©formation : Dans ce cas, on cherche √† pr√©server l‚Äô√©quilibre du portefeuille, afin de ne pas p√©naliser les investisseurs restants.\nLa liquidation est r√©partie uniform√©ment, sans privil√©gier les actifs les plus liquides.\n3. Conditions stress√©es avec d√©formation : Les march√©s sont en tension, les volumes chutent drastiquement.\nOn liquide en priorit√© les actifs les plus liquides, ce qui accentue l‚Äôilliquidit√© r√©siduelle du portefeuille.\n4. Conditions stress√©es sans d√©formation : M√™me en p√©riode de stress, on cherche √† conserver une proportionnalit√© √©quitable dans la liquidation, pour √©viter une concentration d‚Äôactifs illiquides pour les investisseurs restants.\n√âtapes de mise en ≈ìuvre\nPour chaque sc√©nario, on proc√®de selon les √©tapes suivantes :\nNous utiliserons un portefeuille fictif de 10 actifs du CAC, √† savoir :"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-2.html#pr√©sence-de-d√©formation",
    "href": "posts/ensai/gestion_actifs/TP-2.html#pr√©sence-de-d√©formation",
    "title": "Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Pr√©sence de d√©formation",
    "text": "Pr√©sence de d√©formation\nLa d√©formation dans le cadre de la liquidation d‚Äôun portefeuille d‚Äôactifs fait r√©f√©rence √† la mani√®re dont les quantit√©s liquid√©es sont r√©parties entre les diff√©rents actifs du portefeuille. En effet, lorsque l‚Äôon liquide un portefeuille, on ne peut pas simplement vendre tous les actifs en m√™me temps, compte tenu de leur liquidit√© respective. La d√©formation se produit lorsque l‚Äôon liquide d‚Äôabord les actifs les plus liquides, ce qui entra√Æne une concentration d‚Äôactifs illiquides dans le portefeuille restant. Cela peut avoir un impact significatif sur la valeur du portefeuille et sur le risque de liquidit√©.\n\nSous conditions normales avec d√©formation (waterfall liquidation)\nPour conna√Ætre la quantit√© d‚Äôactifs liquid√©s chaque jour, on fait l‚Äôhypoth√®se que les liquidations futures se feront aux prix observ√©s aujourd‚Äôhui. En pratique, ce que l‚Äôon peut r√©ellement liquider en une journ√©e correspond √† la quantit√© que l‚Äôon peut vendre sans impacter le prix, c‚Äôest-√†-dire min(quantit√© liquidable en 1 jour, quantit√© restant dans le portefeuille).\nSur cette base, on peut calculer la valeur de march√© du portefeuille, aussi bien au moment initial qu‚Äô√† chaque jour de liquidation. Cette valeur est g√©n√©ralement exprim√©e en pourcentage de l‚Äôencours total.\nOn peut ensuite calculer le cumul des pourcentages liquid√©s au fil des jours. Cette courbe cumulative repr√©sente ce que l‚Äôon appelle le profil d‚Äô√©coulement du portefeuille.\nDans le code ci-dessous, nous pr√©sentons les quantit√©s liquid√©es par jour pour chaque actif.\n\nüí° Par convention, le jour 0 correspond √† la quantit√© initialement d√©tenue dans le portefeuille, avant toute op√©ration de liquidation.\n\n\n#---------------------------#\n# Liquidation du portefeuille\n#---------------------------#\n\nADV[\"Quantity liquidated\"] = 0  # Initialement, rien n'est liquid√©\n\n# Au jour 0, on a liquid√© 0. La colonne 0 sert de quantit√© initiale\nquantity_liquidated_per_day = [ADV[\"Quantity\"]]\n\nwhile (ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]).sum() &gt; 0 : # Tant qu'il y a des liquidations √† faire\n    # Calculer la quantit√© liquide au jour i\n    liquidated_today = np.minimum(ADV[\"Quantity in 1day\"], ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"])\n    \n    # Mettre √† jour les quantit√©s liquid√©es dans le DataFrame\n    ADV[\"Quantity liquidated\"] += liquidated_today\n    \n    # Stocker les quantit√©s liquid√©es ce jour dans une liste\n    quantity_liquidated_per_day.append(liquidated_today)\n\nliquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\nliquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n\nliquidation_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n2044183.0\n391342.0\n391342.0\n391342.0\n391342.0\n391342.0\n87473.0\n0.0\n0.0\n\n\nGLE.PA\n901868.0\n420246.0\n420246.0\n61376.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nHO.PA\n80668.0\n47413.0\n33255.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nENGI.PA\n3417470.0\n826502.0\n826502.0\n826502.0\n826502.0\n111462.0\n0.0\n0.0\n0.0\n\n\nCAP.PA\n486075.0\n90080.0\n90080.0\n90080.0\n90080.0\n90080.0\n35675.0\n0.0\n0.0\n\n\nCA.PA\n1394173.0\n439345.0\n439345.0\n439345.0\n76138.0\n0.0\n0.0\n0.0\n0.0\n\n\nORA.PA\n5863363.0\n797115.0\n797115.0\n797115.0\n797115.0\n797115.0\n797115.0\n797115.0\n283558.0\n\n\nAC.PA\n599943.0\n116806.0\n116806.0\n116806.0\n116806.0\n116806.0\n15913.0\n0.0\n0.0\n\n\nOR.PA\n219549.0\n60868.0\n60868.0\n60868.0\n36945.0\n0.0\n0.0\n0.0\n0.0\n\n\nACA.PA\n2110202.0\n717541.0\n717541.0\n675120.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=5)\nprice_data = get_data(start_date, end_date, index, assets_ticker, column=\"Close\")\n\nprice_data[\"portfolio_data\"].head()\nprice_dict = price_data[\"portfolio_data\"].iloc[-1].to_dict()\n\nprint(\"=\"*50)\nprint(\"Prix des actifs √† la date du jour\")\nprint(\"=\"*50)\nfor ticker, price in price_dict.items():\n    print(f\"Le prix de l'actif {selected_assets[ticker]} est de {price} ‚Ç¨\")\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n==================================================\nPrix des actifs √† la date du jour\n==================================================\nLe prix de l'actif Accor est de 49.15999984741211 ‚Ç¨\nLe prix de l'actif Cr√©dit agricole est de 18.040000915527344 ‚Ç¨\nLe prix de l'actif Carrefour est de 14.880000114440918 ‚Ç¨\nLe prix de l'actif Capgemini est de 113.5999984741211 ‚Ç¨\nLe prix de l'actif Engie est de 26.09000015258789 ‚Ç¨\nLe prix de l'actif Soci√©t√© g√©n√©rale est de 73.41999816894531 ‚Ç¨\nLe prix de l'actif Thales est de 252.5 ‚Ç¨\nLe prix de l'actif L'Oreal est de 391.3999938964844 ‚Ç¨\nLe prix de l'actif Orange est de 16.459999084472656 ‚Ç¨\nLe prix de l'actif Sanofi est de 82.19999694824219 ‚Ç¨\n\n\n\n\n\n\n# Valeur liquide des actions par jour de liquidation\nmarket_value =[\n    price_dict[ticker] * liquidation_df.loc[ticker]\n    for ticker in selected_assets\n]\n\nmarket_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\nmarket_value\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n1.680318e+08\n3.216831e+07\n3.216831e+07\n3.216831e+07\n3.216831e+07\n3.216831e+07\n7.190280e+06\n0.000000e+00\n0.000000e+00\n\n\nGLE.PA\n6.621515e+07\n3.085446e+07\n3.085446e+07\n4.506226e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nHO.PA\n2.036867e+07\n1.197178e+07\n8.396888e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nENGI.PA\n8.916179e+07\n2.156344e+07\n2.156344e+07\n2.156344e+07\n2.156344e+07\n2.908044e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nCAP.PA\n5.521812e+07\n1.023309e+07\n1.023309e+07\n1.023309e+07\n1.023309e+07\n1.023309e+07\n4.052680e+06\n0.000000e+00\n0.000000e+00\n\n\nCA.PA\n2.074529e+07\n6.537454e+06\n6.537454e+06\n6.537454e+06\n1.132933e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nORA.PA\n9.651095e+07\n1.312051e+07\n1.312051e+07\n1.312051e+07\n1.312051e+07\n1.312051e+07\n1.312051e+07\n1.312051e+07\n4.667364e+06\n\n\nAC.PA\n2.949320e+07\n5.742183e+06\n5.742183e+06\n5.742183e+06\n5.742183e+06\n5.742183e+06\n7.822831e+05\n0.000000e+00\n0.000000e+00\n\n\nOR.PA\n8.593148e+07\n2.382373e+07\n2.382373e+07\n2.382373e+07\n1.446027e+07\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nACA.PA\n3.806805e+07\n1.294444e+07\n1.294444e+07\n1.217917e+07\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n\n\n\n\n\n\n# Calcul de la valeur de march√© initiale et totale\nmarket_value_0 = market_value.iloc[:, 0]\ntotal_market_value_0 = market_value_0.sum()\n\nprint(f\" La valeur de march√© initiale est de {total_market_value_0}‚Ç¨\")\n\n# Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\ncumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\ncumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\ncumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\ncumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\nweights = {}\nfor ticker in assets_ticker :\n    weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\nweights = pd.DataFrame(weights).T\nweights.head()\n\n La valeur de march√© initiale est de 669744530.421916‚Ç¨\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n0.250889\n0.271301\n0.309168\n0.348018\n0.367474\n0.167474\n0.0\n0.0\nNaN\n\n\nGLE.PA\n0.098866\n0.070610\n0.013435\n0.000000\n0.000000\n0.000000\n0.0\n0.0\nNaN\n\n\nHO.PA\n0.030413\n0.016767\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\nNaN\n\n\nENGI.PA\n0.133128\n0.134985\n0.137254\n0.119067\n0.027151\n0.000000\n0.0\n0.0\nNaN\n\n\nCAP.PA\n0.082447\n0.089829\n0.103613\n0.119298\n0.133380\n0.094394\n0.0\n0.0\nNaN\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 4))\n\n# Barplot empil√©\nbottom = None\nfor asset in weights.index:\n    plt.bar(\n        pd.to_numeric(weights.columns),  \n        weights.loc[asset],  \n        bottom=bottom,  \n        label=selected_assets[asset]  \n    )\n    bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\nplt.xlabel(\"Days of Liquidation\")\nplt.ylabel(\"Portfolio Weights\")\nplt.title(\"D√©formation du portefeuille\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nNous constatons avec ce profile de liquidation que les actifs les plus liquides sont liquid√©s en premier, ce qui entra√Æne une concentration d‚Äôactifs illiquides dans le portefeuille restant. Cela peut avoir un impact significatif sur la valeur du portefeuille et sur le risque de liquidit√©. En effet, √† la fin de la p√©riode de liquidation, le portefeuille est uniquement constitu√© de l‚Äôactif Orange, qui nous avons vu pr√©c√©demment est l‚Äôactif le plus illiquide.\n\nüí° Bon √† savoir : Pour un fonds de droit fran√ßais r√©glement√©, il est interdit d‚Äôinvestir plus de 5‚ÄØ% du portefeuille dans un seul √©metteur. Cependant, √† titre exceptionnel, il est possible d‚Äôinvestir jusqu‚Äô√† 10‚ÄØ% dans certains titres, √† condition que la somme des expositions sup√©rieures √† 5‚ÄØ% ne d√©passe pas 40‚ÄØ% du portefeuille. C‚Äôest ce que l‚Äôon appelle la r√®gle des 5/10/40, un ratio r√©glementaire applicable aux OPC (organismes de placement collectif). Toutes les pertes li√©es √† un non-respect de ce ratio doivent √™tre support√©es par la soci√©t√© de gestion. De plus, tout d√©passement doit √™tre d√©clar√© √† l‚ÄôAMF.\n\n\n# Valeur liquide du portefeuille\nmarket_value_df = pd.DataFrame()\n\nmarket_value_df[\"market_value\"] = market_value.sum(axis=0)\n\n# Calculer la valeur liquide relative par rapport au jour 0\nmarket_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n\n# Calculer la valeur cumul√©e liquide relative du portefeuille\nmarket_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n\nmarket_value_df\n\n\n\n\n\n\n\n\nmarket_value\nrelative value\ncumulative value\n\n\n\n\n0\n6.697445e+08\n1.000000\n0.000000\n\n\n1\n1.689594e+08\n0.252274\n0.252274\n\n\n2\n1.653845e+08\n0.246937\n0.499211\n\n\n3\n1.298741e+08\n0.193916\n0.693127\n\n\n4\n9.842074e+07\n0.146953\n0.840080\n\n\n5\n6.417214e+07\n0.095816\n0.935896\n\n\n6\n2.514576e+07\n0.037545\n0.973441\n\n\n7\n1.312051e+07\n0.019590\n0.993031\n\n\n8\n4.667364e+06\n0.006969\n1.000000\n\n\n\n\n\n\n\n\nmarket_value_df = market_value_df.iloc[1:]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,  # Center text\n        height,  # Position slightly above the bar\n        f'{height:.2f}',  # Format with 2 decimal places\n        ha='center',  # Center horizontally\n        va='bottom',  # Position text at the bottom\n        fontsize=10, color=\"black\"\n    )\n\nplt.xlabel(\"Days\")\nplt.ylabel(\"Cumulative Value (%)\")\nplt.title(\"Profil de liquidation du portefeuille\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nPour voir ce qui arrive au profil d‚Äô√©coulement lorsque les quantit√©s varient, on va utiliser un facteur de modulation de la quantit√©. Cela permet de d√©terminer quelle est la taille cible du portefeuille qui permet d‚Äôavoir la liquidit√© pour un certain niveau en nombre de jours qu‚Äôon se fixe. Cet exercice est fait une seule fois √† l‚Äôinitialisation du portefeuille.\nPour facilier l‚Äôimpl√©mentation, nous allons utiliser une fonction liquidation_profile qui va nous permettre de calculer le profil d‚Äô√©coulement du portefeuille, en int√©grant le facteur de modulation. Lorsqu‚Äôon ne souhaite pas de modulation, on peut simplement passer un facteur de modulation de 1.\n\ndef waterfall_liquidation(ADV, price_dict, selected_assets, fact_modulation=0.30, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumul√©es.\n    \"\"\"\n    \n    # Initialisation des quantit√©s liquid√©es\n    ADV = ADV.copy()\n    ADV[\"Quantity\"] = round(ADV[\"Quantity\"] * fact_modulation)\n\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"]]\n    \n    time_elapsed = 0 \n    # Calcul des quantit√©s liquid√©es par jour\n    while (ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]).sum() &gt; 0 :\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]\n        )\n        ADV[\"Quantity liquidated\"] += liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n        time_elapsed += 1\n    \n    print(f\"Temps de liquidation du portefeuille : {time_elapsed} jours\")\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    #---------------------------#\n    # Calcul des poids par jour #\n    #---------------------------#\n\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(10, 4))\n        \n\n        # Barplot empil√©\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns), \n                weights.loc[asset],  \n                bottom=bottom, \n                label=selected_assets[asset]  \n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    #---------------------------#\n    #   Profil de liquidation   #\n    #---------------------------#\n\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n        bars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(\n                bar.get_x() + bar.get_width() / 2,  # Center text\n                height,  # Position slightly above the bar\n                f'{height:.2f}',  # Format with 2 decimal places\n                ha='center',  # Center horizontally\n                va='bottom',  # Position text at the bottom\n                fontsize=10, color=\"black\"\n            )\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n\n    return market_value_df, market_value, weights\n\nNous constatons qu‚Äôen utilisant un facteur de modulation de 0.3, le temps de liquidation est r√©duit √† 3 jours, ce qui est plus rapide que le temps de liquidation initial de 8 jours. Cela montre l‚Äôimpact significatif de la modulation sur le profil de liquidation du portefeuille.\n\nfact_modulation= 0.3\nnew_market_value_df, new_market_value, new_weights = waterfall_liquidation(ADV, price_dict, selected_assets, fact_modulation, plot_graphs=True)\n\nTemps de liquidation du portefeuille : 3 jours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSous conditions stress√©es avec d√©formation\nPour stresser le portefeuille, √† la baisse, on va diviser la profondeur de march√© par 2. Cela signifie que l‚Äôon peut vendre moins d‚Äôactifs sans impacter le prix de mani√®re significative. En cons√©quence, le temps de liquidation augmente, car il faut plus de temps pour liquider la m√™me quantit√© d‚Äôactifs. Il est √©galement possible d‚Äôavoir des conditions stress√©es √† la hausse, o√π l‚Äôon multiplie la profondeur de march√© par 2. Cela signifie que l‚Äôon peut vendre plus d‚Äôactifs sans impacter le prix de mani√®re significative. En cons√©quence, le temps de liquidation diminue, car il faut moins de temps pour liquider la m√™me quantit√© d‚Äôactifs.\nDans notre cas, en stressant le portefeuille √† la baisse, nous constatons que le temps de liquidation augmente. On passe de 8 jours de liquidatin √† 15 jours.\n\n#---------------------------#\n# Stress Test\n#---------------------------#\nADV_stressed = ADV.copy()\n\n# Quantit√© journali√®re\nmarket_depth = (20/100)/2  # On stresse la liquidit√© √† la baisse\nADV_stressed[\"Quantity in 1day\"] = round(ADV_stressed[\"ADV\"] * market_depth)\n\n# Calcul du nombre de jours de liquidation\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Quantity\"]/ADV_stressed[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\n\ntime_elapsed = ADV_stressed['Days of liquidation'].max()\nprint(f\"Temps de liquidation du portefeuille stress√©: {time_elapsed} jours\")\n\nTemps de liquidation du portefeuille stress√©: 15 jours\n\n\n\nstressed_market_value_df, stressed_market_value, stressed_weights = waterfall_liquidation(ADV=ADV_stressed, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\nTemps de liquidation du portefeuille : 15 jours"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-2.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "href": "posts/ensai/gestion_actifs/TP-2.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "title": "Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Absence de d√©formation du portefeuille (pro forma)",
    "text": "Absence de d√©formation du portefeuille (pro forma)\nNous avons vu dans la section pr√©c√©dente que, lors de la liquidation d‚Äôun portefeuille, on a tendance √† commencer par les actifs les plus liquides. Cela conduit √† une concentration progressive d‚Äôactifs illiquides dans le portefeuille r√©siduel, ce qui peut affecter significativement sa valeur et accro√Ætre le risque de liquidit√©.\nDans cette section, nous allons √©tudier comment √©viter cette d√©formation en proc√©dant √† une liquidation proportionnelle : l‚Äôobjectif est de pr√©server la r√©partition initiale du portefeuille tout au long du processus de liquidation.\nPour cela, on commence par estimer, comme pr√©c√©demment, la quantit√© liquidable en un jour pour chaque titre. Cette estimation permet d‚Äôen d√©duire le pourcentage liquidable quotidien par rapport √† la position totale sur chaque actif. Si l‚Äôon souhaite que tous les titres soient liquid√©s √† la m√™me vitesse, il faut adopter le rythme de liquidation de l‚Äôactif le plus lent. Ainsi, on calcule le pourcentage liquidable en un jour pour chaque titre, puis on en retient le minimum. Ce minimum d√©finit alors le pourcentage quotidien de liquidation appliqu√© √† l‚Äôensemble du portefeuille. Ce proc√©d√© allonge la dur√©e totale de liquidation, mais il permet de conserver une structure de portefeuille stable. N√©anmoins, une l√©g√®re d√©formation peut subsister, notamment en raison des arrondis et des limites pratiques de liquidit√© sur certains actifs.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef pro_forma_liquidation(ADV, price_dict, selected_assets, fact_modulation=0.30, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation pro forma et visualise les graphiques des poids et des valeurs cumul√©es.\n    \"\"\"\n\n    ADV = ADV.copy()\n    ADV[\"Quantity\"] = ADV[\"Quantity\"] * fact_modulation\n\n    time_elapsed = ADV['Days of liquidation'].max()\n\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"]]\n    \n    min_liquidated_today = np.ones(len(ADV[\"Quantity\"]))\n    time_elapsed = 0 \n\n    # Calcul des quantit√©s liquid√©es par jour\n    while min_liquidated_today.sum() &gt; 0:  \n\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]\n        ) / ADV[\"Quantity\"]\n        min_liquidated_today = round(np.min(liquidated_today) * ADV[\"Quantity\"])# On liquide √† la vitesse de l'actif le moins liquide\n        ADV[\"Quantity liquidated\"] += min_liquidated_today\n        quantity_liquidated_per_day.append(min_liquidated_today)\n        time_elapsed += 1\n    \n    print(f\"Temps de liquidation du portefeuille : {time_elapsed} jours\")\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    #---------------------------#\n    # Calcul des poids par jour\n    #---------------------------#\n\n    # Calcul de la valeur de march√© initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n    \n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns), \n                weights.loc[asset],  \n                bottom=bottom, \n                label=selected_assets[asset]  \n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n        plt.show()\n\n    \n    #---------------------------#\n    #   Profil de liquidation   #\n    #---------------------------#\n    \n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n        bars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(\n                bar.get_x() + bar.get_width() / 2,  # Center text\n                height,  # Position slightly above the bar\n                f'{height:.2f}',  # Format with 2 decimal places\n                ha='center',  # Center horizontally\n                va='bottom',  # Position text at the bottom\n                fontsize=10, color=\"black\"\n            )\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nSous condition normale sans d√©formation\n\nproforma_market_value_df, proforma_market_value, proforma_weights = pro_forma_liquidation(ADV=ADV, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\nTemps de liquidation du portefeuille : 9 jours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSous conditions stress√©es sans d√©formation\n\nproforma_s_market_value_df, proforma_s_market_value, proforma_s_weights = pro_forma_liquidation(ADV=ADV_stressed, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\nTemps de liquidation du portefeuille : 16 jours"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-2.html#conclusion",
    "href": "posts/ensai/gestion_actifs/TP-2.html#conclusion",
    "title": "Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Conclusion",
    "text": "Conclusion\nLa liquidation d‚Äôun portefeuille est un exercice complexe qui n√©cessite de prendre en compte plusieurs facteurs : la liquidit√© des actifs, la taille du portefeuille, et les conditions de march√©. En utilisant des outils tels que le profil d‚Äô√©coulement (ou profil de liquidation), les investisseurs peuvent mieux anticiper l‚Äôimpact potentiel de leurs transactions sur les prix de march√©, et ainsi minimiser l‚Äôeffet de march√©.\nLe processus de liquidation choisi peut conduire √† des rythmes de sortie diff√©rents, et il est essentiel d‚Äôen tenir compte dans la gestion active du portefeuille.\nPour pr√©server la liquidit√© d‚Äôun portefeuille, notamment en p√©riode de tension sur les march√©s, plusieurs m√©canismes r√©glementaires et op√©rationnels existent. Ils visent √† prot√©ger les investisseurs restants et √† maintenir la qualit√© du portefeuille. En cas de crise financi√®re ou de forte volatilit√©, les soci√©t√©s de gestion de portefeuille (SGP) peuvent activer ces dispositifs, conform√©ment aux dispositions pr√©vues par la r√©glementation.\nCes mesures permettent d‚Äô√©viter des ventes forc√©es d‚Äôactifs et de limiter les effets de contagion sur le reste du portefeuille.\nPrincipaux m√©canismes de gestion de la liquidit√© :\n\nLes gates (plafonnement de rachat)\nLes gates permettent de limiter les rachats quotidiens √† un certain pourcentage de l‚Äôactif net du fonds. En g√©n√©ral, si les rachats d√©passent 5‚ÄØ% de l‚Äôactif net, la SGP a le droit (mais non l‚Äôobligation) de n‚Äôhonorer que les premiers 5‚ÄØ% et de reporter le reste sur les jours suivants, en fonction des conditions de march√©.\nCe m√©canisme permet de r√©duire l‚Äôimpact sur les prix de march√©, en √©vitant une vente massive d‚Äôactifs en un seul jour. C‚Äôest une mesure de protection des porteurs restants, bien que son activation soit souvent per√ßue comme un signal n√©gatif.\nL‚Äôexistence du m√©canisme des gates doit figurer dans le prospectus du fonds, sauf justification sp√©cifique de la SGP. Il convient de noter que l‚Äôactivation des gates est optionnelle.\nLa suspension des souscriptions et des rachats\nIl s‚Äôagit d‚Äôune mesure plus radicale, qui consiste √† geler temporairement les op√©rations d‚Äôentr√©e et de sortie du fonds. Elle est utilis√©e dans des situations exceptionnelles, telles que des crises de march√© ou une volatilit√© extr√™me, lorsque la valorisation des actifs devient incertaine ou que la liquidit√© dispara√Æt. Ce m√©canisme vise √† : pr√©server l‚Äô√©galit√© de traitement entre les investisseurs, √©viter des ventes pr√©cipit√©es d‚Äôactifs, stabiliser la structure du portefeuille. La suspension doit √™tre justifi√©e et temporaire, et elle est lev√©e d√®s que les conditions de march√© se normalisent.\nLes m√©canismes de d√©formation du portefeuille\nIl s‚Äôagit de mesures proactives consistant √† adapter la composition du portefeuille lors des rachats, pour pr√©server la liquidit√© r√©siduelle. Concr√®tement, la SGP peut choisir de vendre en priorit√© les actifs les plus liquides, ce qui permet de r√©pondre rapidement aux demandes de rachat sans impacter significativement les prix. Ces m√©canismes sont particuli√®rement utilis√©s en cas de rachats massifs, pour : limiter l‚Äôimpact de march√©, prot√©ger les investisseurs restants, √©viter une d√©stabilisation du portefeuille.\nN√©anmoins, cela entra√Æne une d√©formation du portefeuille, c‚Äôest-√†-dire que les investisseurs qui restent se retrouvent avec une part moins liquide du portefeuille initial. Ce compromis doit √™tre g√©r√© avec prudence."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-1.html",
    "href": "posts/ensai/gestion_actifs/TP-1.html",
    "title": "Le risque de march√© en asset management",
    "section": "",
    "text": "La gestion d‚Äôactifs (ou asset management) est l‚Äôart de g√©rer un portefeuille d‚Äôactifs financiers. Il s‚Äôagit de g√©rer l‚Äôargent de clients ‚Äî particuliers ou institutionnels ‚Äî avec pour objectif principal de maximiser le rendement tout en minimisant le risque.\nLes √©tapes usuelles de gestion du risque\nOn distingue g√©n√©ralement quatre grandes cat√©gories de risques :\nCe sont des mesures quantitatives du risque. Parmi les plus utilis√©es : la volatilit√©, qui mesure la variation des rendements d‚Äôun actif par rapport √† sa moyenne; la Value at Risk (VaR), qui mesure la perte maximale anticip√©e sur un portefeuille, avec un certain niveau de confiance \\(\\alpha\\), sur un horizon \\(T\\), la Tracking Error, qui mesure l‚Äô√©cart de performance entre un portefeuille et son indice de r√©f√©rence.\nIl s‚Äôagit de mettre en place des r√®gles de gestion ou des limites pour √©viter des d√©rives du portefeuille. Cela peut inclure :\nConstitution du portefeuille\nDans notre √©tude, nous allons constituer un portefeuille de 10 actions choisies dans l‚Äôindice CAC 40, en leur attribuant des poids al√©atoires.\nLes actifs retenus sont les suivants :\nPuisque nous travaillons avec un portefeuille d‚Äôactions, donc le principal risque est le risque de march√© actions. Nous allons donc nous int√©resser √† trois indicateurs de risque : - la volatilit√© ex-ante, - la Value at Risk ex-ante, - la Tracking Error ex-ante\nc‚Äôest-√†-dire des mesures anticip√©es, bas√©es sur la composition actuelle du portefeuille, et non sur des donn√©es historiques (ex-post)."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-1.html#r√©cup√©ration-des-donn√©es",
    "href": "posts/ensai/gestion_actifs/TP-1.html#r√©cup√©ration-des-donn√©es",
    "title": "Le risque de march√© en asset management",
    "section": "R√©cup√©ration des donn√©es",
    "text": "R√©cup√©ration des donn√©es\n\n#------------------------------------#\n#---------- Package Imports ---------#\n#------------------------------------#\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\ndef get_data(start_date, end_date, index_ticker, tickers):\n    \"\"\"\n    Extraction de donn√©es de cours d'actions\n    \"\"\"\n    # Extraction des prix historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    # Extraction des prix historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\n\nend_date = pd.to_datetime(\"2025-01-27\")\nstart_date = end_date - timedelta(days=2*365)\n\nselected_assets = {\n    \"SAN.PA\" : \"Sanofi\",\n    \"GLE.PA\" : \"Soci√©t√© g√©n√©rale\",\n    \"HO.PA\" : \"Thales\",\n    \"ENGI.PA\" : \"Engie\",\n    \"CAP.PA\" : \"Capgemini\",\n    \"CA.PA\" : \"Carrefour\",\n    \"ORA.PA\" : \"Orange\",\n    \"AC.PA\" : \"Accor\",\n    \"OR.PA\" : \"L'Oreal\",\n    \"ACA.PA\" : \"Cr√©dit agricole\"\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker)\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.tail()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nCA.PA\nCAP.PA\nENGI.PA\nGLE.PA\nHO.PA\nOR.PA\nORA.PA\nSAN.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-01-20\n46.959301\n13.270039\n12.445763\n158.253357\n14.549852\n28.472115\n147.275269\n334.734375\n9.728536\n94.679825\n\n\n2025-01-21\n47.017685\n13.265357\n12.381419\n159.719574\n14.453160\n28.331999\n148.801163\n335.127289\n9.761625\n94.842651\n\n\n2025-01-22\n47.728012\n13.167060\n12.082685\n158.839844\n14.296611\n28.331999\n149.785629\n343.426880\n9.671809\n94.679825\n\n\n2025-01-23\n47.270679\n13.335569\n12.156219\n160.403824\n14.273589\n28.974592\n150.474777\n348.239655\n9.648173\n95.675957\n\n\n2025-01-24\n47.659897\n13.316846\n12.225158\n163.727234\n14.273589\n29.051897\n149.933319\n352.070282\n9.600902\n95.934570\n\n\n\n\n\n\n\n\nbenchmark_data = data[\"benchmark_data\"]\nbenchmark_data.head()\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2023-01-30\n7082.009766\n\n\n2023-01-31\n7082.419922\n\n\n2023-02-01\n7077.109863\n\n\n2023-02-02\n7166.270020\n\n\n2023-02-03\n7233.939941\n\n\n\n\n\n\n\n\n# On attribue des poids √©quitables pour chaque action\nweights_by_asset = {ticker: 1 / len(assets_ticker) for ticker in assets_ticker}\n\nPour connaitre la valeur totale des actifs du portefeuille, nous allons utiliser la notion d‚Äôasset under management (AUM) d√©fini comme suit :\n\\[\nAUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times P_i(T_n),\n\\]\no√π \\(\\omega_i\\) est le poids de l‚Äôactif \\(i\\) dans le portefeuille et \\(P_i(T_n)\\) est le prix de l‚Äôactif \\(i\\) √† la date \\(T_n\\).\nPuisque les rendements sont les seuls facteurs de risque de l‚ÄôAUM, nous allons nous int√©resser √† la variation de l‚ÄôAUM entre deux dates \\(T_n\\) et \\(T_{n+1}\\), soit :\n\\[\n\\Delta AUM(T_n, T_{n+1}) = AUM(T_{n+1}) - AUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times (P_i(T_{n+1}) - P_i(T_n)).\n\\]\n\naum_series = portfolio_data.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\naum_series\n\nAUM = pd.DataFrame(aum_series, columns=[\"AUM\"])\nAUM.head()\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-01-30\n81.287687\n\n\n2023-01-31\n81.069776\n\n\n2023-02-01\n80.829767\n\n\n2023-02-02\n81.927702\n\n\n2023-02-03\n82.645898\n\n\n\n\n\n\n\n\n# --------------------------------------------- #\n# Evolution de la valeur totale du portefeuille\n# ---------------------------------------------- #\n\nplt.figure(figsize=(12, 4))\nplt.plot(AUM, label=\"AUM\")\nplt.title(\"Evolution de l'actif sous gestion\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# On s'interesse aux variations/rendements de l'AUM\nAUM[\"Variation\"] = AUM[\"AUM\"].pct_change()\nAUM[\"Variation\"].head()\n\nDate\n2023-01-30         NaN\n2023-01-31   -0.002681\n2023-02-01   -0.002961\n2023-02-02    0.013583\n2023-02-03    0.008766\nName: Variation, dtype: float64\n\n\n\nEstimation de la volatilit√©\nPour estimer la volatilit√© du portefeuille, on peut calculer l‚Äô√©cart-type des variations de l‚ÄôAUM. On fait le choix de calculer une volatilit√© ex-ante en se basant sur les variation historiques des prix des actifs avec une profondeur historique de 2 ans. Vu qu‚Äôon a une volatilit√© quotidienne, on va l‚Äôannualiser en multipliant par \\(\\sqrt{252}\\).\nEn g√©n√©ral, sur le march√© action, la volatilit√© quotidienne est environ de 1% et la volatilit√© annuelle est entre 10% et 20%.\n\n# Calcul de la volatilit√© du portefeuille\nvolatility_portfolio = np.std(AUM[\"Variation\"])\nannualized_volatility_portfolio = volatility_portfolio * np.sqrt(252)\nprint(f\"Volatilit√© de la performance quotidienne : {volatility_portfolio : .2%}\")\nprint(f\"Volatilit√© de la performance annuelle : {annualized_volatility_portfolio : .2%}\")\n\nVolatilit√© de la performance quotidienne :  0.87%\nVolatilit√© de la performance annuelle :  13.76%\n\n\n\n# Calcul de la volatilit√© de l'indice CAC 40\n\nbenchmark_data[\"Variation\"] = benchmark_data[\"^FCHI\"].pct_change()\nvolatility_benchmark = np.std(benchmark_data[\"Variation\"])\nannualized_volatility_benchmark = volatility_benchmark * np.sqrt(252)\n\nprint(f\"Volatilit√© de l'indice CAC 40 : {volatility_benchmark : .2%}\")\nprint(f\"Volatilit√© de l'indice CAC 40 annuelle : {annualized_volatility_benchmark : .2%}\")\n\nVolatilit√© de l'indice CAC 40 :  0.84%\nVolatilit√© de l'indice CAC 40 annuelle :  13.37%\n\n\nNotre portefeuille nous fournit une volatatilit√© quotidienne sup√©rieure de 3bps √† la volatilit√© du CAC 40. On retrouve sur √† peu pr√®s la m√™me volatilit√© du portefeuille et celle du CAC 40. Il y a donc une certaine homog√©n√©it√© dans le portefeuille que nous avons constitu√©.\n\nüí° Note : bp = 0,01%\n\n\n\nEstimation de la tracking error/erreur de suivi\nLa tracking error est une mesure de l‚Äô√©cart entre la performance d‚Äôun portefeuille et celle de son indice de r√©f√©rence. Elle est calcul√©e comme la volatilit√© de la diff√©rence entre les rendements du portefeuille et de l‚Äôindice de r√©f√©rence :\n\\[\nTE = \\sqrt{Var(R_p - R_b)}\n\\]\nLa tracking error mesure l‚Äôincercitude du portefeuille par rapport √† l‚Äôindice de r√©f√©rence, c‚Äôest une mesure relative. Plus la tracking error est √©lev√©e, plus le portefeuille est risqu√©. On ne souhaite sous ou sur-performer l‚Äôindice de r√©f√©rence. On souhaite suivre v√©ritablement l‚Äôindice de r√©f√©rence.\nPour l‚Äôannualiser, on multiplie par \\(\\sqrt{252}\\) en supposant que les performances quotidiennes sont ind√©pendantes et donc un utilise l‚Äôadditivit√© des variances.\n\nperformance_relative = AUM[\"Variation\"] - benchmark_data[\"Variation\"]\n\nplt.figure(figsize=(12, 4))\nplt.plot(performance_relative, label=\"Performance\")\nplt.title(\"Performance du portefeuille par rapport √† l'indice CAC 40\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Performance\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcul de la tracking error\n\nTE = np.std(AUM[\"Variation\"] - benchmark_data[\"Variation\"]) \nprint(f\"Tracking error : {TE : .2%}\")\n\nTE_annualized = TE * np.sqrt(252)\nprint(f\"Tracking error annualis√© : {TE_annualized : .2%}\")\n\nTracking error :  0.51%\nTracking error annualis√© :  8.05%\n\n\n\n\nEstimation de la Value-at-Risk (VaR)\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de rescaling, i.e.¬†\\(VaR = \\sigma \\times \\Phi^{-1}(\\alpha)\\). 2. Approche param√©trique : On suppose que les rendements suivent une loi normale. 3. Approche Monte Carlo : On simule les rendements futurs.\nPuisqu‚Äôon s‚Äôint√©resse √† un portefeuille d‚Äôactions qui a un indice de r√©f√©rence, on peut √©galement calculer la VaR relative. La VaR relative suit une philosophie proche du tracking error. Elle se calcule sur les √©carts entre le portefeuille et le benchmark. Elle sert √† mesurer de combien mon portefeuille sous-performe par rapport √† l‚Äôindice de r√©f√©rence.\n\n# --------------------------------- #\n# VaR historique\n# --------------------------------- #\n\nseuil = 99/100\n\nVaR_hist_portfolio = np.percentile(AUM[\"Variation\"].dropna(), 100*(1- seuil))\nprint(f\"VaR historique sur le portefeuille : {- VaR_hist_portfolio : .2%}\")\nprint(f\"VaR historique sur 20 jours sur le portefeuille : {-VaR_hist_portfolio*np.sqrt(20) : .2%}\")\n\nprint(\"=*=\"*10)\n\nVaR_hist_benchmark = np.percentile(benchmark_data[\"Variation\"].dropna(), 100*(1 - seuil))\nprint(f\"VaR historique sur l'indice CAC 40 : {-VaR_hist_benchmark : .2%}\")\nprint(f\"VaR historique sur 20 jours sur l'indice CAC 40 : {-VaR_hist_benchmark*np.sqrt(20) : .2%}\")\n\nVaR historique sur le portefeuille :  2.31%\nVaR historique sur 20 jours sur le portefeuille :  10.32%\n=*==*==*==*==*==*==*==*==*==*=\nVaR historique sur l'indice CAC 40 :  2.13%\nVaR historique sur 20 jours sur l'indice CAC 40 :  9.53%\n\n\n\n# ---------------------------------- #\n# VaR param√©trique\n# ---------------------------------- #\n\n# PnL ~ N(mu, sigma) ==&gt; PnL = mu + sigma * Z, o√π Z ~ N(0,1)\n# P(PnL &gt; -VaR) = alpha &lt;=&gt; P(mu + sigma * Z &gt; -VaR) = alpha &lt;=&gt; P(Z &lt; (-VaR - mu) / sigma) = 1 - alpha\n# Donc, -VaR = mu + sigma * quantile(1 - alpha), o√π quantile(1 - alpha) est le quantile de la loi normale standard\n\n\nmu = np.mean(AUM[\"Variation\"].dropna())\nprint(f\"mu sur le portefeuille : {mu : .2}\")\nsigma = np.std(AUM[\"Variation\"].dropna())\nprint(f\"sigma sur le portefeuille : {sigma : .2}\")\n\nVaR_param_portfolio  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_portfolio : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_portfolio * np.sqrt(20): .2%}\")\n\nprint(\"=*=\"*10)\n\nmu = np.mean(benchmark_data[\"Variation\"].dropna())\nprint(f\"mu sur le benchmark: {mu : .2}\")\nsigma = np.std(benchmark_data[\"Variation\"].dropna())\nprint(f\"sigma sur le benchmark : {sigma : .2}\")\n\nVaR_param_benchmark  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_benchmark : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_benchmark * np.sqrt(20): .2%}\")\n\nmu sur le portefeuille :  0.00021\nsigma sur le portefeuille :  0.0087\nVaR param√©trique sur le portefeuille :  1.99%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.92%\n=*==*==*==*==*==*==*==*==*==*=\nmu sur le benchmark:  0.00026\nsigma sur le benchmark :  0.0084\nVaR param√©trique sur le portefeuille :  1.93%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.65%\n\n\n\n# ---------------------------------- #\n# VaR relative\n# ---------------------------------- #\n\nVaR_hist_relative = np.percentile(performance_relative.dropna(), 100*(1- seuil))\nprint(f\"VaR historique relative : {- VaR_hist_relative : .2%}\")\nprint(f\"VaR historique relative sur 20 jours : {-VaR_hist_relative*np.sqrt(20) : .2%}\")\n\nprint(\"=*=\"*10)\n\nmu = np.mean(performance_relative.dropna())\nprint(f\"mu des performances relatives: {mu : .2}\")\nsigma = np.std(performance_relative.dropna())\nprint(f\"sigma des performances relatives : {sigma : .2}\")\n\nVaR_param_relative  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique relative : {VaR_param_relative : .2%}\")\nprint(f\"VaR param√©trique relative sur 20 jours : {VaR_param_relative * np.sqrt(20): .2%}\")\n\nVaR historique relative :  1.08%\nVaR historique relative sur 20 jours :  4.83%\n=*==*==*==*==*==*==*==*==*==*=\nmu des performances relatives: -4.7e-05\nsigma des performances relatives :  0.0051\nVaR param√©trique relative :  1.18%\nVaR param√©trique relative sur 20 jours :  5.29%"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-1.html#conclusion",
    "href": "posts/ensai/gestion_actifs/TP-1.html#conclusion",
    "title": "Le risque de march√© en asset management",
    "section": "Conclusion",
    "text": "Conclusion\nLe risque de march√© constitue un enjeu central dans la gestion d‚Äôactifs. Il est inh√©rent √† toute exposition aux march√©s financiers et doit, √† ce titre, √™tre mesur√©, surveill√© et encadr√© avec rigueur afin de pr√©server les int√©r√™ts des investisseurs.\nDes outils tels que la volatilit√©, la tracking error et la Value at Risk (VaR) permettent de quantifier l‚Äôincertitude li√©e aux rendements du portefeuille et d‚Äôanticiper les pertes potentielles dans des conditions normales de march√©.\nEn compl√©ment, les stress tests jouent un r√¥le fondamental : ils permettent d‚Äô√©valuer la r√©silience du portefeuille face √† des sc√©narios extr√™mes, souvent absents des donn√©es historiques, mais pourtant plausibles dans un contexte de crise."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nCUNY Baruch College Visiting Scholar Paperwork Guide\n\n\nJan 3, 2025\n\n\n\n\nNew York State Climate Policy Goals Clarified\n\n\nNov 1, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html",
    "href": "posts/ensai/gestion_actifs/TP-3.html",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "",
    "text": "Le risque de valorisation est le risque que la valeur d‚Äôun actif financier soit inf√©rieure √† sa valeur comptable ou √† sa valeur de march√©. Ce risque peut √™tre caus√© par des fluctuations des taux d‚Äôint√©r√™t, des changements dans la qualit√© de cr√©dit de l‚Äô√©metteur, ou d‚Äôautres facteurs √©conomiques et financiers. Dans le cadre de nos travaux, nous allons nous interesser √† la valorisation d‚Äôune obligation."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#i.-valorisation-dune-obligation-sans-risque-de-d√©faut",
    "href": "posts/ensai/gestion_actifs/TP-3.html#i.-valorisation-dune-obligation-sans-risque-de-d√©faut",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "I. Valorisation d‚Äôune obligation sans risque de d√©faut",
    "text": "I. Valorisation d‚Äôune obligation sans risque de d√©faut\nNous souhaitons valoriser une obligation sans risque de d√©faut, i.e.¬†\\(\\lambda = 0\\). Pour ce faire, nous fixons les param√®tres suivants :\n\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t sans risque\n\\(\\lambda = 0\\) : l‚Äôintensit√© de d√©faut\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\\(c = 0.02\\) : le coupon annuel\n\\(R = 0.4\\) : le taux de recouvrement\n\\(n = 10\\) : le nombre de coupons\n\\(t = 0\\) : l‚Äôinstant pr√©sent\n\n\nt=0\nlambda_ = 0\nr = 2/100\nT = 10\nc = 2/100\nR = 40/100\nN=1\n\nB_t = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t}\")\n\nPrix de l'obligation vu √† t=0 : 0.9981933497987289\n\n\nEn valorisant l‚Äôobligation, nous obtenons un prix de \\(B_0 \\approx 0.99\\). Ce prix est proche du nominal, car les taux de coupons sont √©gaux au taux de march√©. L‚Äôobligation est dite au pair (Prix = Nominal) car elle r√©mun√®re au taux de march√©. Si \\(c &gt; r\\), le prix de l‚Äôobligation sera sup√©rieur au nominal, car le march√© se serait ru√© sur cette obligation, car elle offrirait plus que le taux de march√©. Sinon, le prix de l‚Äôobligation sera inf√©rieur au nominal, car le march√© serait plus r√©ticent.\n\nüí° La condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est \\(c \\approx r + \\lambda\\). C‚Äôest une obligation qui permet de recouvrir ausi bien le risque de taux (\\(r\\)) et le risque de cr√©dit li√© √† l‚Äôintensit√© de d√©faut (\\(\\lambda\\)). Pour voir ceci, nous avons impl√©ment√© ci-dessous une fonction qui permet d‚Äôextrait le coupon qui permet d‚Äôavoir une obligation au pair, avec les m√™mes param√®tres que pr√©c√©demment.\n\n\nt=0\nlambda_ = 1/100\nr = 2/100\nT = 10\nR = 40/100\nN=1\n\n# M√©thode de dichotomie\ndef trouver_coupon(t, T, r, lambda_, R, N, dt=1, tol=1e-6):\n    \"\"\"\n    Trouve le coupon c tel que la valeur de l'obligation soit √©gale √† N.\n    \"\"\"\n    def equation(c):\n        return pricing_bond(t, c, T, r, lambda_, R, N, dt) - N\n    \n    c_opt = bisect(equation, 0, 1, xtol=tol)  # Recherche de c dans l'intervalle [0,1]\n    return c_opt\n\nc_opt = trouver_coupon(t=t, T=T, r=r, lambda_=lambda_, R=R, N=N)\nc_opt\n\nprint(f\"Coupon pour avoir une obligation au pair vu √† t={t} : {c_opt:.2%}\")\nB_t = pricing_bond(t=t,c=c_opt,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t}\")\n\nCoupon pour avoir une obligation au pair vu √† t=0 : 2.64%\nPrix de l'obligation vu √† t=0 : 1.000007783902349\n\n\n\nDe plus, lorsque l‚Äôintensit√© de d√©faut est tr√®s grande, on retrouve un prix √† peu pr√®s √©gal au taux de recouvrement. En effet, la probabilit√© de d√©faut est tr√®s grande et donc la probabilit√© qu‚Äôil y ait un recouvrement est tr√®s √©lev√©e. (voir exemple ci-dessous)\n\n\nt=0\nlambda_ = 10\nr = 2/100\nT = 10\nc = 2/100\nR = 40/100\nN=1\n\n\nB_t = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t:.4f}\")\nprint(f\"Taux de recouvrement : {R:.4f}\")\n\nPrix de l'obligation vu √† t=0 : 0.3992\nTaux de recouvrement : 0.4000"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#ii.-evolution-du-prix-de-lobligation-en-fonction-du-temps",
    "href": "posts/ensai/gestion_actifs/TP-3.html#ii.-evolution-du-prix-de-lobligation-en-fonction-du-temps",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "II. Evolution du prix de l‚Äôobligation en fonction du temps",
    "text": "II. Evolution du prix de l‚Äôobligation en fonction du temps\nDurant la vie de l‚Äôobligation, son prix √©volue en fonction des paiements de coupons. √Ä chaque distribution de coupon, une chute du prix de l‚Äôobligation est observ√©e. Cette baisse s‚Äôexplique par le fait que, juste avant le versement, le prix de l‚Äôobligation int√®gre la valeur du coupon √† percevoir. Une fois le coupon pay√© aux d√©tenteurs, cette valeur dispara√Æt, entra√Ænant m√©caniquement une diminution du prix de l‚Äôobligation, jusqu‚Äô√† atteindre le nominal de l‚Äôobligation ainsi que le dernier coupon.\nToutefois, apr√®s cette chute li√©e au d√©tachement du coupon, la valeur de l‚Äôobligation remonte progressivement √† mesure que l‚Äô√©ch√©ance du prochain coupon approche. Ce ph√©nom√®ne cr√©e une √©volution en dents de scie, o√π chaque baisse correspond √† un paiement de coupon et chaque remont√©e traduit l‚Äôaccumulation de la valeur du prochain paiement attendu.\nBien que ce ph√©nom√®ne soit logique et attendu, il peut √™tre per√ßu n√©gativement car la forme en dents de scie pourrait donner l‚Äôimpression d‚Äôune d√©gradation de la qualit√© de l‚Äôobligation. C‚Äôest pourquoi on distingue deux types de prix :\n\nLe dirty price (prix sale) : il correspond au prix de l‚Äôobligation tel qu‚Äôaffich√© sur le march√©, int√©grant les variations dues aux paiements de coupons.\nLe clean price (prix net ou pied de coupon) : il correspond au prix de l‚Äôobligation ‚Äúnettoy√©‚Äù des coupons accumul√©s. Ce prix est obtenu en soustrayant les int√©r√™ts courus au dirty price. Ainsi, le clean price permet d‚Äô√©valuer plus pr√©cis√©ment la valeur intrins√®que de l‚Äôobligation sans √™tre pollu√© par les variations dues aux paiements p√©riodiques de coupons. C‚Äôest cette valeur qui est g√©n√©ralement utilis√©e pour comparer les obligations entre elles.\n\nPour illustrer cette √©volution, nous avons trac√© l‚Äô√©volution du prix de l‚Äôobligation en fonction du temps. Nous avons fix√© les param√®tres suivants :\n\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t sans risque\n\\(\\lambda = 0.01\\) : l‚Äôintensit√© de d√©faut\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\\(c = 0.03\\) : le coupon annuel\n\\(R = 0.4\\) : le taux de recouvrement\n\\(n = 10\\) : le nombre de coupons\n\\(t = 0\\) : l‚Äôinstant pr√©sent\n\n\ndef myFloor(x):\n    if x==0:\n        return 0\n    if x==np.floor(x):\n        return x-1\n    return np.floor(x)\n\ndef clean_price(t,c,T,r,lambda_,R,N,dt=1):\n    \"\"\"\n    Fonction qui calcule le prix d'une obligation propre.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    cc = c * (t - myFloor(t))\n\n    return B_t - cc\n\n\nt=0\nlambda_ = 1/100\nr = 2/100\nT = 10\nc = 3/100\nR = 40/100\nN=1\n\n\ndirty_prices = []\nclean_prices = []\ngrid_values_c = np.arange(0,T+0.001,0.001)\nfor t in grid_values_c:\n    dirty_prices.append(pricing_bond(t,c,T,r,lambda_,R,N))\n    clean_prices.append(clean_price(t,c,T,r,lambda_,R,N))\n\n\nimport matplotlib.pyplot as plt\nplt.plot(grid_values_c,dirty_prices, label=\"Dirty prices\")\nplt.plot(grid_values_c,clean_prices, label=\"Clean prices\")\nplt.title(\"Dirty prices vs Clean prices\")\nplt.xlabel(\"t\")\nplt.grid()\nplt.legend()\nplt.ylabel(\"Prix de l'obligation\")\n\nText(0, 0.5, \"Prix de l'obligation\")\n\n\n\n\n\n\n\n\n\n\na. Cas extr√™mes\nNous avons analys√© l‚Äô√©volution du prix de l‚Äôobligation dans deux cas extr√™mes afin d‚Äôobserver l‚Äôimpact du coupon sur la dynamique des prix. Toutes choses √©gales par ailleurs, nous avons modifi√© le coupon de l‚Äôobligation tout en conservant les autres param√®tres constants. Les deux sc√©narios √©tudi√©s sont les suivants :\n\n$ c = 0.01 $ : le coupon est inf√©rieur au taux d‚Äôint√©r√™t sans risque.\n\n$ c = 0.05 $ : le coupon est sup√©rieur au taux d‚Äôint√©r√™t sans risque.\n\n\nCas 1 : $ c = 0.01 $\nLorsque $ c = 1% $ et que ce coupon est inf√©rieur au taux d‚Äôint√©r√™t sans risque $ r $, la valeur de l‚Äôobligation √©volue de mani√®re sp√©cifique :\n\nAu d√©part, l‚Äôobligation est escompt√©e car le coupon est faible, et les investisseurs anticipent un rendement global inf√©rieur au taux du march√©.\n\n√Ä mesure que l‚Äô√©ch√©ance approche, l‚Äôincertitude sur le paiement du coupon dispara√Æt progressivement. Les investisseurs deviennent de plus en plus certains que le paiement aura bien lieu.\n\n√Ä la veille du paiement, l‚Äôobligation converge vers un prix proche de $ N + c $ pour le dirty price (car elle inclut le coupon accumul√©) et $ N $ pour le clean price (qui exclut le coupon accumul√©).\n\nCela signifie que l‚Äôobligation s‚Äôappr√©cie au fil du temps en raison de la certitude croissante du paiement des flux futurs. En d‚Äôautres termes, plus l‚Äô√©ch√©ance se rapproche, plus l‚Äôinvestisseur est assur√© de recevoir ses paiements, ce qui entra√Æne une augmentation progressive de la valeur de l‚Äôobligation.\n\nlambda_ = 1/100\nr = 2/100\nT = 10\nc1 = 1/100\nR = 40/100\nN=1\n\nclean_prices1 = []\ndirty_prices1 = []\nfor t in grid_values_c:\n    B_t_dirty = pricing_bond(t=t,c=c1,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    B_t_clean = clean_price(t=t,c=c1,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    clean_prices1.append(B_t_clean)\n    dirty_prices1.append(B_t_dirty)\n\nplt.plot(grid_values_c,clean_prices1, label=\"Clean prices\")\nplt.plot(grid_values_c,dirty_prices1, label=\"Dirty prices\")\nplt.title(\"Clean prices vs Dirty prices\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"t\")\nplt.ylabel(\"Prix de l'obligation\")\n\nText(0, 0.5, \"Prix de l'obligation\")\n\n\n\n\n\n\n\n\n\n\n\nCas 2 : $ c = 0.05 $\nLorsque $ c = 5% $ et que ce coupon est sup√©rieur au taux d‚Äôint√©r√™t sans risque $ r $, la dynamique du prix de l‚Äôobligation suit une √©volution inverse :\n\nAu d√©part, l‚Äôobligation est pris√©e au-dessus du nominal (elle se n√©gocie avec une prime). Cela s‚Äôexplique par le fait que son coupon g√©n√©reux attire les investisseurs, qui consid√®rent que le rendement offert par l‚Äôobligation compense largement le risque de cr√©dit et est plus attractif que les opportunit√©s de placement √† taux sans risque.\n\n√Ä mesure que l‚Äô√©ch√©ance approche, la valeur de l‚Äôobligation se depr√©cie progressivement. En effet, √† chaque p√©riode, l‚Äôinvestisseur re√ßoit un coupon √©lev√©, mais √† l‚Äô√©ch√©ance, il ne r√©cup√®re que le nominal $ N $, ce qui entra√Æne une correction progressive du prix de march√©.\n\n√Ä la veille du remboursement, l‚Äôobligation converge vers $ N + c $ pour le dirty price (qui inclut le dernier coupon √† verser) et vers $ N $ pour le clean price.\n\nAinsi, cette obligation se d√©pr√©cie progressivement jusqu‚Äô√† l‚Äô√©ch√©ance, car l‚Äôeffet attractif du coupon √©lev√© s‚Äôamenuise √† mesure que le remboursement du capital nominal devient imminent. En d‚Äôautres termes, l‚Äôobligation part d‚Äôune valeur sup√©rieure √† son nominal mais perd progressivement sa prime √† l‚Äôapproche de l‚Äô√©ch√©ance.\n\nlambda_ = 1/100\nr = 2/100\nT = 10\nc2 = 5/100\nR = 40/100\nN=1\n\nclean_prices2 = []\ndirty_prices2 = []\nfor t in grid_values_c:\n    B_t_dirty = pricing_bond(t=t,c=c2,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    B_t_clean = clean_price(t=t,c=c2,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    clean_prices2.append(B_t_clean)\n    dirty_prices2.append(B_t_dirty)\n\nplt.plot(grid_values_c,clean_prices2, label=\"Clean prices\")\nplt.plot(grid_values_c,dirty_prices2, label=\"Dirty prices\")\nplt.title(\"Clean prices vs Dirty prices\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"t\")\nplt.ylabel(\"Prix de l'obligation\")\n\nText(0, 0.5, \"Prix de l'obligation\")"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#iii.-√©volution-du-prix-en-fonction-du-taux-dint√©r√™t",
    "href": "posts/ensai/gestion_actifs/TP-3.html#iii.-√©volution-du-prix-en-fonction-du-taux-dint√©r√™t",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "III. √âvolution du prix en fonction du taux d‚Äôint√©r√™t",
    "text": "III. √âvolution du prix en fonction du taux d‚Äôint√©r√™t\nLe prix d‚Äôune obligation est une fonction d√©croissante du taux d‚Äôint√©r√™t. En effet, plus le taux d‚Äôint√©r√™t est √©lev√©, plus la valeur actualis√©e des flux futurs (coupons et remboursement du nominal) est faible, ce qui r√©duit m√©caniquement le prix de l‚Äôobligation.\nDans cette analyse, il est inutile de distinguer le dirty price et le clean price, car la diff√©rence entre les deux ne d√©pend pas du taux d‚Äôint√©r√™t. De plus, en consid√©rant $t = 0 \\(, il n'y a pas encore d‚Äôint√©r√™ts courus (\\)c = 0 $), donc les deux prix co√Øncident.\n\nObligation au pair\nAutour de $c - = 2% $, le prix de l‚Äôobligation est √©gal au nominal. Cela s‚Äôexplique par le fait que le taux de coupon est exactement √©gal au taux de march√©. L‚Äôobligation est alors dite ‚Äúau pair‚Äù, car les investisseurs n‚Äôont ni prime ni d√©cote √† appliquer sur son prix.\n\n\nExplication de la relation n√©gative entre prix et taux\nLa relation n√©gative entre le prix d‚Äôune obligation et le taux d‚Äôint√©r√™t s‚Äôexplique par l‚Äôeffet de substitution avec les nouvelles √©missions obligataires.\n\nLorsque les taux d‚Äôint√©r√™t augmentent, de nouvelles obligations sont √©mises avec des coupons plus attractifs.\n\nEn cons√©quence, les obligations existantes, qui offrent un coupon fixe plus faible, deviennent moins int√©ressantes pour les investisseurs. Leur prix diminue afin d‚Äôajuster leur rendement effectif au nouveau niveau des taux du march√©.\n\nInversement, si les taux d‚Äôint√©r√™t baissent, les obligations existantes deviennent plus attractives puisqu‚Äôelles offrent un coupon plus √©lev√© que les nouvelles √©missions, ce qui entra√Æne une hausse de leur prix sur le march√© secondaire.\n\nAinsi, la sensibilit√© d‚Äôune obligation aux variations de taux d‚Äôint√©r√™t, appel√©e ‚Äúduration‚Äù, est un √©l√©ment cl√© dans l‚Äô√©valuation du risque de taux et la gestion de portefeuille obligataire.\n\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_r = np.arange(0,1,0.001) \nfor r in grid_values_r:\n    B_t_dirty = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_r,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Prix de l'obligation en fonction du taux d'int√©r√™t\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"r\")\nplt.ylabel(\"Prix de l'obligation\")\n\nText(0, 0.5, \"Prix de l'obligation\")"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#iv.-sensibilit√©-du-prix-de-lobligation-au-taux-dint√©r√™t-duration",
    "href": "posts/ensai/gestion_actifs/TP-3.html#iv.-sensibilit√©-du-prix-de-lobligation-au-taux-dint√©r√™t-duration",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "IV. Sensibilit√© du prix de l‚Äôobligation au taux d‚Äôint√©r√™t / Duration",
    "text": "IV. Sensibilit√© du prix de l‚Äôobligation au taux d‚Äôint√©r√™t / Duration\nLa duration est une mesure de la sensibilit√© du prix d‚Äôune obligation aux variations du taux d‚Äôint√©r√™t. Elle permet d‚Äô√©valuer le risque de taux, c‚Äôest-√†-dire l‚Äôimpact d‚Äôune variation des taux sur la valeur de l‚Äôobligation.\n\nDans le cas de l‚Äô√©volution du prix en fonction du taux d‚Äôint√©r√™t, la duration sera donc la pente de la courbe repr√©sentant cette relation.\n\nMath√©matiquement, la duration est d√©finie comme la d√©riv√©e du prix de l‚Äôobligation par rapport au taux d‚Äôint√©r√™t :\n\\[\n\\delta = - \\frac{d B_t}{d r} \\times \\frac{1}{B_t}\n\\]\nEn utilisant une approximation en diff√©rences finies, on exprime cette d√©riv√©e de la mani√®re suivante :\n\\[\n\\frac{d B_t}{d r}  \\approx \\frac{B_t(r+\\Delta r) - B_t(r)}{\\Delta r}\n\\]\nCette sensibilit√© permet de mesurer la variation du prix de l‚Äôobligation en r√©ponse √† une fluctuation du taux d‚Äôint√©r√™t, offrant ainsi une √©valuation directe du risque de taux auquel est expos√© l‚Äôinvestisseur. De ce fait, si les taux d‚Äôint√©r√™t bouge de \\(\\Delta r\\) = 1%, alors les prix bougeront de -sensibilit√© * \\(\\Delta r\\) .\nNous allons impl√©menter ce calcul en Python, en prenant \\(\\Delta r = 1\\) bp (soit \\(0.0001\\) en notation d√©cimale).\n\ndef sensivity_to_rate(t,c,T,r,lambda_,R,N,dt=1,dr= 0.01/100):\n    \"\"\"\n    Fonction qui calcule la sensibilit√© d'une obligation √† un taux d'int√©r√™t.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    B_t_plus = pricing_bond(t,c,T,r+dr,lambda_,R,N,dt)\n    sensivity = -((B_t_plus - B_t)/dr) * (1/B_t)\n    return sensivity\n\nSi les taux d‚Äôint√©r√™t bouge de \\(\\Delta r\\) = 1%, alors les prix bougeront de -sensibilit√© * \\(\\Delta r\\) = -8,64 * 1%. La duration va √™tre souvent proche de la maturit√©.\n\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nr = 2/100\nN=1\n\nsensivity_to_rate(t,c,T,r,lambda_,R,N)\n\nnp.float64(8.643982489102903)\n\n\nD‚Äôun point de vue graphique, il existe une certaine identit√© entre la maturit√© et la duration, car cette derni√®re peut √™tre interpr√©t√©e comme le barycentre des flux futurs de l‚Äôobligation. Plus ces flux sont concentr√©s dans le temps, plus leur pond√©ration affecte la sensibilit√© du prix aux variations des taux d‚Äôint√©r√™t.\n\nLien entre duration et maturit√©\n\nLa duration est une approximation de la dur√©e d‚Äôexposition au risque, ajust√©e en fonction des flux de paiements.\n\nElle est souvent proche de la maturit√© moyenne de l‚Äôobligation, bien que l√©g√®rement inf√©rieure (environ 80% de la maturit√© totale, en fonction des conditions de march√© et du niveau des coupons).\n\nAinsi, plus l‚Äô√©ch√©ance de l‚Äôobligation se rapproche, plus la duration tend √† augmenter, car les flux futurs deviennent plus proches dans le temps, rendant l‚Äôobligation plus sensible aux variations des taux d‚Äôint√©r√™t.\n\nLorsque la maturit√© de l‚Äôobligation approche, sa sensibilit√© au taux d‚Äôint√©r√™t tend √† augmenter. En effet, la duration peut √™tre interpr√©t√©e comme une mesure du temps moyen pond√©r√© pendant lequel l‚Äôinvestisseur est expos√© au risque de taux.\n\n\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_T = np.arange(1,20,1)\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_rate(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\n\n\nCas particulier : absence de coupon, de taux d‚Äôint√©r√™t et de risque de d√©faut\nLorsque le coupon, le taux d‚Äôint√©r√™t et l‚Äôintensit√© de d√©faut sont nuls, la duration est exactement √©gale √† la maturit√© de l‚Äôobligation. Puisque la duration peut √™tre interpr√©t√©e comme le barycentre des flux futurs de l‚Äôobligation, lorsque le coupon et l‚Äôintensit√© de d√©faut sont nuls, tous les flux sont concentr√©s √† l‚Äô√©ch√©ance, ce qui √©quivaut √† la maturit√© de l‚Äôobligation.\n\nt=0\nc = lambda_ = 10e-6\nR = 40/100\nN=1\n\n\ndirty_prices = []\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_rate(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© au taux en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\n\nDans le risque de taux, le principale indicateur est la duration."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#v.-mod√®lisation-de-la-var",
    "href": "posts/ensai/gestion_actifs/TP-3.html#v.-mod√®lisation-de-la-var",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "V. Mod√®lisation de la VaR",
    "text": "V. Mod√®lisation de la VaR\n\nV.1. Approche par la sensibilit√©\nSelon le mod√®le de Hull et White, le taux d‚Äôint√©r√™t est mod√©lis√© par :\n\\[\ndr = \\theta ( \\mu - r) dt + \\sigma dW\n\\]\no√π \\(\\theta\\) est le coefficient de vitesse de r√©version, \\(\\mu\\) est le taux d‚Äôint√©r√™t moyen, \\(\\sigma\\) est la volatilit√© du taux d‚Äôint√©r√™t et \\(dW\\) est un mouvement brownien. Ce mod√®le a la sp√©cifit√© d‚Äô√™tre normale. De ce fait, \\(\\Delta r \\sim N(0, \\sigma^2 \\Delta t)\\). En faisant l‚Äôapproximation de la variation du prix, il est possible d‚Äôapprocher une VaR par la sensibilit√© :\n\\[\n\\begin{aligned}\nDuration &= - \\frac{d B_t}{d r} \\times \\frac{1}{B_t} \\\\\n\\frac{d B_t}{B_t} &\\approx - Duration \\times  \\Delta r \\\\\n\\end{aligned}\n\\]\n\n# Objectif : √©crire une fonction qui calcule la VaR avec l'approche par duration\nfrom scipy.stats import norm\n\ndef sensitive_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h,dt=1,alpha=0.99) :\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    Duration = sensivity_to_rate(t,c,T,r,lambda_,R,N,dt)\n\n    return -(mu + Duration * sigma * np.sqrt(h) * norm.ppf(1 - alpha))\n\n#--------------------------------------\n# Param√®tres du mod√®le de taux\n#---------------------------------------\n\nmu = 0\nh = 1/12\nsigma = 0.01\n\n#--------------------------------------\n# Param√®tres de la valorisation du bond\n#---------------------------------------\n\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nT=10\n\nVaR = sensitive_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h=h,dt=1,alpha=0.99)\nprint(f\"VaR estim√© par l'approche par la sensibilit√© : {VaR:.4%}\")\n\nVaR estim√© par l'approche par la sensibilit√© : 5.8049%\n\n\nPour cette estimation, nous avions suppos√© que la volatilit√© \\(\\sigma = 1\\%\\). Cependant, cette hypoth√®se pourrait s‚Äô√©carter de la r√©alit√©. Pour ce faire, nous allons extraire les donn√©es de taux d‚Äôint√©r√™t et calculer la volatilit√© empirique. L‚ÄôESTR est un taux un jour collat√©ralis√©,c‚Äôest donc quasiment sans risque. Nous allons donc utiliser ce taux pour calculer la volatilit√© empirique. Pour cela, nous allons utiliser l‚Äôhistorique des taux d‚Äôint√©r√™t de l‚ÄôESTR sur une p√©riode de 5 ans, i.e.¬†10/03/2025 - 12/03/2020, disponible sur ce lien.\nPour ce faire, nous utilisons la formule suivante :\n\\[\n\\sigma = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (r_i - \\bar{r})^2}\n\\]\nNous constatons que la volatilit√© empirique sur les 5 ans est de 25.45%. Cette valeur est tr√®s √©lev√©e, ce qui signifie que les taux d‚Äôint√©r√™t ont connu des variations importantes sur cette p√©riode. Cela peut √™tre du √† la trop grande p√©riode utilis√©e pour la calibration de la volatilit√©. Pour ce faire, nous avons restreint la p√©riode √† 1 an, i.e.¬†10/03/2025 - 10/03/2024. La volatilit√© empirique sur cette p√©riode est de 1.04%. Cette valeur est plus coh√©rente avec les taux d‚Äôint√©r√™t sans risque.\n\nimport pandas as pd\nestr_df = pd.read_excel(\"data/estr.xlsx\", skiprows=6)\n#date as date\nestr_df[\"Date\"] = pd.to_datetime(estr_df[\"Date\"], format=\"%Y-%m-%d\")\nestr_df = estr_df.set_index(\"Date\")\nestr_df = estr_df.sort_index()\n\nestr_df.head()\n\n\n\n\n\n\n\n\nPX_LAST\nCHG_PCT_1D\n\n\nDate\n\n\n\n\n\n\n2020-03-12\n-0.540\n0.5525\n\n\n2020-03-13\n-0.541\n-0.1852\n\n\n2020-03-16\n-0.536\n0.9242\n\n\n2020-03-17\n-0.531\n0.9328\n\n\n2020-03-18\n-0.529\n0.3766\n\n\n\n\n\n\n\n\nvol_est = np.std(estr_df[\"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 5 ans: {vol_est:.4f}\")\n\n# volatilit√© sur 1 an\nvol_est= np.std(estr_df.loc[\"2024-03-10\":\"2025-03-10\", \"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 1 an : {vol_est:.4f}\")\n\nVolatilit√© estim√©e sur 5 ans: 0.2545\nVolatilit√© estim√©e sur 1 an : 0.0104\n\n\n\n\nV.2. Approche avec un repricing\nCette approche consiste √† revaloriser, sous l‚Äôhypoth√®se de normalit√© des taux, l‚Äôobligation √† l‚Äôinstant \\(t\\) pour un taux \\(r + \\Delta r\\) et de calculer la perte maximale possible. De ce fait, elle est plus pr√©cise que l‚Äôapproche par la sensibilit√©.\nPuisque dans le cas d‚Äôune obligation, ce dont on veut se pr√©munir c‚Äôest de la hausse des taux (puisqu‚Äôelle fait baisser le taux d‚Äôint√©r√™t). De ce fait, la VaR est donn√©e par :\n\\[\n\\text{VaR} =  - \\frac{B_t(r + \\Delta r) - B_t }{B_t},\n\\]\no√π \\(\\Delta r\\) est la variation des taux d‚Äôint√©r√™t.\nPar d√©finition, la VaR estim√©e sera plus basse que l‚Äôautre approche en raison de la convexit√© de l‚Äô√©volution du prix de l‚Äôobligation en fonction des taux d‚Äôint√©r√™t. La Var par l‚Äôapproche de la sensibilit√©, quant √† elle, suppose une lin√©arit√© de l‚Äô√©volution du prix de l‚Äôobligation en fonction des taux d‚Äôint√©r√™t.\n\ndef repricing_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h,dt=1,alpha=0.99) :\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    delta_r = mu + sigma * np.sqrt(h) * norm.ppf(alpha)\n    P_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    P_t_shocked = pricing_bond(t,c,T,r+delta_r,lambda_,R,N,dt)\n\n    VAR = -( (P_t_shocked - P_t)/P_t)\n\n    return VAR, delta_r\n\n\n#--------------------------------------\n# Param√®tres du mod√®le de taux\n#---------------------------------------\nmu = 0\nh = 1/12\nsigma = 0.01\n\n#--------------------------------------\n# Param√®tres de la valorisation du bond\n#---------------------------------------\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nT=10\n\nVaR, delta_r = repricing_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h=h,dt=1,alpha=0.99)\n\nprint(f\"VaR estim√© par l'approche par le r√©ajustement : {VaR:.4%}\")\nprint(f\"Choc de taux : {delta_r:.2%}\")\n\nVaR estim√© par l'approche par le r√©ajustement : 5.6272%\nChoc de taux : 0.67%"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#vi.-focus-risque-de-cr√©dit-contrepartie",
    "href": "posts/ensai/gestion_actifs/TP-3.html#vi.-focus-risque-de-cr√©dit-contrepartie",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "VI. Focus risque de cr√©dit & contrepartie",
    "text": "VI. Focus risque de cr√©dit & contrepartie\nLe risque de cr√©dit est le risque que l‚Äô√©metteur d‚Äôune obligation ne soit pas en mesure de respecter ses engagements financiers, entra√Ænant ainsi des pertes pour les investisseurs. Ce risque est particuli√®rement pertinent dans le contexte des obligations, car il peut affecter la valeur de l‚Äôobligation et la capacit√© de l‚Äôinvestisseur √† r√©cup√©rer son capital. Le risque de contrepartie, quant √† lui, est le risque que la contrepartie d‚Äôune transaction financi√®re ne respecte pas ses obligations contractuelles. Dans le cas des obligations, cela peut se traduire par le non-paiement des coupons ou du capital √† l‚Äô√©ch√©ance. Ce risque est particuli√®rement important dans les transactions de gr√© √† gr√© (OTC), o√π les parties ne sont pas soumises aux m√™mes exigences r√©glementaires que celles des march√©s organis√©s.\n\nVI.1. Comment estimer l‚Äôintensit√© de d√©faut ?\nPr√©cedemment, nous avons valoriser l‚Äôobligation de la mani√®re suivante :\n\\[\n\\begin{aligned}\nB_t &= C_t + N_t + R_t \\\\\n&= N \\left[ \\sum_{i=1}^{n}  c \\times e^{-(r + \\lambda) \\times (T_i\n-t)} \\mathbb{1}_{T_i \\geq t} + e^{-(r+\\lambda)(T-t)} \\mathbb{1}_{T \\geq t} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\mathbb{1}_{T \\geq t} \\right]\n\\end{aligned}\n\\]\net nous avons signfi√© la condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est \\(c \\approx r + \\lambda\\). Cependant, ce n‚Äôest pas exact.\nSoit un coupon pay√© en continu, la valorisation de l‚Äôobligation est donn√©e par :\n\\[\n\\begin{aligned}\nB_t &= N \\left[ \\int_{t}^{T} c e^{-(r + \\lambda) \\times (u-t)} du + e^{-(r+\\lambda)(T-t)} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\right]\\\\\n&= N \\left[ \\frac{c}{r + \\lambda} \\left(1 - e^{-(r + \\lambda)(T-t)} \\right) + e^{-(r+\\lambda)(T-t)} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\right]\\\\\nB_t = N &\\Leftrightarrow c = r + \\lambda (1 - R) \\\\\n& c - r = \\lambda (1 - R)\n\\end{aligned}\n\\]\nDe ce fait, en extrayant \\(c\\), la condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est $c = r + (1 - R) $.\n\nüí° \\(s = c-r\\) est la prime de cr√©dit ou encore le spread de cr√©dit. C‚Äôest la prime que l‚Äôinvestisseur demande pour le risque de cr√©dit. Si \\(c &gt; r\\), l‚Äôobligation est √©mise √† un prix sup√©rieur √† 100% du nominal. Si \\(c &lt; r\\), l‚Äôobligation est √©mise √† un prix inf√©rieur √† 100% du nominal. Ce spread permet de faire la relation entre la PD exprim√© par \\(\\lambda\\) et la LGD exprim√© par \\(1 -R\\). Cette information est plus facile √† avoir que l‚Äôintensit√© de d√©faut car le spread est cot√© sur le march√© √† travers les CDS. Par d√©finition, on en d√©duit facilement que plus cettre prime est √©lev√©, plus l‚Äôemetteur est risqu√©.\n\n\nTake away : Les notions de duration et de spread sont tr√®s importants dans la mod√©lisation du risque de taux\n\n\ncas de l‚Äôargentine, pays risqu√©\n\ns = 1031/10000 \nR = 0.4\nlambda_ = s / (1 - R)\nt=0\n\nprint(f\"Spread de cr√©dit : {s:.2%}\")\nprint(f\"Recouvrement : {R:.2%}\")\nprint(f\"Intensit√© de d√©faut : {lambda_:.2%}\")\nPS = [np.exp( - lambda_ * (T - t)) for T in range(1, 21)]\n\nplt.plot(range(1,21),PS)\nplt.title(\"Probabilit√© de survie en fonction de la maturit√©\")\nplt.xlabel(\"T\")\nplt.ylabel(\"Probabilit√© de survie\")\nplt.grid()\n\nSpread de cr√©dit : 10.31%\nRecouvrement : 40.00%\nIntensit√© de d√©faut : 17.18%\n\n\n\n\n\n\n\n\n\n\n\ncas de la France\n\n# Cas de la France\ns = 32.4/10000 \nR = 0.4\nlambda_ = s / (1 - R)\nt=0\n\nprint(f\"Spread de cr√©dit : {s:.2%}\")\nprint(f\"Recouvrement : {R:.2%}\")\nprint(f\"Intensit√© de d√©faut : {lambda_:.2%}\")\nPS = [np.exp( - lambda_ * (T - t)) for T in range(1, 21)]\n\nplt.plot(range(1,21),PS)\nplt.title(\"Probabilit√© de survie en fonction de la maturit√©\")\nplt.xlabel(\"T\")\nplt.ylabel(\"Probabilit√© de survie\")\nplt.grid()\n\nSpread de cr√©dit : 0.32%\nRecouvrement : 40.00%\nIntensit√© de d√©faut : 0.54%\n\n\n\n\n\n\n\n\n\nEn comparant les deux pays, l‚ÄôArgentine et la France. On sait que l‚Äôargentine est un pays plus risqu√© que la France. Cela se voit √©galement au niveau des spreads de cr√©dit. En effet, le spread de l‚ÄôArgentine est plus √©lev√© que celui de la France. Cela signifie que les investisseurs demandent une prime de risque plus √©lev√©e pour investir dans des obligations argentines que dans des obligations fran√ßaises. De plus, en regardant la probabilit√© de survie des deux pays, on constate que la probabilit√© de survie de l‚ÄôArgentine est plus faible que celle de la France. Cela signifie que les investisseurs consid√®rent que l‚ÄôArgentine est plus susceptible de faire d√©faut que la France.\n\n\n\nVI.2 Sensibilit√© cr√©dit\n\ndef sensivity_to_credit(t,c,T,r,lambda_,R,N,dt=1,dlambda_= 0.01/100):\n    \"\"\"\n    Fonction qui calcule la sensibilit√© d'une obligation √† un taux d'int√©r√™t.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    B_t_plus = pricing_bond(t,c,T,r,lambda_ + dlambda_,R,N,dt)\n    \n    sensivity = -((B_t_plus - B_t)/dlambda_) * (1/B_t) * (1/ (1-R))\n\n    return sensivity\n\n\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nr = 2/100\nN=1\n\nsensivity_to_credit(t,c,T,r,lambda_,R,N,dlambda_=0.01/100)\n\nnp.float64(8.821190868023761)\n\n\n\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_T = np.arange(1,20,1)\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_credit(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© cr√©dit en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\nSoit le mod√®le normale pour le taux :\n\\[\ndr_t = \\theta (\\mu - r_t) dt + \\sigma_r dW_t\n\\]\net un mod√®le log normale pour le spread de cr√©dit, puisqu‚Äôil ne peut √™tre n√©gatif : \\[\n\\frac{ds_t}{s_t} = \\sigma_s dZ_t\n\\]\nCes deux mod√®les sont li√©s par \\(dW_t dZ_t = \\rho dt\\). On peut exprimer cette relation par \\(Z_t = \\rho W_t + \\sqrt{1 - \\rho^2} V_t\\), o√π \\(V_t\\) est un mouvement brownien standard ind√©pendant de \\(W_t\\). Cette corr√©lation est positive. &gt; üí° si les taux d‚Äôint√©r√™t montent, le risque de cr√©dit augmente puisque les entreprises ont plus de mal √† rembourser leur dette. De ce fait, le spread de cr√©dit augmente.\nSupposons qu‚Äôon cherche √† calculer une VaR d‚Äôhorizon \\(h\\). Pour cela, il faudra faire des simulations de Monte Carlo pour les deux mod√®les.\n\\[\n\\begin{aligned}\nr_{t+h} &= r_t + \\theta (\\mu - r_t) h + \\sigma_r \\sqrt{h} W_t \\\\\ns_{t+h} &= s_t \\exp(\\sigma_s \\sqrt{h} Z_t) \\\\\nou \\quad s_{t+h} &= s_t (1 + \\sigma_s \\sqrt{h} Z_t) \\quad (\\text{par DL})\n\\end{aligned}\n\\]\nPosons les param√®tres suivants :\n\n\\(\\theta = 0.1\\) : le coefficient de vitesse de r√©version\n\\(\\mu = 0.02\\) : le taux d‚Äôint√©r√™t moyen\n\\(\\sigma_r = 0.01\\) : la volatilit√© du taux d‚Äôint√©r√™t\n\\(\\sigma_s = 0.4\\) : la volatilit√© du spread de cr√©dit\n\\(\\rho = 0.4\\) : la corr√©lation entre les deux mouvements browniens\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t initial\n\\(h = 1/12\\) : l‚Äôhorizon de calcul de la VaR\n\\(c = 3\\%\\) : le coupon annuel\n\\(R = 40\\%\\) : le taux de recouvrement\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\n\nimport numpy as np\n\ndef MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1):\n    # print(\"Parameters\")\n    # print(f\"coupon : {c:.2%}\")\n    # print(f\"maturity : {T} years\")\n    # print(f\"risk free rate : {r:.2%}\")\n    # print(f\"credit spread : {lambda_:.2%}\")\n    # print(f\"recovery rate : {R:.2%}\")\n    # print(f\"nominal : {N}\")\n\n    prices = []\n    \n    P_0 = pricing_bond(t=0,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N,dt=dt)\n    # print(f\"P0 : {P_0}\")\n\n    s_0 = lambda_ * (1 - R)\n    r_0 = r\n\n    for t in range(N_MC):\n        # Generate correlated Brownian motions\n        W1 = np.random.normal()\n        W2 = rho * W1 + np.sqrt(1 - rho) * np.random.normal()\n\n        # Euler discretization \n        r_h = r_0 + sigma_r * np.sqrt(h) * W1\n        s_h = s_0 * ( 1 + sigma_s * np.sqrt(h) * W2)\n        lambda_h = s_h / (1 - R)\n        # print(\"=\"*30)\n        # print(\"Parameters\")\n        # print(f\"coupon : {c:.2%}\")\n        # print(f\"maturity : {T} years\")\n        # print(f\"risk free rate : {r_h:.2%}\")\n        # print(f\"credit spread : {lambda_h:.2%}\")\n        # print(f\"recovery rate : {R:.2%}\")\n        # print(f\"nominal : {N}\")\n\n        P_h = pricing_bond(t=h,c=c,T=T,r=r_h,lambda_ = lambda_h,R=R,N=N,dt=dt)\n        # print(f\"Price : {P_h}\")\n        variation = (P_h - P_0)/P_0\n        prices.append(variation)\n\n\n    VaR = np.quantile(prices, 1 - alpha)\n\n    return VaR\n\nsigma_r = 0.01\nsigma_s = 0.4\nrho = 0.40\n\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nh = 1/12\nT= 10\n\nVaR_monte_carlo = MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1)\nprint(f\"VaR estim√© par Monte Carlo : {VaR_monte_carlo:.4%}\")\n\nVaR estim√© par Monte Carlo : -6.3438%\n\n\nLorsque $ $ (le coefficient de corr√©lation entre les taux d‚Äôint√©r√™t et le spread de cr√©dit) est positif, une hausse des taux d‚Äôint√©r√™t entra√Æne √©galement une hausse du spread de cr√©dit. Cette dynamique amplifie le risque global, car les pertes dues √† la hausse des taux s‚Äôajoutent aux pertes induites par l‚Äô√©largissement du spread de cr√©dit. Ainsi, la Value at Risk (VaR) est plus √©lev√©e, refl√©tant l‚Äôaccumulation des risques li√©s aux deux facteurs.\nEn revanche, lorsque $ $ est n√©gatif, une hausse des taux d‚Äôint√©r√™t tend √† r√©duire le spread de cr√©dit, et inversement. Il se cr√©e alors un effet de compensation : les pertes g√©n√©r√©es par l‚Äô√©volution des taux sont partiellement absorb√©es par les gains r√©sultant de la contraction du spread (ou inversement). C‚Äôest le principe de diversification. Dans ce cas, la VaR est plus faible, car les effets du taux d‚Äôint√©r√™t et du spread de cr√©dit s‚Äôannulent en partie, r√©duisant ainsi l‚Äôampleur des pertes potentielles.\n\n# VaR en fonction de rho\nfrom tqdm import tqdm \n\nrhos = np.linspace(-1,1,100)\nVaRs = [MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1) for rho in tqdm(rhos)]\n\nplt.plot(rhos,VaRs)\nplt.title(\"VaR en fonction de la corr√©lation\")\nplt.xlabel(\"rho\")\nplt.ylabel(\"VaR\")\nplt.grid()\n\n  0%|          | 0/100 [00:00&lt;?, ?it/s] 11%|‚ñà         | 11/100 [00:00&lt;00:00, 101.45it/s] 22%|‚ñà‚ñà‚ñè       | 22/100 [00:00&lt;00:00, 102.17it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00&lt;00:00, 102.04it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:00&lt;00:00, 101.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:00&lt;00:00, 102.33it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:00&lt;00:00, 102.45it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:00&lt;00:00, 102.70it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [00:00&lt;00:00, 103.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:00&lt;00:00, 103.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00&lt;00:00, 102.64it/s]\n\n\n\n\n\n\n\n\n\n\nEstimation de la volatilit√© du spread de cr√©dit\nPour la volatilit√© du spread, nous avons suppos√© que \\(\\sigma_s = 40\\%\\). Nous allons maintenant estimer cette volatilit√© √† partir des donn√©es de march√©, en utilisant l‚Äôhistorique des spreads de cr√©dit sur une p√©riode de 5 ans, i.e.¬†11/03/2025 - 11/03/2020, disponible sur ce lien.\nLa volatilit√© du spread est une volatilit√© annualis√©e. De ce fait, la formulation de la volatilit√© empirique, lorsque la fr√©quence est quotidienne, est la suivante :\n\\[\n\\sigma_s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (s_i - \\bar{s})^2} \\times \\sqrt{255}\n\\]\n\ncds_df = pd.read_excel(\"data/cds.xlsx\", skiprows=6)\n#date as date\ncds_df[\"Date\"] = pd.to_datetime(cds_df[\"Date\"], format=\"%Y-%m-%d\")\ncds_df = cds_df.set_index(\"Date\")\ncds_df = cds_df.sort_index()\n\ncds_df.head()\n\n\n\n\n\n\n\n\nPX_LAST\n\n\nDate\n\n\n\n\n\n2020-03-11\n30.772\n\n\n2020-03-12\n36.977\n\n\n2020-03-13\n37.869\n\n\n2020-03-16\n44.432\n\n\n2020-03-17\n49.923\n\n\n\n\n\n\n\n\nvol_est = np.std(cds_df[\"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 5 ans: {vol_est*np.sqrt(255):.4f}\")\n\n# volatilit√© sur 1 an\nvol_est= np.std(cds_df.loc[\"2024-03-11\":\"2025-03-11\", \"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 1 an : {vol_est*np.sqrt(255):.4f}\")\n\nVolatilit√© estim√©e sur 5 ans: 0.4512\nVolatilit√© estim√©e sur 1 an : 0.5181"
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#risque-de-mod√®le",
    "href": "posts/ensai/gestion_actifs/TP-3.html#risque-de-mod√®le",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "Risque de mod√®le",
    "text": "Risque de mod√®le\nLe risque de mod√®le est le risque associ√© √† une mauvaise utilisation d‚Äôun mod√®le dans le processus de prise de d√©cision. Il peut √™tre caus√© par :\n\nune incertitude sur les param√®tres,\nune qualit√© insuffisante des donn√©es,\nou une inad√©quation structurelle du mod√®le par rapport √† la r√©alit√© observ√©e.\n\nCe risque peut conduire √† des erreurs de pr√©vision, √† des d√©cisions inappropri√©es ou √† des pertes financi√®res. Il est donc essentiel de bien comprendre les hypoth√®ses et limites des mod√®les employ√©s.\nC‚Äôest une activit√© o√π l‚Äôintervention humaine reste indispensable : il est n√©cessaire de proc√©der √† des sanity checks, du backtesting, et de challenger les mod√®les √† l‚Äôaide de versions plus simples (d√©g√©n√©r√©es) ou plus riches, afin d‚Äôen comparer les r√©sultats et de mieux cerner leurs faiblesses.\nPour r√©duire le risque de mod√®le, il est courant de mettre en place une fonction ind√©pendante de validation, g√©n√©ralement assur√©e par les √©quipes MRM (Model Risk Management). Ces derni√®res sont charg√©es :\n\nd‚Äôauditer les mod√®les,\nde mener des tests de robustesse,\net de recommander des provisions en cas de risque de sur√©valuation des actifs.\n\nLes comp√©tences cl√©s pour cette activit√© incluent : - les math√©matiques financi√®res, - la connaissance des march√©s financiers et des mod√®les de risque, - la programmation et l‚Äôutilisation d‚Äôoutils quantitatifs, - ainsi que des aptitudes √† la communication, au travail en √©quipe et au raisonnement critique."
  },
  {
    "objectID": "posts/ensai/gestion_actifs/TP-3.html#risque-climatique",
    "href": "posts/ensai/gestion_actifs/TP-3.html#risque-climatique",
    "title": "Risque de valorisation d‚Äôobligation",
    "section": "Risque climatique",
    "text": "Risque climatique\nLe risque climatique d√©signe les impacts potentiels des changements climatiques et des politiques environnementales sur les entreprises et les march√©s financiers. Contrairement aux risques classiques, les trajectoires de r√©f√©rence sont d√©finies par des organismes scientifiques comme le GIEC, ce qui limite l‚Äôappropriation directe des mod√®les par les institutions financi√®res. Il s‚Äôagit donc d‚Äôun domaine o√π le risque de mod√®le est indirect.\nLe r√©seau NGFS (Network for Greening the Financial System) a √©t√© mis en place pour aider les r√©gulateurs et les banques centrales √† mieux int√©grer les risques climatiques dans leurs cadres prudentiels.\n√Ä l‚Äôheure actuelle, il n‚Äôexiste pas de consensus clair sur la mani√®re d‚Äôint√©grer pleinement le risque climatique dans les mod√®les financiers ; il s‚Äôagit plut√¥t de tentatives progressives d‚Äôadaptation. Le NGFS propose plusieurs sc√©narios climatiques, parmi lesquels :\n\nCurrent Policies : continuit√© des politiques actuelles sans nouvel engagement.\nNDC (Nationally Determined Contributions) : politiques actuelles + engagements annonc√©s par les √âtats.\nDisorderly Transition (1.5¬∞C) : les engagements sont mis en ≈ìuvre avec retard ou de fa√ßon d√©sorganis√©e.\nNet Zero / 2¬∞C : sc√©nario optimis√© pour limiter le r√©chauffement √† 2¬∞C ‚Äî le cadre le plus ambitieux et le plus stable.\n\nOn distingue g√©n√©ralement deux types de risques climatiques :\n\n1. Risque physique\nCe risque est li√© aux cons√©quences directes des changements climatiques sur les infrastructures et l‚Äôenvironnement : - Risques aigus : √©v√©nements extr√™mes (inondations, s√©cheresses, temp√™tes, etc.). - Risques chroniques : √©volutions lentes (√©l√©vation du niveau des mers, hausse des temp√©ratures, etc.).\nCes risques peuvent entra√Æner : - des pertes mat√©rielles, - des interruptions d‚Äôactivit√©, - des co√ªts de r√©paration et d‚Äôadaptation.\n\n\n2. Risque de transition\nCe risque est associ√© aux mesures prises pour r√©duire les √©missions de gaz √† effet de serre et passer √† une √©conomie bas carbone. Il peut provoquer : - des pertes de valeur sur certains actifs, - des co√ªts de transition √©lev√©s, - des bouleversements sectoriels et technologiques.\nIl comprend deux composantes : - le risque politique (durcissement r√©glementaire, interdictions, fiscalit√© verte, etc.), - et le risque/opportunit√© technologique (√©mergence de nouvelles technologies, changement dans les pr√©f√©rences de consommation, etc.).\nOn peut formuler ce risque comme une relation :\n\\[\n\\text{Risque de transition} = \\text{Risque politique} - \\text{Opportunit√©s technologiques}\n\\]\nAinsi, une entreprise bien positionn√©e sur les technologies vertes peut compenser tout ou partie du risque politique subi.\n\nLe risque physique est plus √©lev√© dans les sc√©narios Current Policies, NDC, et Disorderly Transition, car ils impliquent une action climatique insuffisante ou retard√©e.\n√Ä l‚Äôinverse, le risque de transition est plus important dans les sc√©narios ambitieux comme Net Zero, o√π les ajustements politiques et √©conomiques sont rapides et profonds."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/reglementation_prudentielle.html",
    "href": "posts/ensai/analyses_financieres/reglementation_prudentielle.html",
    "title": "La r√©glementation prudentielle",
    "section": "",
    "text": "La r√©glementation prudentielle a √©t√© initi√©e par le d√©veloppement des march√©s financiers et des chocs aliment√©s par diverses crises financi√®res. Face √† ce constat, les autorit√©s de contr√¥le bancaire ainsi que les autorit√©s de march√© ont pris des d√©cisions pour r√©guler les march√©s. C‚Äôest notamment le r√¥le qu‚Äôoccupe le Comit√© de B√¢le ou la Commission bancaire, qui ont pour objectif de renforcer la stabilit√© des march√©s financiers. En France, l‚ÄôACPR (Autorit√© de Contr√¥le Prudentiel et de R√©solution) et la Banque de France sont membres du Comit√© de B√¢le et participent √† ses travaux et d√©cisions.\nIl existe par ailleurs plusieurs textes r√©glementaires ou documents relatifs au risque de march√©. Parmi ces textes, on peut citer le document de r√©f√©rence pour calculer le ratio de solvabilit√© de la Commission bancaire, intitul√© ‚ÄúModalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL‚Äù, actualis√© tous les ans par l‚ÄôACPR en France."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "href": "posts/ensai/analyses_financieres/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "title": "La r√©glementation prudentielle",
    "section": "Approche standard de mesure du risque de march√©",
    "text": "Approche standard de mesure du risque de march√©\nL‚Äôapproche standard de mesure du risque de march√© consiste √† calculer les exigences en fonds propres pour chaque cat√©gorie de risque, √† savoir :\n\nle risque de taux (g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque li√© aux titres de propri√©t√©(g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque de change calcul√© sur l‚Äôensemble des op√©rations appartenant aussi bien au portefeuille de n√©gociation ou non;\nle risque sur mati√®res premi√®res calcul√© sur l‚Äôensemble des op√©rations du portefeuille de n√©gociation ou non;\nles risques op√©rationnels calcul√©s sur les options associ√©es √† chachune des cat√©gories de risque cit√©es ci-dessus.\n\nPar la suite, il s‚Äôagit de les additionner de mani√®re arithm√©tique. Par exemple, pour les titres de propri√©t√©, l‚Äôexigence de fonds propres est la somme de l‚Äôexigence de fonds propres pour le risque g√©n√©ral et l‚Äôexigence de fonds propres pour le risque sp√©cifique.\nPour le calcul des exigences de fonds propres au titre des risques de march√©, il faut tout d‚Äôabord d√©terminer les positions nettes. Les positions de titrisation log√©es dans le portefeuille de n√©gociation sont trait√©es comme tout instrument de dette au titre du risque de taux.\nPour le risque sp√©cifique, l‚Äôexigence en fonds propres sera la somme des positions nettes multipli√©es par un coefficient de pond√©ration (2%, 4%, 8% ou 12%) choisi en fonction de la liquidit√© et la diversification de la position. Pour le risque g√©n√©ral, l‚Äôexigence en fonds propres est la somme des positions nettes globales (pour chaque march√© national) multipli√©es par 8%."
  },
  {
    "objectID": "posts/ensai/analyses_financieres/reglementation_prudentielle.html#approche-mod√®le-interne",
    "href": "posts/ensai/analyses_financieres/reglementation_prudentielle.html#approche-mod√®le-interne",
    "title": "La r√©glementation prudentielle",
    "section": "Approche mod√®le interne",
    "text": "Approche mod√®le interne\nL‚Äôapproche mod√®le interne est une m√©thode de calcul des exigences en fonds propres pour le risque de march√© qui permet aux √©tablissements de calculer leurs propres exigences. L‚Äôexigence en fonds propres est g√©n√©ralement un calcul de la VaR. Cette approche est soumise √† des conditions strictes et √† une validation par l‚ÄôACPR.\nConcernant l‚Äôutilisation conjointe des mod√®les internes et de l‚Äôapproche standard, la position de la commission pr√™te une attention particuli√®re √† la permanence des m√©thodes ainsi qu‚Äô√† leur √©volution. L‚Äôobjectif est de s‚Äôorienter vers un mod√®le global qui tient compte de l‚Äôensemble des risques de march√©.\n\nAinsi, un √©tablissement commen√ßant √† utiliser des mod√®les pour une ou plusieurs cat√©gories de facteurs de risque doit en principe √©tendre progressivement ce syst√®me √† tous ses risques √† la m√©thodologie standardis√©e (√† moins que la Commission Bancaire ne lui ait retir√© son agr√©ment pour ses mod√®les).\n\nPour une banque, la construction d‚Äôun mod√®le interne doit permettre de fournir une mesure plus √©conomique du risque de march√©. Au titre de l‚Äôarticle 363 du CRR (R√®glement sur les exigences de fonds propres), l‚Äôautorit√© comp√©tente autorise les √©tablissements assujettis √† utiliser leurs mod√®les internes pour calculer les exigences de fonds propres pour risques de march√©, apr√®s avoir v√©rifi√© qu‚Äôils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3√®me partie du CRR (journal?). L‚Äôautorisation d‚Äôutiliser des mod√®les internes accord√©e par les autorit√©s comp√©tentes est requise pour chaque cat√©gorie de risques (risque g√©n√©ral et sp√©cifique li√©s aux actions et titres de cr√©ance, risque de change et risque sur mati√®res premi√®res), et elle n‚Äôest accord√©e que si le mod√®le interne couvre une part importante des positions d‚Äôune certaine cat√©gorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives √† la mesure du risque (articles 367) mais aussi d‚Äôordre g√©n√©ral (article 365).\n\nExigences g√©n√©rales\nLe calcul de la valeur en risque vis√©e √† l‚Äôarticle 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprim√© en centiles et unilat√©ral, de 99 %;\nune p√©riode de d√©tention de dix jours;\nune p√©riode effective d‚Äôobservation historique d‚Äôau moins un an, √† moins qu‚Äôune p√©riode d‚Äôobservation plus courte ne soit justifi√©e par une augmentation significative de la volatilit√© des prix;\ndes mises √† jour au moins mensuelles des s√©ries de donn√©es.\n\nL‚Äô√©tablissement peut utiliser des mesures de la valeur en risque calcul√©es sur la base de p√©riodes de d√©tention inf√©rieures √† dix jours, qu‚Äôil porte √† dix jours selon une m√©thode appropri√©e qu‚Äôil revoit r√©guli√®rement.\nChaque √©tablissement doit √©galement calculer, au moins hebdomadairement, une ‚Äúvaleur en risque en situation de tensions‚Äù (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit √™tre calcul√©e conform√©ment aux m√™mes exigences que la VaR standard √©nonc√©es plus haut (intervalle de confiance de 99% etc.). Cependant, les donn√©es d‚Äôentr√©e du mod√®le de Stressed VaR doivent √™tre calibr√©es par rapport √† une p√©riode historique de tensions financi√®res significatives d‚Äôau moins 12 mois, pertinente pour le portefeuille de l‚Äô√©tablissement. Le choix de cette p√©riode de tensions historiques fait l‚Äôobjet d‚Äôun examen au moins annuel par l‚Äô√©tablissement, qui en communique les r√©sultats aux autorit√©s comp√©tentes. L‚Äôobjectif est de s‚Äôassurer que la Stressed VaR refl√®te de mani√®re ad√©quate les risques auxquels l‚Äô√©tablissement serait expos√© en p√©riode de crise financi√®re.\nPour r√©sumer, les √©tablissements doivent calculer la perte potentielle quotidiennement pour une p√©riode de d√©tention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent √©galement calculer une Stressed VaR au moins une fois par semaine, en utilisant des donn√©es historiques de p√©riodes de tensions financi√®res significatives.\nNotons \\(VaR(t)\\) la valeur en risque √† la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions √† la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) √† la date t pour le risque de march√© sont calcul√©es comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\no√π \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu‚Äôon vera plus tard.\nDans des p√©riodes normales, l‚Äôexigence en fonds propres sera donc la somme d‚Äôun multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n‚Äôest que dans les p√©riodes de crises financi√®res que l‚Äôexigence en fonds propres correspond √† la VaR ou √† la sVaR du jour pr√©c√©dent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est √©gal √† la somme du chiffre 3, au minimum, et d‚Äôun cumulateur compris entre 0 et 1 conform√©ment au tableau 1. Ce cumulateur d√©pend du nombre de d√©passements, sur les 250 derniers jours ouvr√©s, mis en √©vidence par les contr√¥les a posteriori de la mesure de la valeur en risque, au sens de l‚Äôarticle 365, paragraphe 1, effectu√©s par l‚Äô√©tablissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqu√©s depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqu√©s depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n  Nombre.de.d√©passements Cumulateur\n1             moins de 5       0.00\n2                      5       0.40\n3                      6       0.50\n4                      7       0.65\n5                      8       0.75\n6                      9       0.85\n7             10 ou plus       1.00\n\n\nEn ce qui concerne le risque sp√©cifique, tout mod√®le interne utilis√© pour calculer les exigences de fonds propres et tout mod√®le interne utilis√© pour la n√©gociation en corr√©lation satisfont aux exigences suppl√©mentaires suivantes:\n\nle mod√®le interne explique la variation historique des prix √† l‚Äôint√© rieur du portefeuille;\nil refl√®te la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement d√©favorable;\nil est valid√© par des contr√¥les a posteriori(backtesting) visant √† √©tablir si le risque sp√©cifique a √©t√© correctement pris en compte. Si l‚Äô√©tablissement effectue ces contr√¥les a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de mani√®re coh√©rente;\nil tient compte du risque de base li√© √† la signature et, en particulier, il est sensible aux diff√©rences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d‚Äô√©v√©nement.\n\nLe risque sp√©cifique vise √† tenir compte du risque de contrepartie li√© √† l‚Äôemetteur de l‚Äôinstrument.\nPour en savoir plus, reportez au r√®glement (UE) No 575/2013 du parlement europ√©en du journal officiel de l‚ÄôUnion Europ√©enne, appel√© aussi r√®glement CRR. (voir aussi la notice 2020 relative aux ¬´ Modalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV¬ª)."
  },
  {
    "objectID": "posts/ensai/others/finance_durable.html",
    "href": "posts/ensai/others/finance_durable.html",
    "title": "Investissement responsable",
    "section": "",
    "text": "Le cadre de la finance durable\nLa finance durable vise √† concilier rentabilit√© financi√®re et prise en compte des enjeux environnementaux, sociaux et de gouvernance (ESG). Elle repose sur trois grands objectifs :\n\nR√©orienter les flux de capitaux vers des investissements responsables.\nG√©rer les risques financiers li√©s aux crit√®res ESG (changement climatique, violations des droits humains, mauvaise gouvernance).\nFavoriser la transparence et une vision √† long terme gr√¢ce aux nouvelles r√©gulations.\n\nPour structurer cette transition, plusieurs outils ont √©t√© mis en place :\n\nLes crit√®res ESG, qui permettent d‚Äô√©valuer la durabilit√© d‚Äôun investissement.\nLa Taxonomie Europ√©enne, qui d√©finit quelles activit√©s √©conomiques sont r√©ellement durables. Il a 6 objectifs principaux.\nLes labels ISR (pour des investissements prenant en compte les crit√®res ESG), Greenfin (pour des projects plus verts, excluant les √©nergies fossiles) et Finansol (une finance solidaire pour prendre en compte l‚Äôaspect ¬´social¬ª), qui garantissent des investissements align√©s avec la finance responsable.\nLes obligations r√©glementaires (SFDR - R√®glement, CSRD - Directive), qui imposent un reporting extra-financier aux entreprises et investisseurs.\n\nCependant, des d√©fis subsistent, comme le greenwashing, le manque d‚Äôuniformisation des notations ESG et la crainte d‚Äôun impact n√©gatif sur la performance financi√®re. Malgr√© cela, la finance durable est devenue un levier incontournable pour assurer une √©conomie plus r√©siliante et √©thique. La finance durable n‚Äôest plus une option mais une n√©cessit√© pour r√©pondre aux d√©fis climatiques et sociaux tout en maintenant la stabilit√© du syst√®me financier mondial.\n\n\nLa finance durable appliqu√©e √† l‚Äôinvestissement\nLa finance durable ne se limite pas √† des principes th√©oriques ; elle se traduit par des strat√©gies d‚Äôinvestissement concr√®tes visant √† int√©grer les crit√®res ESG dans la gestion des actifs financiers. Pour cela, plusieurs approches existent.\n\nLes strat√©gies d‚Äôinvestissement responsable\n\nL‚ÄôExclusion : √âliminer certains secteurs ou entreprises jug√©s non responsables (ex : tabac, √©nergies fossiles, armement).\nLe Best-in-Class : S√©lectionner les entreprises ayant les meilleures pratiques ESG dans chaque secteur, sans exclure de domaines sp√©cifiques.\nL‚ÄôEngagement actionnarial : Influer sur les entreprises en tant qu‚Äôactionnaire via le vote en assembl√©e g√©n√©rale et le dialogue.\nL‚ÄôImpact Investing : Financer directement des entreprises ou projets ayant un impact environnemental ou social positif mesurable. Les strat√©gies les plus actives et efficaces sont l‚ÄôEngagement Actionnarial et l‚ÄôImpact Investing, car elles permettent d‚Äôinfluencer directement les pratiques ESG et de cr√©er un impact mesurable sur l‚Äô√©conomie.\n\n\n\nL‚Äôimportance des donn√©es ESG\nL‚Äôinvestissement responsable repose sur des donn√©es ESG fiables pour √©valuer la performance extra-financi√®re des entreprises. Cependant, plusieurs limites persistent :\n\nManque de standardisation : Chaque fournisseur (MSCI, Sustainalytics, Moody‚Äôs ESG) utilise des m√©thodologies diff√©rentes, rendant les comparaisons complexes.\nRisque de biais et greenwashing : Certaines entreprises embellissent leurs rapports ESG pour obtenir de meilleures notes.\nAbsence de transparence : Les investisseurs doivent analyser minutieusement les sources de donn√©es avant de prendre des d√©cisions.\n\nDes solutions existent pour am√©liorer la fiabilit√© des notations ESG :\n\nHarmonisation des standards via des r√©gulations comme la directive CSRD.\nAudit ind√©pendant des donn√©es ESG pour √©viter les manipulations.\nUtilisation de l‚ÄôIA et du Big Data pour analyser des sources plus diversifi√©es et d√©tecter les incoh√©rences.\n\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôint√©gration de l‚ÄôESG est une approche dynamique et √©volutive, qui d√©pend du niveau d‚Äôengagement souhait√© par l‚Äôinvestisseur.L‚ÄôEngagement actionnarial et l‚ÄôImpact Investing sont les strat√©gies les plus efficaces, car elles permettent d‚Äôinfluencer l‚Äô√©conomie de mani√®re proactive. La fiabilit√© des donn√©es ESG doit encore √™tre renforc√©e pour garantir une transparence totale dans la finance durable.\n\n\n\n\n\nPrise en compte du risque climatique dans la gestion d‚Äôactifs\nLe changement climatique est devenu un facteur cl√© dans la gestion d‚Äôactifs. Les investisseurs doivent int√©grer ces risques pour prot√©ger leurs portefeuilles, se conformer aux r√©gulations et capter les opportunit√©s de la transition √©nerg√©tique.\n\nImpact du risque climatique sur les actifs financiers\nDeux types de risques √† prendre en compte :\n\nRisque physique ‚Üí Catastrophes naturelles (ouragans, inondations, s√©cheresses) qui endommagent les infrastructures et r√©duisent la valeur des actifs.\nRisque de transition ‚Üí R√©gulations environnementales et √©volutions du march√© qui d√©valorisent les industries polluantes (p√©trole, charbon, transport intensif, etc.). Exemple : L‚Äôimmobilier en zone inondable peut perdre de la valeur, et les entreprises p√©troli√®res risquent de voir leurs co√ªts augmenter avec la taxe carbone.\n\n\n\nMesure et gestion des risques climatiques en finance\nIndicateurs utilis√©s :\n\nScore ESG ‚Üí √âvaluation des crit√®res environnementaux, sociaux et de gouvernance.\n√âmissions de GES (Scope 1, 2, 3) ‚Üí Mesure l‚Äôempreinte carbone d‚Äôune entreprise ou d‚Äôun portefeuille.\nAlignement 2¬∞C ‚Üí V√©rifie si un investissement est compatible avec les objectifs climatiques de l‚ÄôAccord de Paris.\nStress-tests climatiques ‚Üí Simulent l‚Äôimpact des sc√©narios climatiques sur les portefeuilles d‚Äôinvestissement.\n\n\n\nOutils de gestion des risques\n\nDiversification des actifs ‚Üí R√©duire l‚Äôexposition aux secteurs vuln√©rables au climat.\nExclusion des industries polluantes ‚Üí √âviter les investissements dans le charbon et le p√©trole.\nInvestissement dans des actifs verts (Green Bonds, infrastructures bas carbone) ‚Üí Financer la transition √©nerg√©tique.\nEngagement actionnarial ‚Üí Influer sur les entreprises en votant en assembl√©e g√©n√©rale pour exiger des strat√©gies bas carbone.\n\n\n\nRaisons pour int√©grer les sc√©narios climatiques en gestion de portefeuille ?\n\n√âviter les pertes financi√®res ‚Üí Anticiper l‚Äôimpact du climat sur les entreprises et secteurs sensibles.\nSe conformer aux r√©gulations ‚Üí Respecter les normes SFDR, CSRD et Taxonomie Europ√©enne.\nCapter les opportunit√©s d‚Äôinvestissement ‚Üí Financer des entreprises et projets align√©s avec la transition √©nerg√©tique.\n\nExemple : Un investisseur qui anticipe les r√©glementations sur les √©nergies fossiles pourra transf√©rer ses capitaux vers les √©nergies renouvelables, √©vitant ainsi des pertes et profitant de la croissance du secteur.\n\n\n\n\n\n\nNote\n\n\n\nLe risque climatique est un enjeu central en gestion d‚Äôactifs. Il affecte la valorisation des entreprises, les r√©gulations financi√®res et les d√©cisions d‚Äôinvestissement. Les investisseurs doivent int√©grer ces risques dans leur gestion de portefeuille, en utilisant des indicateurs ESG, des stress-tests climatiques et des strat√©gies de diversification. Ne pas prendre en compte ces sc√©narios expose √† des actifs d√©valoris√©s et √† des pertes financi√®res √† long terme.\n\n\n\n\n\n\n\n\nTake away\n\n\n\nL‚Äôinvestissement responsable int√®gre les crit√®res ESG (Environnement, Social, Gouvernance) dans la gestion d‚Äôactifs afin de concilier performance financi√®re et impact durable. Il repose sur diff√©rentes strat√©gies : exclusion des secteurs controvers√©s, Best-in-Class, Engagement actionnarial et Impact Investing. Les r√©gulations, comme la Taxonomie Europ√©enne, la SFDR et la CSRD, imposent plus de transparence aux entreprises et investisseurs pour lutter contre le greenwashing et favoriser la transition vers une √©conomie bas carbone. Le risque climatique, divis√© en risques physiques (catastrophes naturelles) et risques de transition (r√©gulations et √©volutions de march√©), influence fortement la valorisation des actifs. Les outils comme l‚Äôalignement 2¬∞C et les stress-tests climatiques aident √† √©valuer l‚Äôexposition des portefeuilles. Enfin, les Green Bonds et autres financements durables permettent de soutenir la transition √©nerg√©tique et d‚Äôorienter les capitaux vers des projets √† fort impact environnemental positif. L‚Äôinvestissement responsable est donc un levier cl√© pour une finance plus durable et r√©siliente."
  },
  {
    "objectID": "posts/ensai/others/ISR.html",
    "href": "posts/ensai/others/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "posts/ensai/others/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "posts/ensai/others/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "posts/ensai/others/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "posts/ensai/others/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL‚ÄôISR pr√©sente de multiples avantages tant pour les investisseurs que pour les entreprises engag√©es dans cette d√©marche. Ces avantages refl√®tent l‚Äô√©volution des attentes soci√©tales et la reconnaissance croissante de l‚Äôimportance de la durabilit√© et de l‚Äô√©thique dans le monde des affaires. J‚Äôen ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement am√©liorer la r√©putation d‚Äôune entreprise. Elle t√©moigne de son engagement envers des pratiques durables et √©thiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financi√®re : De nombreuses √©tudes d√©montrent que les entreprises avec une notation ESG √©lev√©e tendent de meilleures performances financi√®rement sur le long terme. Cela s‚Äôexplique par une meilleure anticipation des risques, une gestion plus efficace et une capacit√© √† saisir les opportunit√©s de march√© li√©es √† la durabilit√©.\nMeilleure gestion des risques : L‚Äôadoption de pratiques ESG solides permet aux entreprises de mieux identifier et g√©rer les risques, qu‚Äôils soient climatiques, sociaux ou de march√©.\nMeilleure attractivit√© pour les investisseurs : En d√©montrant un engagement clair envers la durabilit√© et l‚Äô√©thique, les entreprises attirent davantage d‚Äôinvestisseurs conscients de l‚Äôimportance des crit√®res ESG. Cette attractivit√© accrue peut se traduire par un acc√®s facilit√© au capital et √† de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution √† la r√©duction de certains risques financiers : En investissant dans des entreprise int√©grant les crit√®res ESG dans leur processus de d√©cision, les investisseurs contribuent indirectement √† une meilleure identification et anticipation les risques li√©s au changement climatique, aux probl√©matiques sociales, et aux d√©fis de gouvernance, ce qui contribue √† une meilleure protection de leur capital sur le long terme.\nImpact positif sur la soci√©t√© : L‚ÄôISR permet aux investisseurs de contribuer activement √† une √©conomie plus durable et √©quitable. En choisissant d‚Äôinvestir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des mod√®les √©conomiques respectueux de l‚Äôenvironnement et du bien-√™tre social.\n\nEn somme, l‚ÄôISR offre une perspective d‚Äôinvestissement qui va au-del√† des retours financiers imm√©diats pour embrasser des b√©n√©fices √† long terme, tant sur le plan √©conomique que social et environnemental."
  },
  {
    "objectID": "posts/ensai/others/ISR.html#comment-investir",
    "href": "posts/ensai/others/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire √† un fonds ISR ?\nSouscrire √† un fonds1 ISR (Investissement Socialement Responsable) est une d√©marche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la soci√©t√© et l‚Äôenvironnement(comment?). En consultant son conseiller financier ou son √©tablissement bancaire, il est possible de placer son argent dans une vari√©t√© de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d‚Äô√âpargne en Actions (PEA) : (sous r√©server de s‚Äôassurer que le fonds ISR choisi est bien √©ligible au PEA) Offre la possibilit√© de placer son √©pargne en actions de soci√©t√©s europ√©ennes.\nLes compte-titres ordinaires(CTO) : permet d‚Äôinvestir en bourse sur les march√©s financiers fran√ßais et/ou √©trangers et dans tout type de valeurs mobili√®res (OPC2, actions, obligations, mon√©taire, warrants, trackers‚Ä¶).\nL‚Äô√©pargne salariale ou les plans d‚Äô√©pargne d‚Äôentreprise (PEE) : un produit d‚Äô√©pargne collectif qui permet aux salari√©s d‚Äôune entreprise de se constituer un portefeuille de valeurs mobili√®res qui peuvent proposer des fonds ISR.\nEnfin, certains produits d‚Äô√©pargne retraite individuelle, comme le Plan d‚ÄôEpargne Retraite (PER).\n\nCes v√©hicules d‚Äôinvestissement permettent aux particuliers de contribuer √† une √©conomie plus durable tout en recherchant une performance financi√®re. Il est recommand√© de se rapprocher d‚Äôun conseiller pour d√©terminer le produit le mieux adapt√© √† ses objectifs financiers et √† ses valeurs √©thiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) n√©cessite une approche combinant analyses personnelle, financi√®re et extra-financi√®re, cette derni√®re se concentrant sur les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance)(comment2021?).\nPour choisir efficacement une entreprise ISR, il est crucial de r√©aliser une analyse √† triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR¬†qui vous convient le mieux. Elles vous aideront √† identifier le type de placement √† privil√©gier et d√©finir par exemple des fonds th√©matiques, d‚Äôexclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financi√®re : Elle permet d‚Äô√©valuer la performance √©conomique de l‚Äôentreprise, sa sant√© financi√®re, sa capacit√© √† g√©n√©rer des profits et √† maintenir une croissance durable. Cette analyse est indispensable pour s‚Äôassurer que l‚Äôentreprise est non seulement responsable, mais aussi viable et performante √† long terme.\nAnalyse extra-financi√®re (ESG) : Cette analyse compl√®te l‚Äô√©valuation financi√®re en examinant comment l‚Äôentreprise aborde les d√©fis et saisit les opportunit√©s li√©es aux crit√®res environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds int√©grant les crit√®res ESG comme les fonds labellis√©s ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du minist√®re de l‚Äô√©conomie et des finances fran√ßais qui garantit que le fonds d‚Äôinvestissement respecte des crit√®res ESG stricts4 dans ses choix d‚Äôinvestissement. Il assure √©galement que le fonds investit dans des entreprises qui adh√®rent √† ces principes, offrant ainsi une couche suppl√©mentaire de confiance pour les investisseurs soucieux de l‚Äôimpact de leurs placements.\nCe label est attribu√© aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,class√©es en 6 cat√©gories, qui constituent les 6 piliers du r√©f√©rentiel (crit√®resa?)."
  },
  {
    "objectID": "posts/ensai/others/ISR.html#footnotes",
    "href": "posts/ensai/others/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appel√© fonds de placement. Il s‚Äôagit d‚Äôune soci√©t√© d‚Äôordre public ou priv√© qui investit du capital pour soutenir des projets souvent innovants.‚Ü©Ô∏é\norganismes de placement collectif‚Ü©Ô∏é\nLes fonds d‚Äôinvestissement d‚Äôexclusion normatifs font r√©f√©rence aux fonds faisant l‚Äôobjet de plusieurs controverses.‚Ü©Ô∏é\ncf liste des fonds labellis√©s (listede?).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_garch.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_garch.html",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 et du TP-2 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap (TP1). Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, nous avons utilis√© des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold (TP2). Jusqu‚Äô√† maintenant, on consid√©rait que la s√©rie est iid. Cependant, dans la r√©alit√©, les s√©ries financi√®res sont souvent caract√©ris√©es par une d√©pendance temporelle et une volatilit√© conditionnelle.\nDans le cadre du TP3, il s‚Äôagira de prendre en compte la d√©pendance temporelle et la volatilit√© conditionnelle dans les s√©ries temporelles financi√®res. Pour ce faire, nous utiliserons un mod√®le de VAR dynamique avec le mod√®le GARCH.\nLe mod√®le GARCH (Generalized Autoregressive Conditional Heteroskedasticity) est un mod√®le de volatilit√© conditionnelle qui permet de mod√©liser la volatilit√© des rendements financiers. Il a √©t√© introduit par Bollerslev en 1986. Le mod√®le GARCH est une extension du mod√®le ARCH (Autoregressive Conditional Heteroskedasticity) introduit par Engle en 1982. Le mod√®le GARCH est d√©fini par les √©quations suivantes:\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\sum \\alpha_i \\epsilon_{t-i}^2 + \\sum \\beta_i \\sigma_{t-i}^2\n\\]\nDans ce mod√®le \\(\\mu_t\\) est un param√®tre de tendance moyenne √† identifier, \\(\\epsilon_t\\) est le r√©sidu, \\(\\sigma_t^2\\) est la variance conditionnelle, \\(z_t\\) est un bruit blanc, \\(\\omega\\) est un param√®tre de constante, \\(\\alpha_i\\) et \\(\\beta_i\\) sont les param√®tres du mod√®le GARCH √† identifier.\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\",start=\"2008-01-01\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\ndata.columns = data.columns.get_level_values(0)\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "I. Impl√©mentation de la VaR dynamique",
    "text": "I. Impl√©mentation de la VaR dynamique\n\nI.1. Pertinence du mod√®le AR(1)-GARCH(1,1)\nLe mod√®le AR(1)-GARCH(1,1) est le mod√®le qui, en pratique, est utilis√© pour r√©aliser la VaR dynamique. Cependant, il n‚Äôest pas tout le temps adapt√© aux donn√©es financi√®res. Dans ce TP, nous allons commencer par tester l‚Äô√©ligibilit√© de ce mod√®le dans le cadre des donn√©es que nous poss√©dons.\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, label='Train')\nplt.title('CAC 40 Log Returns')\nplt.show()\n\n\n\n\n\n\n\n\n\n## ACF et PACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(data_train, ax=plt.gca(), lags=40)\nplt.subplot(222)\nplot_pacf(data_train, ax=plt.gca(), lags=40)\nplt.show()\n\n\n\n\n\n\n\n\nDans la s√©rie temporelle que nous poss√©dons, nous constatons que la s√©rie peut √™tre mod√©liser par un AR(1). Pour un test plus rigoureux de cette hypoth√®se, nous allons utiliser la m√©thode de Lljung Box afin de d√©terminer le meilleur mod√®le qui puisse mod√©liser la s√©rie. Ainsi, pour un ordre pmax = 2 et qmax=2, nous allons : 1. Estimer les param√®tres du mod√®le ARMA(p,q) pour chaque combinaison de p et q 2. Calculer la statistique de Ljung Box pour chaque combinaison de p et q afin d‚Äôexaminer si les r√©sidus d‚Äôun mod√®le sont du bruit blanc 3. Filtrer les mod√®les pour lesquels les r√©sidus sont du bruit blanc 4. Choisir le meilleur mod√®le en utilisant le crit√®re d‚ÄôAkaike\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Param√®tres du mod√®le\np_max = 2\nq_max = 2\nbest_aic = np.inf\nbest_order = (0, 0, 0)\n\n# Chargement de la s√©rie temporelle (remplacer par la vraie s√©rie data_unindex)\n# Exemple fictif avec des donn√©es al√©atoires\nnp.random.seed(42)\ndata_unindex = data_train.copy()\ndata_unindex.reset_index(drop=True, inplace=True)\n\n# Cr√©ation de la matrice pour stocker les AIC\naic_matrix = pd.DataFrame(np.nan, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\nbb_test = pd.DataFrame(0, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\n# Boucle pour estimer les mod√®les et stocker les AIC\nfor p in range(p_max + 1):\n    for q in range(q_max + 1):\n        try:\n            model = ARIMA(data_unindex, order=(p, 0, q))\n            out = model.fit()\n            aic_matrix.loc[f\"p={p}\", f\"q={q}\"] = out.aic  # Stockage de l'AIC\n            \n            # Test de la blancheur des r√©sidus\n            ljung_box_result = acorr_ljungbox(out.resid, lags=[1], return_df=True)\n            p_value = ljung_box_result['lb_pvalue'].iloc[0]\n\n            if p_value &gt; 0.05:\n                bb_test.loc[f\"p={p}\", f\"q={q}\"] = 1\n            \n            # Mise √† jour du meilleur mod√®le\n            if out.aic &lt; best_aic :\n                best_aic = out.aic\n                best_order = (p, 0, q)\n                \n        except Exception as e:\n            print(f\"Erreur avec (p={p}, q={q}): {e}\")\n\nprint(f\"Meilleur mod√®le ARIMA: {best_order} avec AIC={best_aic}\")\n\nprint(\"=\"*30)\nprint(\"Matrice des AIC:\")\nprint(aic_matrix)\nprint(\"=\"*30)\nprint(\"Matrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\")\nprint(bb_test)\n\nMeilleur mod√®le ARIMA: (0, 0, 0) avec AIC=-20100.176479566246\n==============================\nMatrice des AIC:\n              q=0           q=1           q=2\np=0 -20100.176480 -20098.205891 -20097.679059\np=1 -20098.227385 -20099.862840 -20097.046957\np=2 -20097.887027 -20098.545030 -20094.033191\n==============================\nMatrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\n     q=0  q=1  q=2\np=0    1    1    1\np=1    1    1    1\np=2    1    1    1\n\n\n\np = 1\nq = 0\n\nAR1 = ARIMA(data_unindex, order=(p, 0, q))\nprint(AR1.fit().summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Wed, 11 Feb 2026   AIC                         -20098.227\nTime:                        03:02:56   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nEn utilisant la m√©thode √©nonc√©e plus haut, nous constatons que le mod√®le ARMA(0,0) est le meilleur mod√®le. En effet, c‚Äôest le mod√®le avec le crit√®re d‚ÄôAkaike le plus faible. Cela porte √† croire que la tendance moyenne de la s√©rie est constante. Nous allons tout de m√™me utiliser un mod√®le AR(1) pour la mod√©liser. En effet, c‚Äôest le deuxi√®me mod√®le avec un AIC faible.\nDans la s√©rie des r√©sidus, nous constatons des clusters de volatilit√© ce qui est signe d‚Äôune volatilit√© conditionnelle, et donc de la pr√©sence d‚Äôun GARCH. De plus, dans la s√©rie des r√©sidus du log-rendement, nous constatons une faible autocorr√©lation, ce qui les fait ressembler √† du bruit blanc. Toutefois, lorsque l‚Äôon examine ces r√©sidus au carr√©, la s√©rie temporelle pr√©sente g√©n√©ralement une forte autocorr√©lation, mise en √©vidence par la pr√©sence de grappes de volatilit√©. Cela sugg√®re que les rendements repr√©sentent un processus h√©t√©rosc√©dastique, ce qui rend le mod√®le GARCH particuli√®rement pertinent dans le cadre de notre √©tude.\n\nAR1_resid = AR1.fit().resid\nplt.figure(figsize=(10, 5))\nplt.plot(AR1_resid)\nplt.title(\"R√©sidus du mod√®le AR(1)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(AR1_resid, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1)\")\nplt.subplot(222)\nplot_acf(AR1_resid**2, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1) au carr√©\")\nplt.show()\n\n\n\n\n\n\n\n\nMotiv√©s par les commentaires de (Franke, H√§rdle et Hafner 2004) sugg√©rant que, dans les applications pratiques, les mod√®les GARCH avec des ordres plus petits d√©crivent souvent suffisamment les donn√©es et que dans la plupart des cas GARCH(1,1) est ad√©quat, nous avons consid√©r√© quatre combinaisons diff√©rentes de p=0, 1 et q=1, 2 pour chaque p√©riode afin d‚Äôentra√Æner le mod√®le GARCH, en supposant que les r√©sidus standardis√©s suivent une distribution normale.\n\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\n\ndef find_garch(p_min, p_max, q_min, q_max, data, dist=\"normal\"):\n    \"\"\"\n    Trouve le meilleur mod√®le GARCH(p, q) en minimisant l'AIC.\n\n    Param√®tres :\n    - p_min, p_max : Bornes pour p (ordre de l'AR dans la variance)\n    - q_min, q_max : Bornes pour q (ordre de MA dans la variance)\n    - data : S√©rie temporelle utilis√©e pour l'estimation\n    - dist : Distribution des erreurs (\"normal\", \"t\", \"ged\", etc.)\n\n    Retour :\n    - DataFrame contenant les valeurs de AIC pour chaque combinaison (p, q)\n    - Meilleur mod√®le GARCH trouv√© en fonction du crit√®re AIC\n    \"\"\"\n    \n    best_aic = np.inf\n    best_order = (0, 0, 0)\n    \n    results = []\n\n    for p in range(p_min, p_max + 1):\n        for q in range(q_min, q_max + 1):\n            try:\n                # Sp√©cification du mod√®le GARCH(p, q)\n                garch_spec = arch_model(data, vol='Garch', p=p, q=q, mean='Zero', dist=dist)\n                out = garch_spec.fit(disp=\"off\")\n                \n                # Calcul de l'AIC\n                current_aic = out.aic * len(data)\n\n                # Mettre √† jour le meilleur mod√®le si un plus petit AIC est trouv√©\n                if current_aic &lt; best_aic:\n                    best_aic = current_aic\n                    best_order = (p, 0, q)\n                \n                # Ajouter les r√©sultats dans la liste\n                results.append({'p': p, 'q': q, 'aic': current_aic, 'relative_gap': np.nan})\n            \n            except Exception as e:\n                print(f\"Erreur pour (p={p}, q={q}): {e}\")\n                continue\n    \n    # Convertir en DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Calculer l'√©cart relatif par rapport au meilleur AIC\n    results_df['relative_gap'] = (results_df['aic'] - best_aic) * 100 / best_aic\n    \n    return results_df, best_order\n\nresults_df, best_garch_order = find_garch(p_min=1, p_max=2, q_min=0, q_max=2, data=data_unindex, dist=\"normal\")\n\nprint(f\"Meilleur mod√®le GARCH: {best_garch_order} avec AIC={best_aic}\")\nprint(\"=\"*30)\nprint(\"R√©sultats pour les mod√®les test√©s:\")\nresults_df.sort_values(by='relative_gap', ascending=False)\n\nMeilleur mod√®le GARCH: (1, 0, 1) avec AIC=-20100.176479566246\n==============================\nR√©sultats pour les mod√®les test√©s:\n\n\n\n\n\n\n\n\n\np\nq\naic\nrelative_gap\n\n\n\n\n1\n1\n1\n-7.493455e+07\n-0.000000\n\n\n4\n2\n1\n-7.490963e+07\n-0.033254\n\n\n2\n1\n2\n-7.486519e+07\n-0.092567\n\n\n5\n2\n2\n-7.483014e+07\n-0.139333\n\n\n3\n2\n0\n-7.189514e+07\n-4.056091\n\n\n0\n1\n0\n-7.073946e+07\n-5.598342\n\n\n\n\n\n\n\nEn utilisant le crit√®re AIC pour s√©lectionner le meilleur mod√®le, nous avons conclu que GARCH(1,1) est effectivement le meilleur mod√®le.\n\ngarch11 = arch_model(data_unindex, vol='Garch', p=1, q=1, mean='Zero', dist='normal')\nprint(\"=\"*78)\nprint(\"R√©sum√© du mod√®le GARCH(1,1)\")\nprint(\"=\"*78)\nprint(garch11.fit(disp=\"off\").summary())\n\n==============================================================================\nR√©sum√© du mod√®le GARCH(1,1)\n==============================================================================\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Wed, Feb 11 2026   Df Residuals:                     3523\nTime:                        03:02:56   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\n\n\n\ncond_resid =garch11.fit(disp=\"off\").conditional_volatility # Volatilit√© conditionnelle =&gt; sigma_t\nresid = garch11.fit(disp=\"off\").resid # r√©sidus du mod√®le =&gt; eps_t\nresid_std = garch11.fit(disp=\"off\").std_resid  # r√©sidus studentis√©s =&gt; eta_t\n\n# jarque bera test\n\nfrom scipy.stats import jarque_bera\n\njb_test = jarque_bera(resid_std)\nprint(\"H0: Les r√©sidus studentis√©s suivent une loi normale\")\nprint(f\"Test de Jarque-Bera sur les r√©sidus studentis√©s: JB={jb_test[0]}, p-value={jb_test[1]}\")\n# reject the null hypothesis of normality for the distribution of the residuals, \n# as a rule of thumb, which implies that the data to be fitted is not\n# normally distributed\n\nH0: Les r√©sidus studentis√©s suivent une loi normale\nTest de Jarque-Bera sur les r√©sidus studentis√©s: JB=848.8557767883675, p-value=4.71313744144075e-185\n\n\n\n### y revenir\n\n### coeff &lt;1\n\n\n# Test d'homosc√©dasticit√©\n# Ljung-Box test sur r√©sidus\nlb_test_resid = acorr_ljungbox(resid_std, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur r√©sidus:\\n\", lb_test_resid)\n\n# Ljung-Box test sur carr√©s des r√©sidus\nlb_test_resid_sq = acorr_ljungbox(resid_std**2, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur carr√©s des r√©sidus:\\n\", lb_test_resid_sq)\n\nLjung-Box Test sur r√©sidus:\n      lb_stat  lb_pvalue\n1   0.028087   0.866904\n2   0.572026   0.751253\n3   0.690100   0.875530\n4   1.235029   0.872298\n5   2.199461   0.820914\n6   2.491801   0.869384\n7   2.828137   0.900433\n8   2.941010   0.938005\n9   3.793237   0.924486\n10  4.644433   0.913631\n11  4.727144   0.943657\n12  6.763448   0.872842\nLjung-Box Test sur carr√©s des r√©sidus:\n       lb_stat  lb_pvalue\n1    0.280711   0.596235\n2    0.339634   0.843819\n3    6.670837   0.083163\n4    7.395445   0.116409\n5    8.091586   0.151260\n6    8.233789   0.221471\n7    8.724987   0.273009\n8    9.386238   0.310768\n9    9.938908   0.355454\n10  11.579309   0.314198\n11  13.394501   0.268325\n12  13.845698   0.310670\n\n\n\n# LM test pour les effets ARCH\nfrom statsmodels.stats.diagnostic import het_arch\n\nlm_test = het_arch(resid_std)\nprint('LM Test Statistique: %.3f, p-value: %.3f' % (lm_test[0], lm_test[1]))\n\nLM Test Statistique: 12.218, p-value: 0.271\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(resid_std, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus studentis√©s\")\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\nplt.subplot(222)\nplot_pacf(resid_std, lags=40, ax=plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\nLe mod√®le AR(1)-GARCH(1,1) estim√© est le suivant :\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\no√π \\(\\mu_t = 0.0001 - 0.0037 r_{t-1}\\)\n\\[\n\\epsilon_t = \\sigma_t \\eta_t\n\\]\n\\[\n\\sigma_t^2 = 3.89 \\times 10^{-6} + 0.10 \\times \\epsilon_{t-i}^2 + 0.88 \\times \\sigma_{t-i}^2\n\\]\navec \\(\\eta_t\\) un bruit blanc suppos√©e gaussien.\nDans ce cas, nous rencontrons des probl√®mes au niveau de la significativit√© du coefficient AR(1). En effet, il aurait √©t√© plus judicieux de ne pas mod√©liser la tendance moyenne du rendement et la supposer constante. De plus, au niveau du GARCH(1,1), les r√©sidus sont bien des bruits blancs homosc√©dastiques (test de lljung box et test LM). Cependant, nous avons suppos√© que \\(\\eta_t\\) est un bruit blanc gaussien. Cela n‚Äôest pas v√©rifi√©. Il aurait √©t√© judicieux de tester d‚Äôautres distributions telles que Students‚Äôs t (‚Äôt‚Äô, ‚Äòstudentst‚Äô), Skewed Student‚Äôs t (‚Äòskewstudent‚Äô, ‚Äòskewt‚Äô) ou encore Generalized Error Distribution (GED).\n**Test de Lagrange Multiplier (LM) pour l'effet ARCH**\n\nLe test de Lagrange Multiplier (LM) pour l'effet ARCH est un outil statistique qui v√©rifie la pr√©sence d'effets ARCH (AutoRegressive Conditional Heteroskedasticity) dans une s√©rie temporelle.\n\nL'effet ARCH se manifeste lorsque la variance d'une erreur est une fonction de ses erreurs pass√©es. Cette propri√©t√© est courante dans les s√©ries temporelles financi√®res, o√π de grandes variations des rendements sont souvent suivies par de grandes variations et vice versa.\n\nLe test de LM v√©rifie l'hypoth√®se nulle que les erreurs sont homosc√©dastiques (variance constante). Si la p-value du test est inf√©rieure √† un seuil pr√©d√©fini (g√©n√©ralement 0,05), l'hypoth√®se nulle est rejet√©e, indiquant la pr√©sence d'effets ARCH.\n\n# Cr√©ation de la figure avec des sous-graphiques align√©s verticalement\nplt.figure(figsize=(10, 12))\n\n# Premier graphique : CAC 40\nplt.subplot(311)\nplt.plot(resid) \nplt.title(\"R√©sidus du mod√®le AR(1)\")\n\n# Deuxi√®me graphique : R√©sidus du mod√®le AR(1)\nplt.subplot(312)\nplt.plot(cond_resid)\nplt.title(\"Volatile conditionnelle du mod√®le GARCH(1,1)\")\n\n# Troisi√®me graphique : R√©sidus studentis√©s du mod√®le GARCH(1,1)\nplt.subplot(313)\nplt.plot(resid_std, label='R√©sidus studentis√©s du mod√®le GARCH(1,1)')\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\n\n# Affichage des graphiques\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2. Dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\)\n\\[\nr_t = \\mu_t + \\sigma_t \\times \\epsilon_t\n\\quad\n\\text{avec} \\quad\n\\begin{cases}\n    \\mu_t = \\mu + \\varphi r_{t-1} \\\\\n    \\sigma_t^2 = \\omega + a (r_{t-1} - \\mu_{t-1})^2 + b \\sigma_{t-1}^2\n\\end{cases}\n\\]\nPour avoir la dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\), nous allons utiliser les donn√©es historiques de la s√©rie temporelle ainsi que les estimations des param√®tres \\(\\Theta = (\\mu, \\varphi, \\omega, a, b)\\) du mod√®le AR(1)-GARCH(1,1) que nous avons estim√© pr√©c√©demment par maximum de vraisemblance.\nPour \\(t=1\\), nous allons initialiser \\(\\mu_1\\) par la moyenne \\(\\hat{\\mu}\\) et \\(\\sigma_1\\) par la variance √† long terme \\(\\frac{\\omega}{1 - a - b}\\).\n\nprint(AR1.fit().summary())\n\n# tester arima avec arch_model ou arch\nmu = AR1.fit().params[0]\nprint(f\"Param√®tre mu: {mu}\")\nphi = AR1.fit().params[1]\nprint(f\"Param√®tre phi: {phi}\")\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Wed, 11 Feb 2026   AIC                         -20098.227\nTime:                        03:02:56   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nParam√®tre mu: 0.00014959052741773347\nParam√®tre phi: -0.003743634042716415\n\n\n\nprint(garch11.fit(disp=\"off\").summary())\nomega = garch11.fit(disp=\"off\").params[0]\nprint(f\"Param√®tre omega: {omega}\")\na = garch11.fit(disp=\"off\").params[1]\nprint(f\"Param√®tre alpha: {a}\")\nb = garch11.fit(disp=\"off\").params[2]\nprint(f\"Param√®tre beta: {b}\")\n\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Wed, Feb 11 2026   Df Residuals:                     3523\nTime:                        03:02:57   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\nParam√®tre omega: 3.892997741815931e-06\nParam√®tre alpha: 0.1\nParam√®tre beta: 0.88\n\n\n\nT_train = len(data_train)\nT_test = len(data_test)\n\nT = T_train + T_test\n\n# Initialisation des s√©ries\nr = pd.concat([data_train, data_test], axis=0)\nmu_t = np.zeros(T)    # Composante moyenne\nsigma2 = np.zeros(T)  # Variance conditionnelle\n\n# Conditions initiales\nmu_t[0] = mu\nsigma2[0] = omega / (1 - a - b)  # Variance de long terme\n\n# Simulation du mod√®le\nfor t in range(1, T):\n    mu_t[t] = mu + phi * r[t-1]  # Partie moyenne\n    sigma2[t] = omega + a * (r[t-1] - mu_t[t-1])**2 + b * sigma2[t-1]  # Variance conditionnelle\n\n# Affichage des r√©sultats\nfig, ax = plt.subplots(3, 1, figsize=(10, 12))\n\nax[0].plot(r, color=\"blue\")\nax[0].set_title(\"Rendements $r_t$\")\nax[0].legend()\n\nax[1].plot(mu_t, color=\"green\")\nax[1].set_title(\"Composante moyenne $\\mu_t$\")\nax[1].legend()\n\nax[2].plot(np.sqrt(sigma2), color=\"red\")\nax[2].set_title(\"Volatilit√© conditionnelle $\\sigma_t$\")\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEn analysant la dynamique de \\(\\mu_t\\), nous constatons que la tendance moyenne est tr√®s semblable √† la s√©rie des log-rendements. Cela est d√ª au fait que le mod√®le AR(1) n‚Äôest pas pertinent pour mod√©liser la s√©rie. En effet, la s√©rie des log-rendements ressemble d√©j√† √† un bruit blanc. Par ailleurs, nous observons de fortes p√©riodes de volatilit√© dans la s√©rie des log-rendements pendant les p√©riodes de crises, i.e.¬†2008-2009 qui correspond √† la crise des subprimes et 2020 qui correspond √† la crise du Covid-19. Le mod√®le GARCH semble bien capturer ces p√©riodes de volatilit√© dans la volatilit√© conditionnelle calibr√©e.\n\n\nI.3. Estimation de la VaR\n\n# VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1 - alpha))\n\n# VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n# Loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n    \n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n    \n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(resid_std)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0], \n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n    \n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\n#### A FAIRE VAR POT et BM\n\nfrom scipy.stats import genextreme as gev\n\nimport numpy as np\nimport pandas as pd\nneg_resid = -resid_std\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nextremes = get_extremes(neg_resid, block_size=21, min_last_block=0.6)\nparams_gev = gev.fit(extremes)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de GEV sont : \")\nprint(\"-\"*15)\nprint(f\"Shape (xi) = {params_gev[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gev[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gev[2]:.2f}\")\nprint(\"=\"*80)\n\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\n\nu = 0.03\nexcess_values = [value - u for value in neg_resid if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*80)\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\nprint(\"=\"*80)\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.4250705574852528\nSigma :  0.8686243849485155\nGamma :  -0.6074103171572136\nNu :  5.607563514180729\n================================================================================\n================================================================================\nLes param√®tres estim√©s de la loi de GEV sont : \n---------------\nShape (xi) = -0.01\nLocalisation (mu) = 1.64\nEchelle (sigma) = 0.72\n================================================================================\n================================================================================\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = -0.04\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.80\n================================================================================\n\n\n\nalpha = 0.99\n\nvar_hist_train = historical_var(resid_std, alpha=alpha)\nvar_gauss_train = gaussian_var(resid_std, alpha=alpha)\nvar_sstd_train = sstd_var(alpha, params_sstd)\nvar_BM_train,_ = BM_var(0.99, 21, *params_gev)\nvar_POT_train,_ = POT_var(neg_resid, alpha, u,*params_gpd)\n\n# in a df\nvar = pd.DataFrame({\n    'Historique': [var_hist_train],\n    'Gaussienne': [var_gauss_train],\n    'Skew Student': [var_sstd_train],\n    'Block Maxima': [var_BM_train],\n    'Peak Over Threshold': [var_POT_train]\n})\n\nprint(\"=\"*80)\nprint(\"Value at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\")\nprint(round(100*var,2))\nprint(\"=\"*80)\n\n================================================================================\nValue at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\n   Historique  Gaussienne  Skew Student  Block Maxima  Peak Over Threshold\n0      264.12      229.76        280.29        268.68               284.98\n================================================================================\n\n\n\na. VaR historique dynamique\n\nvar_t = np.zeros(T_test)    # Composante moyenne\nnb_exp = 0\nfor t in range(T_test):\n    var_t[t] = - (mu_t[t+T_train] + np.sqrt(sigma2[t+T_train])*var_hist_train)\n    nb_exp += (r[t+T_train] &lt; var_t[t]).astype(int)\n    \nvar_t = pd.Series(var_t, index=data_test.index)\nprint(f\"Nombre d'exceptions = {nb_exp} sur {T_test} jours\")\n\nNombre d'exceptions = 4 sur 586 jours\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"blue\", label='Train')\nplt.plot(data_test, color=\"orange\", label='Test')\nplt.plot(var_t, color=\"red\",label='VaR dynamique')\nplt.axvline(x=data_test.index[0], color='black', linestyle='--')\nplt.legend()\nplt.title('S√©rie des log-rendements et VaR dynamique')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_test, color=\"orange\")\nplt.plot(var_t, color=\"red\")\nplt.title('Zoom sur la VaR dynamique')\n\nText(0.5, 1.0, 'Zoom sur la VaR dynamique')\n\n\n\n\n\n\n\n\n\n\n# backtest √† faire (optionnel)"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_evt.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_evt.html",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap. Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, on peut utiliser des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold.\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\",start=\"2008-01-01\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\ndata.columns = data.columns.get_level_values(0)\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes",
    "text": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes\n\nI.1. VaR TVE : Approche Maxima par bloc\nL‚Äôapproche des Block Maxima (BM) est une m√©thode mod√©lise les maxima des rendements sur des blocs de taille fixe \\(s\\) en utilisant la distribution GEV. Le seuil de confiance \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nNous allons dans ce projet une taille de blocs \\(s = 21\\) jours ouvr√©s comme ce qui souvent utilis√© en pratique. De ce fait, nous parvenons √† construire 239 blocs de taille 21 et un bloc de taille De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ _{}$ est :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV (\\(\\hat \\mu, \\hat \\sigma, \\hat \\xi\\)) estim√©e.\n\n\nI.1.1. Construction de l‚Äô√©chantillon de maxima sur data_train\n\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extr√™mes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n\n\n\n\n\n\n\n\nPour avoir une id√©e de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour \\(s=21\\), nous utilisons un Gumbel plot qui est un outil graphique pour juger de l‚Äôhypoth√®se \\(\\xi=0\\), i.e.¬†la distribution GEV se r√©duit √† la distribution de Gumbel.\nPour le construire, nous devons suivre les √©tapes suivantes :\n\ncalculer l‚Äôabscisse avec la s√©rie des maximas ordon√©es \\(R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}\\).\ncalculer l‚Äôordonn√©e de la mani√®re suivante :\n\n\\[\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n\\]\nLorsque la distribution adapt√©e est celle de Gumbel alors le Gumbel plot est lin√©aire. Dans notre cas, nous constatons une courbure ce qui nous pousse √† conclure qu‚Äôune distribution Gumbel n‚Äôest pas adapt√©e dans la mod√©lisation des maxima des pertes de rendements du CAC 40. Une distribution Fr√©chet ou de Weibull serait plus adapt√©e.\n\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.1.2. Estimation des param√®tres de la loi de GEV\nEn estimant les param√®tres de la loi GEV, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = -0.15, \\mu=0.02, \\sigma=0.01\\). Nous constatons par ailleurs que le param√®tre de forme \\(\\xi\\) est n√©gatif ce qui est coh√©rent avec notre observation pr√©c√©dente.\n\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParam√®tres estim√©s de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\nPour accorder plus de poids √† cette observation, nous avons calcul√© un intervalle de confiance profil√© √† 95% pour le param√®tre de forme \\(\\xi\\). Pour ce faire, nous avons suivi les √©tapes suivantes : 1. Estimation des param√®tres par maximum de vraisemblance : Nous avons estim√© \\(\\hat{\\xi}\\), \\(\\hat{\\mu}\\) et \\(\\hat{\\sigma}\\) en maximisant la log-vraisemblance de la loi GEV.\n\nConstruction du profil de vraisemblance : Nous avons fix√© \\(\\xi\\) √† diff√©rentes valeurs autour de \\(\\hat{\\xi}\\) et, pour chacune, r√©estim√© \\(\\mu\\) et \\(\\sigma\\) afin d‚Äôobtenir une log-vraisemblance profil√©e.\nSeuil bas√© sur le test du rapport de vraisemblance : Le seuil critique est d√©termin√© par la statistique $ ^2(1) $ :\n\\[\n\\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n\\]\nD√©termination des bornes de l‚ÄôIC : L‚Äôintervalle est form√© par les valeurs de \\(\\xi\\) pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l‚Äôincertitude en √©vitant les approximations asymptotiques classiques. La mod√©lisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adapt√©e.\nNous obtenons ainsi un intervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\). Comme 0 n‚Äôappartient pas √† cet intervalle, nous pouvons rejeter l‚Äôhypoth√®se \\(\\xi=0\\). De ce fait, la distribution de Weibull est plus adapt√©e pour mod√©liser les maxima des pertes de rendements du CAC 40 car \\(\\xi\\) est n√©gatif.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance n√©gative de la distribution GEV\n    en fixant le param√®tre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale &lt;= 0:  # Contrainte pour √©viter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profil√©s pour le param√®tre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estim√©e\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # R√©optimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' √† la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# D√©terminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood &gt;= chi2_threshold)[0]\nif len(valid_points) &gt; 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profil√© pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de d√©terminer des IC profil√©s avec les param√®tres actuels.\")\n\n# Trac√© du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n\nIC profil√© pour shape: [-0.287, -0.042]\n\n\n\n\n\n\n\n\n\n\na. Validation ex-ante\nOn remarque la loi GEV estim√©e par une weibull semble coller √† la distribution des rendements extr√™mes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles th√©oriques de la GEV-Weibull et empiriques sembelnt align√©s sauf pour les quantiles √©l√©v√©s o√π l‚Äôon constate un d√©crochage.\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV\")\nplt.xlabel('Quantiles th√©oriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nb. Calcul de la VaR TVE par MB\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV\\((\\hat \\mu, \\hat \\sigma, \\hat \\xi)\\) estim√©e, et \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nPour convertir la VaR √† horizon 1jour en VaR √† horizon T jours, la m√©thode de scaling soul√®ve quelques questions, car elle repose essentiellement sur la normalit√© et l‚Äôind√©pendance des rendements ce qui n‚Äôest pas le cas en pratique. De ce fait, nous utiliserons la m√©thode alternative reposant sur la th√©orie des valeurs extr√™mes.\ny revenir\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n\n\n\n\n\nI.1.3. Estimation des param√®tres de la loi de EV\nBien que l‚Äôintervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\) ne contienne pas 0, nous avons tout de m√™me estim√© les param√®tres de la loi EV pour comparer les r√©sultats avec ceux de la loi GEV. En estimant tout de m√™me les param√®tres de la loi EV, nous obtenons les param√®tres suivants : \\(\\mu=0.02, \\sigma=0.01, \\xi=0\\).\nNous constatons que la loi EV ne semble pas mal s‚Äôadapter √† la distribution des rendements extr√™mes du CAC 40.\n\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParam√®tres estim√©s de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GEV ajust√©e\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\n\n# Densit√© Gumbel ajust√©e\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajust√©e')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV Weibull\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi Gumbel\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n\n\nDe plus, les r√©sultats en terme de VaR sont tr√®s proches entre les deux mod√®les."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "href": "posts/ensai/risques_financiers/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "I.2. VaR TVE : Approche Peak over threshold",
    "text": "I.2. VaR TVE : Approche Peak over threshold\n\nI.2.1. Choix du seuil u\nCette m√©thode est bas√©e sur la mod√©lisation de la distribution des exc√®s au-dessus d‚Äôun seuil √©lev√© de log-rendement n√©gatif (\\(u\\)), seuil d√©termin√© de mani√®re subjective √† partir de l‚Äôanalyse du mean residual life plot, en ajustant une distribution de Pareto g√©n√©ralis√©e (GPD). Dans le mean residual life plot, si les exc√®s au-del√† de ùíñ suivent une loi GPD, alors le mean-excess plot a un comportement lin√©aire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est lin√©aire. Nous ne privil√©gions pas les seuils \\(u\\) √©lev√©s puisque la moyenne est faite sur peu d‚Äôobservations.\nNous allons choisir un seuil \\(u = 0.03\\) pour lequel le mean residual life plot est lin√©aire. Nous allons ensuite ajuster une distribution GPD pour les exc√®s au-dessus de ce seuil.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Param√®tres :\n    - data : array-like, donn√©es d'entr√©e.\n    - tlim : tuple (min, max), limites des seuils (si None, calcul√© automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils √† consid√©rer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les donn√©es\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn &lt;= 5:\n        raise ValueError(\"Les donn√©es contiennent trop peu de valeurs valides.\")\n\n    # D√©finition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # √âvite les 4 plus grandes valeurs\n\n    if np.all(data &lt;= tlim[1]):\n        raise ValueError(\"La borne sup√©rieure du seuil est trop √©lev√©e.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data &lt;= tlim[0]), np.mean(data &lt;= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les r√©sultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data &gt; u] - u  # Exc√®s au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Trac√© du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile √† 5%\n\n\n\n\n\n\n\n\n\n\nI.2.2. Estimation des param√®tres de la loi GPD\nEn estimant les param√®tres de la loi GPD, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01\\). De ce fait, la distribution de Pareto g√©n√©ralis√©e est adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\).\n\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n\n\n\n\nI.2.3. Validation ex-ante\nEn comparant la distribution GPD estim√©e et la distribution empirique des exc√®s, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estim√© indique que les quantiles th√©oriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observ√©s dans notre distribution des exc√®s. Nous concluons que la distribution GPD n‚Äôest pas adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\). Cela peut √™tre d√ª √† un mauvais choix du seuil \\(u\\), une analyse plus aprofondie aurait √©t√© n√©cessaire pour choisir un seuil plus adapt√©.\n\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Donn√©es observ√©es des exc√®s')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GPD ajust√©e\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajust√©e')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une mod√©lisation par loi GPD\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance \\({\\alpha}\\) est alors obtenue par la formule :\n\\[\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n\\]\no√π \\(\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})\\) est la fonction de r√©partition de la GPD(\\(\\hat{\\sigma},\\hat{\\xi}\\)) estim√©e, \\(\\alpha_{\\text{POT}}\\) est le quantile ajust√©, n√©cessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\nComme on ne se concentre que sur l‚Äô√©chantillon des exc√®s dans cette mod√©lisation, l‚Äôestimation de la VaR √† partir de la GPD ne doit pas se faire au niveau \\(\\alpha\\), mais √† un niveau ajust√© \\(\\alpha_{\\text{POT}}\\) d√©fini par la relation suivante :\n\\[\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n\\]\no√π \\(n\\) repr√©sente le nombre total d‚Äôobservations, \\(N_u\\) correspond au nombre d‚Äôexc√®s au-del√† du seuil \\(u\\),\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html",
    "title": "Projet de gestion de risques multiples",
    "section": "",
    "text": "Les banques jouent un r√¥le central dans l‚Äô√©conomie nationale et internationale. En effet, elles assurent l‚Äôinterm√©diation entre les agents disposant d‚Äôun exc√©dent de financement et ceux ayant un besoin de financement, facilitant ainsi les transactions et soutenant l‚Äôinvestissement, et donc la croissance √©conomique. Toutefois, cette activit√© les expose √† divers risques majeurs, notamment le risque de cr√©dit, le risque de liquidit√© et le risque de march√©. La quantification de ces risques est essentielle pour permettre aux institutions bancaires de s‚Äôen pr√©munir et de les surveiller efficacement. Dans ce cadre, la Value-at-Risk (VaR) s‚Äôimpose comme une mesure de r√©f√©rence. Elle permet d‚Äô√©valuer la perte potentielle maximale qu‚Äôune institution pourrait subir, avec un certain niveau de confiance, sur un horizon temporel donn√© et pour un portefeuille sp√©cifique.\nDans l‚Äôanalyse de la VaR, un portefeuille est souvent compos√© d‚Äôau moins deux cr√©ances. Il est donc indispensable de prendre en compte les d√©pendances entre les facteurs de risque. Une approche classique consiste √† supposer que le vecteur des risques individuels suit une distribution normale multivari√©e et √† utiliser le coefficient de corr√©lation lin√©aire de Pearson comme mesure de d√©pendance. Cependant, cette hypoth√®se est souvent trop restrictive en finance : les distributions des facteurs de risque ne sont pas n√©cessairement gaussiennes et le coefficient de Pearson ne permet pas toujours de capturer les structures de d√©pendance non lin√©aires. De plus, cette mesure de corr√©lation est pertinente uniquement dans un cadre gaussien, qui repr√©sente rarement les dynamiques financi√®res r√©elles.\nDans ce contexte, la th√©orie des copules constitue un outil statistique puissant permettant de mod√©liser la d√©pendance entre les risques sans se limiter √† l‚Äôhypoth√®se de normalit√©. Une copule est une fonction qui caract√©rise la structure de d√©pendance entre plusieurs variables al√©atoires ind√©pendamment de leurs distributions marginales. En s√©parant la mod√©lisation des distributions marginales et celle de la d√©pendance conjointe, les copules offrent une flexibilit√© accrue pour l‚Äôanalyse du risque et permettent de mieux repr√©senter les interactions entre les actifs financiers.\nL‚Äôobjectif de ce projet est d‚Äô√©valuer le risque de cr√©dit en calculant une CreditVaR avec un niveau de confiance de 99% sur un portefeuille compos√© de deux obligations bancaires. Ces obligations, bien que diff√©rentes en termes de subordination et de risque de recouvrement, appartiennent au m√™me secteur, ce qui accro√Æt le risque global du portefeuille. Il est donc essentiel de mod√©liser ad√©quatement la d√©pendance entre ces actifs pour obtenir une estimation r√©aliste du risque de cr√©dit."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#ii.3.-comparaison-des-distributions",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#ii.3.-comparaison-des-distributions",
    "title": "Projet de gestion de risques multiples",
    "section": "II.3. Comparaison des distributions",
    "text": "II.3. Comparaison des distributions\n\nplt.figure(figsize=(10, 5))\n\nplt.plot(x, y_BNP, label='BNP')\nplt.plot(x, y_SG, label='SG')\nplt.title(\"Densit√©s de probabilit√© des taux de recouvrement\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBNP senior : L‚Äôobligation senior b√©n√©ficie d‚Äôun taux de recouvrement plus √©lev√© et plus stable, donc moins risqu√©.\nSG junior : L‚Äôobligation junior a une probabilit√© √©lev√©e d‚Äôun recouvrement tr√®s faible, ce qui refl√®te un risque plus important.\n\nCela est coh√©rent avec la cat√©gorisation des obligations. Dans la hi√©rarchie des dettes, une obligation peut √™tre class√©e comme senior ou junior (subordonn√©e) en fonction de la priorit√© de remboursement en cas de faillite de l‚Äô√©metteur. Cette distinction est essentielle pour √©valuer le risque de cr√©dit et le taux de recouvrement attendu. Une obligation class√©e senior est moins risqu√©e qu‚Äôune obligation junior, car elle est rembours√©e en premier en cas de d√©faut de l‚Äô√©metteur, cependant le rendement attendu est moins √©lev√©. Par cons√©quent, les obligations senior ont un taux de recouvrement plus √©lev√© et plus stable que les obligations junior."
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.1.-analyse-exploratoire-univari√©e-des-donn√©es-actions-de-ces-deux-entreprises.",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.1.-analyse-exploratoire-univari√©e-des-donn√©es-actions-de-ces-deux-entreprises.",
    "title": "Projet de gestion de risques multiples",
    "section": "III.1. Analyse exploratoire univari√©e des donn√©es actions de ces deux entreprises.",
    "text": "III.1. Analyse exploratoire univari√©e des donn√©es actions de ces deux entreprises.\nEn observant le prix des actions BNP et SG, nous constatons que les actions BNP ont un prix plus √©lev√©s que les actions SG. Cela est coh√©rent avec la capitalisation boursi√®re des deux entreprises. De plus, nous constatons que les rendements de BNP sont semblables √† ceux de SG. Cela est coh√©rent avec le fait que les deux entreprises sont des banques fran√ßaises et sont donc expos√©es aux m√™mes risques macro√©conomiques. N√©anmoins, les actions de BNP pr√©sentent une volatilit√© l√©g√®rement plus √©lev√© que celles de SG.\n\n# read data.txt\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data_copules.txt', sep=\"\\t\")\ndata.head()\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n0\n42.36\n55.24\n\n\n1\n42.72\n55.59\n\n\n2\n43.20\n56.45\n\n\n3\n42.67\n55.55\n\n\n4\n41.81\n54.50\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data['BNP'], label='BNP')\nplt.plot(data['SG'], label='SG')\nplt.title(\"Prix des actions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nreturns = pd.DataFrame()\nreturns[\"BNP\"] = data[\"BNP\"].pct_change().dropna()\nreturns[\"SG\"] = data[\"SG\"].pct_change().dropna()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Premier sous-graphique : Rendements BNP\naxes[0].plot(returns[\"BNP\"], color='tab:blue')\naxes[0].set_title(\"Rendements BNP\")\naxes[0].grid(True)\n\n# Deuxi√®me sous-graphique : Rendements SG\naxes[1].plot(returns[\"SG\"], color='tab:orange')\naxes[1].set_title(\"Rendements SG\")\naxes[1].grid(True)\n\n# Ajustement automatique pour √©viter les chevauchements\nplt.tight_layout()\n\n# Affichage\nplt.show()\n\n\n\n\n\n\n\n\n\nreturns.describe()\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n999.000000\n999.000000\n\n\nmean\n-0.000590\n-0.000473\n\n\nstd\n0.024340\n0.020690\n\n\nmin\n-0.116199\n-0.093616\n\n\n25%\n-0.013771\n-0.011760\n\n\n50%\n-0.000358\n-0.000547\n\n\n75%\n0.012605\n0.011286\n\n\nmax\n0.086786\n0.079479"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.2.-mod√©lisation-des-distributions-univari√©es-des-facteurs-de-risques",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.2.-mod√©lisation-des-distributions-univari√©es-des-facteurs-de-risques",
    "title": "Projet de gestion de risques multiples",
    "section": "III.2. Mod√©lisation des distributions univari√©es des facteurs de risques",
    "text": "III.2. Mod√©lisation des distributions univari√©es des facteurs de risques\nDans le cadre des prix des actions, le seul facteur de risque est le rendement. Nous allons donc mod√©liser les rendements des actions de BNP et SG par des lois normales, lois student, skew student, et Normal Inverse Gaussian afin de d√©terminer la loi qui s‚Äôajuste le mieux aux donn√©es.\n\nIII.2.1. Mod√©lisation des rendements de BNP\nEn ce qui concerne les rendements de BNP, nous constatons que les lois de student et normal inverse gaussian sont les plus adapt√©es pour mod√©liser les rendements de BNP. En effet, lorsqu‚Äôon compare les QQ-plot des rendements de BNP avec les lois normales, student, skew student et normal inverse gaussian, on constate que les quantiles empiriques des rendements de BNP sont plus proches des quantiles th√©oriques des lois student et normal inverse gaussian. De plus, les densit√©s ajust√©es semblent √©galement mieux coller aux donn√©es.\nSi l‚Äôon devait choisir une loi pour mod√©liser les rendements de BNP, nous choisirions la loi normal inverse gaussian. En effet, bien qu‚Äôelle soit plus complexe √† mod√©liser, elle permet d‚Äôavoir un p-value, au test de Kolmogorov-Smirnov, plus √©lev√© que la loi student. Cela signifie que la loi normal inverse gaussian est plus adapt√©e pour mod√©liser les rendements de BNP.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de donn√©es pour l'exemple (remplace par tes donn√©es r√©elles)\ndata = returns[\"BNP\"]\n\n# Cr√©ation de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densit√© ajust√©e - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des param√®tres de la distribution de Student pour vos donn√©es.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densit√© ajust√©e - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densit√© ajust√©e - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU = pd.DataFrame(index=returns.index, columns=returns.columns)\n\nU['BNP'] = stats.norminvgauss.cdf(data,*params_nig)\n\n\n\n\n\n\n\n\n\nprint(params_norm)\nprint(params_std)\nprint(params_nig)\n\n(np.float64(-0.0005897932128196069), np.float64(0.024327449407557697))\n(np.float64(4.605552436800593), np.float64(-0.00046153919290846087), np.float64(0.018734588738427177))\n(np.float64(1.114374971099747), np.float64(-0.01637147555477723), np.float64(-0.00021011775496697913), np.float64(0.025828797181227353))\n\n\n\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.051878  0.008908\nStudent                   0.027225  0.441752\nNormal Inverse Gaussian   0.026466  0.477953\n==================================================\n\n\n\n\nIII.2.1. Mod√©lisation des rendements de SG\nComme les rendements de BNP, les rendements de SG sont mieux mod√©lis√©s par les lois student et normal inverse gaussian.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de donn√©es pour l'exemple (remplace par tes donn√©es r√©elles)\ndata = returns[\"SG\"]\n\n# Cr√©ation de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densit√© ajust√©e - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des param√®tres de la distribution de Student pour vos donn√©es.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densit√© ajust√©e - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densit√© ajust√©e - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU['SG'] = stats.norminvgauss.cdf(data,*params_nig)\n\nU = U.to_numpy()\n\n\n\n\n\n\n\n\n\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.047345  0.021967\nStudent                   0.025635  0.519206\nNormal Inverse Gaussian   0.022462  0.685866\n=================================================="
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.3.-etude-de-la-structure-de-d√©pendance",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.3.-etude-de-la-structure-de-d√©pendance",
    "title": "Projet de gestion de risques multiples",
    "section": "III.3. Etude de la structure de d√©pendance",
    "text": "III.3. Etude de la structure de d√©pendance\nL‚Äô√©valuation de la d√©pendance entre les facteurs de risque sera r√©alis√©e en utilisant des outils graphiques bas√©s sur des crit√®res non param√©triques, tels que les nuages de points, les ajustements lin√©aires et le d√©pendogramme.\nCes m√©thodes, choisies pour leur capacit√© √† traiter des donn√©es sans pr√©supposer une distribution sp√©cifique, offrent une approche flexible et visuelle pour identifier et analyser les relations entre les variables de risque. Par exemple, les nuages de points permettent de visualiser la dispersion et la relation potentielle entre deux variables, tandis que les ajustements lin√©aires cherchent √† mod√©liser la relation par une ligne droite, facilitant ainsi la compr√©hension des tendances g√©n√©rales.\nLe d√©pendogramme, quant √† lui, repr√©sente la structure de d√©pendance sous la forme du nuage de points des marges uniformes extraites de l‚Äô√©chantillon n couples de donn√©es \\(\\left(\\left(x_{1,1} ; x_{2,1}\\right), \\cdots,\\left(x_{1, n} ; x_{2, n}\\right)\\right)\\), i.e.¬†:\n\\[\nu_{i, j}=\\frac{1}{n} \\sum_{k=1}^n 1_{\\left\\{x_{j, k} \\leq x_{j,i}\\right\\}}, \\quad i \\in[1, n], \\quad \\forall j \\in[1,2]\n\\]\nLe d√©pendogramme de l‚Äô√©chantillon est donc la repr√©sentation de n couples \\(\\left(\\left(u_{1,1} ; u_{2,1}\\right), \\cdots,\\left(u_{1, n} ; u_{2, n}\\right)\\right)\\). Il permet d‚Äôobserver le caract√®re plus ou moins simultan√© des r√©alisations issues de l‚Äô√©chantillon.\n\n# Tableax avec tx de pearson, spearman et kendall\n\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\npearson = pearsonr(returns[\"BNP\"], returns[\"SG\"])\nspearman = spearmanr(returns[\"BNP\"], returns[\"SG\"])\nkendall = kendalltau(returns[\"BNP\"], returns[\"SG\"])\n\ntableau_correlation = pd.DataFrame({\n    \"Pearson\": pearson,\n    \"Spearman\": spearman,\n    \"Kendall\": kendall\n}, index=[\"Coefficient\", \"p-value\"])\n\ntableau_correlation\n\n\n\n\n\n\n\n\nPearson\nSpearman\nKendall\n\n\n\n\nCoefficient\n8.646588e-01\n8.409565e-01\n6.690959e-01\n\n\np-value\n2.383438e-300\n3.609139e-268\n9.880880e-220\n\n\n\n\n\n\n\nEn analysant la corr√©lation entre les rendements de BNP et SG, nous constatons qu‚Äôil y a une corr√©lation positive significative peu importe le test de corr√©lation effectu√©. La corr√©lation de spearman et de pearson indique qu‚Äôil y a une liaison monotone et lin√©aire forte d‚Äôau moins 84% entre les rendements des deux entreprises. En ce qui concerne le taux de kendall, nous constatons une corr√©lation positive significative de 67% environ. Cela signifie que les rendements des actions de BNP et SG sont positivement corr√©l√©s.\n\n\n\n\n\n\nWarning\n\n\n\nAttention la corr√©lation de pearson n‚Äôest pas une mesure de concordance contrairement au coefficient de spearman. Dans notre cas, il indique une corr√©lation lin√©aire forte.\n\n\n\n# Dependogramme\n# Posons x1, u1 = BNP et x2, u2 = SG\nimport warnings\nfrom scipy.stats import rankdata\n\nwarnings.filterwarnings(\"ignore\")\n\n# 1. Transformation en pseudo-observations\ndef pseudo_observations(X):\n    \"\"\"Transforme les donn√©es en pseudo-observations U dans [0,1].\"\"\"\n    n, d = X.shape\n    U = np.zeros((n, d))\n    for j in range(d):\n        U[:, j] = rankdata(X[:, j]) / (n + 1)  # Pour √©viter les 1 stricts\n    return U\n\nX = returns.to_numpy()\nu_obs = pseudo_observations(X)\n\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1,2,1) # 1 ligne, 2 colonnes, premier graphique\n\n# nuages de points des donn√©es BNP et SG\nplt.scatter(returns[\"BNP\"], returns[\"SG\"], cmap=\"viridis\")\n# Ajout la droite qui s'ajuste aux donn√©es\nplt.plot(np.unique(returns[\"BNP\"]), np.poly1d(np.polyfit(returns[\"BNP\"], returns[\"SG\"], 1))(np.unique(returns[\"BNP\"])), color=\"red\", label=\"Droite d'ajustement\")\nplt.title(\"Nuages de points des rendements\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.grid(False)\n\nplt.subplot(1,2,2)\nplt.scatter(u_obs[:,0], u_obs[:,1])\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\nEn observant le d√©pendogramme, nous constatons la m√™me d√©pendance positive entre les deux entreprises. Cela signifie que si l‚Äôune des entreprises fait d√©faut, l‚Äôautre a plus de chance de faire d√©faut √©galement. Cela est coh√©rent avec la corr√©lation positive observ√©e entre les rendements des actions de BNP et SG.\nDe plus, en observant le d√©pendogramme, il semble avoir des d√©pendances √† gauche et √† droite entre les deux entreprises. Nous allons tout de m√™me tester un √©ventail de copules afin de v√©rifier laquelle des copules est la plus adapt√©e √† notre cas : - Copules elliptiques : gaussienne, Student. - Copules archim√©diennes : Clayton, Gumbel, Frank.\nIl faudrait au pr√©alable estimer les param√®tres des copules archim√©diennes et elliptiques pour d√©terminer laquelle des copules est la plus adapt√©e √† notre cas. Pr√©cedemment, nous avons estim√© les param√®tres des lois marginales des rendements de BNP et SG. Nous allons utiliser ces param√®tres pour estimer les param√®tres des copules. Nous allons utiliser ces param√®tres pour la mod√©lisation des diff√©rentes copules.\n\nfrom scipy.optimize import minimize\nfrom statsmodels.distributions.copula.api import (\n    GaussianCopula, StudentTCopula, ClaytonCopula, GumbelCopula, FrankCopula\n)\n\ndef get_copula(copula_type, params):\n    if copula_type == \"gaussian\":\n        rho = params[0]\n        return GaussianCopula(corr=np.array([[1, rho], [rho, 1]]))\n    elif copula_type == \"student\":\n        rho, nu = params\n        return StudentTCopula(corr=np.array([[1, rho], [rho, 1]]), df=nu)\n    elif copula_type == \"clayton\":\n        theta = params[0]\n        return ClaytonCopula(theta=theta)\n    elif copula_type == \"gumbel\":\n        theta = params[0]\n        return GumbelCopula(theta=theta)\n    elif copula_type == \"frank\":\n        theta = params[0]\n        return FrankCopula(theta=theta)\n    else:\n        raise ValueError(f\"Copula type {copula_type} not supported\")\n\n\n# Log-vraisemblance n√©gative pour estimation MLE\ndef negative_log_likelihood(params, U, copula_type):\n    copula = get_copula(copula_type, params)\n    log_likelihood = copula.logpdf(U)\n    return -np.sum(log_likelihood)\n\n# Ajustement de la copule (MLE)\ndef fit_copula(U, copula_type):\n    if copula_type in [\"gaussian\", \"gumbel\", \"clayton\", \"frank\"]:\n        if copula_type==\"gaussian\":\n            x0 = [0.6]\n        else:\n            x0 = [3]\n        bounds = [(1e-5, 10)] if copula_type != \"gaussian\" else [(-0.99, 0.99)]\n    elif copula_type == \"student\":\n        x0 = [0.6, 3]  # rho et df\n        bounds = [(-0.99, 0.99), (2, 30)]\n\n    result = minimize(negative_log_likelihood, x0, args=(U, copula_type), bounds=bounds, method='Nelder-Mead')\n\n    if not result.success:\n        raise RuntimeError(f\"MLE failed for {copula_type} copula: {result.message}\")\n\n    return result.x"
  },
  {
    "objectID": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.4.-mod√©lisation-la-structure-de-d√©pendance-au-moyen-des-copules-param√©triques",
    "href": "posts/ensai/risques_financiers/value-at-risk/pj_copules.html#iii.4.-mod√©lisation-la-structure-de-d√©pendance-au-moyen-des-copules-param√©triques",
    "title": "Projet de gestion de risques multiples",
    "section": "III.4. Mod√©lisation la structure de d√©pendance au moyen des copules param√©triques",
    "text": "III.4. Mod√©lisation la structure de d√©pendance au moyen des copules param√©triques\nPour mod√©liser la structure de d√©pendance entre les d√©fauts, de mani√®re pr√©cise, nous utiliserons les copules. Une copule est une fonction de r√©partition multivari√©e de marginales uniformes sur \\([0,1]\\). Dans le cas bivari√©, on a:\n\\[\n\\mathrm{C}\\left(\\mathrm{u}_1, \\mathrm{u}_{\\mathrm{2}}\\right)=\\mathrm{P}\\left[\\mathrm{U}_1 \\leq \\mathrm{u}_1,\\mathrm{U}_{\\mathrm{2}} \\leq \\mathrm{u}_{\\mathrm{2}}\\right]\n\\]\nDans le cadre de ce projet, nous allons √©tudier deux principales familles de copules pr√©sent√©s dans le tableau :\n\nCopules elliptiques : gaussienne, Student\nCopules archim√©diennes : Clayton, Gumbel, Frank\n\n\\[\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{l|c|cc}\n\\hline\nFamille & Nom    & Copule $C(u,v)$   & Param√®tres \\\\ \\hline\n\\multirow{2}{*}{Elliptique} & Gaussien & $\\Phi_{\\Sigma}\\left(\\Phi^{-1}(u),\\Phi^{-1}(v)\\right)$ & $\\rho$ \\\\\n& Student &  $T_{\\Sigma}\\left(T_{\\nu}^{-1}(u),T_{\\nu}^{-1}(v)\\right)$ & $\\rho, \\nu$ \\\\\n\\midrule\n\\multirow{3}{*}{Archim√©dienne} & Frank  & $\\frac{1}{\\theta} \\left(-\\ln(1+ \\frac{(e^{-\\theta u}  - 1)(e^{-\\theta v} - 1)}{e^{-\\theta v} - 1})\\right)$ & $\\theta \\ne 0$ \\\\\n& Gumbel & $\\exp\\left(-\\left((- \\ln(u))^{\\theta} + (- \\ln(v))^{\\theta}\\right)^{\\frac{1}{\\theta}}\\right)$ & $\\theta \\geq 1$ \\\\\n& Clayton  & $(u^{-\\theta} + v^{-\\theta} -1)^{-\\frac{1}{\\theta}}$ & $\\theta &gt; 0$\n\\\\ \\hline\n\\end{tabular}\\\\\n{\\footnotesize *$\\Sigma$ est la matrice de variance covariance. }\n\\caption{Copules archim√©diennes bivari√©es les plus courantes.}\n\\label{tab:copule_family}\n\\end{table}\n\\]\nIl s‚Äôagira apr√®s estimation des param√®tres et des tests d‚Äôajustement, la copule la plus ad√©quate pour mod√©liser au mieux la d√©pendance entre les variables √©tudi√©es.\nPour l‚Äôestimation des param√®tres des copules s√©lectionn√©es, plusieurs approches m√©thodologiques s‚Äôoffrent √† nous: la m√©thode des moments, la m√©thode du maximum de vraisemblance et l‚Äôapproche IFM. Nous privil√©gierons l‚Äôapproche IFM (Inference Functions for Margins) pr√©sent√© ci dessous (algo \\(\\ref{IFM}\\)). Cet algorithme a l‚Äôavantage d‚Äô√™tre plus rapide que la m√©thode du maximum de vraisemblance.\nPour l‚Äô√©valuation de l‚Äôajustement des copules √† la structure de d√©pendance d‚Äôun √©chantillon, nous utiliserons des outils graphiques tels que le d√©pendogramme, pr√©sent√© pr√©cedemment, et le Kendall plot.\nLe Kendall plot permet une comparaison directe entre la copule empirique et la copule th√©orique. Plus le Kendall plot se rapproche d‚Äôune droite, plus l‚Äôajustement entre la structure de d√©pendance de l‚Äô√©chantillon et la copule estim√©e sur ce m√™me √©chantillon est bon.\n\nIII.4.1 Copule gaussienne\n\nIII.4.1.a. Estimation des param√®tres de la copule gaussienne\nDans le cadre de la copule gaussienne, il nous faut la matrice de variance qui est donn√©e par :\n\\[\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\]\nDans notre cas, le seul param√®tre √† estimer est le coefficient de corr√©lation de pearson. Nous allons donc estimer le coefficient de corr√©lation de pearson entre les rendements des actions de BNP et SG.\n\nrho = fit_copula(u_obs, \"gaussian\")[0]\n\nmat= np.array([[1, rho], [rho, 1]])\nmat\n\narray([[1.        , 0.86097656],\n       [0.86097656, 1.        ]])\n\n\n\n\nIII.4.1.b. Simulation de la copule gaussienne\nPour simuler la copule gaussienne, nous allons utiliser la m√©thode de distribution puisque la loi est facile √† impl√©menter.\n\n# # Simuler r√©alisation W suivant une loi normale centr√©e multivari√©e\n# import scipy.stats as stats\n\n# np.random.seed(0)\n# n = 1000\n# W = np.random.multivariate_normal([0, 0], mat, n)\n\n# # Calculer U1 et U2\n# U = np.zeros((n, 2))\n\n# for i in range(n):\n#     U[i,0] = stats.norm.cdf(W[i,0])\n#     U[i,1] = stats.norm.cdf(W[i,1])\n\n\nn = u_obs.shape[0]\n_ = GaussianCopula(corr = rho).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.1.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule gaussienne\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Kendall plot\nimport numpy as np\n\ndef calculer_Hi_vect(U):\n    n=len(U)\n    H = np.zeros(n, dtype=int)\n    # Comparaison de chaque paire une seule fois\n    for i in range(n):\n        # Cr√©er des masques bool√©ens pour les conditions\n        cond1 = U[:, 0] &lt;= U[i, 0]  # u_{1,j} &lt;= u_{1,i}\n        cond2 = U[:, 1] &lt;= U[i, 1]  # u_{2,j} &lt;= u_{2,i}\n\n        # Appliquer les conditions et exclure le cas o√π i == j\n        H[i] = (np.sum(np.logical_and(cond1, cond2)) - 1)\n\n    return H/(len(U)-1)\n\n\ndef kendall_plot(U,S=1000,copula=\"gaussian\",rho=None,nu=None,theta=None):\n\n    H_i = calculer_Hi_vect(U)\n    n = len(U)\n\n    H_means = np.zeros((S, n)) # S lignes et n colonnes\n    for s in range(S):\n        if copula == 'gaussian':\n            X_ = GaussianCopula(corr=rho).rvs(n)\n        elif copula == 'student':\n            X_ = StudentTCopula(df = nu, corr = rho).rvs(n)\n        elif copula == \"gumbel\":\n            X_= GumbelCopula(theta = theta).rvs(n)\n        elif copula == \"clayton\":\n            X_= ClaytonCopula(theta = theta).rvs(n)\n        elif copula == \"frank\":\n            X_= FrankCopula(theta = theta).rvs(n)\n        U_=pseudo_observations(X_)\n        H_means[s] = np.sort(calculer_Hi_vect(U_))\n\n    H_mean = np.mean(H_means, axis=0) # axis=0 pour moyenne par colonne\n\n    x,y = np.sort(H_i), np.sort(H_mean)\n    print(x.shape, y.shape)\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(x, y)\n    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n    plt.title(f\"Kendall plot for {copula} copula\")\n    plt.xlabel(\"i\")\n    plt.ylabel(\"Kendall\")\n    plt.grid(True)\n    plt.show()\n\nS=1000\nkendall_plot(u_obs,S,copula=\"gaussian\",rho=rho)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.1.d. Test d‚Äôad√©quation\n\n# Copule empirique Cn(u)\ndef empirical_copula_cdf(U, u):\n    \"\"\"Calcule la copule empirique Cn(u).\"\"\"\n    return np.mean(np.all(U &lt;= u, axis=1))\n\n# Statistique de Cram√©r-von Mises\ndef cramer_von_mises_stat(U, copula):\n    \"\"\"Calcule la statistique de test Tn.\"\"\"\n    n = len(U)\n    Tn = 0.0\n    for i in range(n):\n        u_i = U[i]\n        Cn = empirical_copula_cdf(U, u_i)  # Copule empirique Cn(u_i)\n        C_theta = copula.cdf([u_i])  # Copule th√©orique estim√©e CŒ∏(u_i)\n        Tn += (Cn - C_theta) ** 2\n    return Tn\n\n# Test d‚Äôad√©quation complet avec bootstrap param√©trique\ndef adequation_test(X, copula_type=\"gaussian\", M=500):\n    \"\"\"\n    Test d'ad√©quation de Genest & R√©millard (2008) pour une copule avec\n    statistique de Cram√©r-von Mises et bootstrap param√©trique.\n    \"\"\"\n    # Pseudo-observations\n    U = pseudo_observations(X)\n    n = len(U)\n\n    # Estimation MLE de la copule sur les donn√©es\n    params = fit_copula(U, copula_type)\n    copula = get_copula(copula_type, params)\n\n    # Calcul de la statistique observ√©e Tn\n    T_obs = cramer_von_mises_stat(U, copula)\n\n    # Bootstrap param√©trique\n    T_boot = []\n    for _ in range(M):\n        # 1. Simulation d‚Äôun √©chantillon sous la copule ajust√©e\n        U_boot = copula.rvs(n)\n\n        # 2. R√©-estimation de la copule sur U_boot\n        params_boot = fit_copula(U_boot, copula_type)\n        copula_boot = get_copula(copula_type, params_boot)\n\n        # 3. Calcul de Tn pour cet √©chantillon bootstrap\n        T_boot.append(cramer_von_mises_stat(U_boot, copula_boot))\n\n    # Calcul de la p-value (proportion des T_boot sup√©rieurs √† T_obs)\n    p_value = np.mean(np.array(T_boot) &gt;= T_obs)\n\n    return {\n        \"copula_type\": copula_type,\n        \"params\": params,\n        \"T_obs\": T_obs,\n        \"p_value\": p_value\n    }\n\nfrom pprint import pprint\nresult_gaussian = adequation_test(X, copula_type=\"gaussian\")\npprint(result_gaussian)\n\n{'T_obs': np.float64(0.05456423166335546),\n 'copula_type': 'gaussian',\n 'p_value': np.float64(0.892),\n 'params': array([0.86097656])}\n\n\n\n\n\nIII.4.2 Copule de student\n\nIII.4.2.a. Estimation des param√®tres de la copule student\n\nrho,nu = fit_copula(u_obs, \"student\")\n\nprint(\"rho = \", rho)\nprint(\"nu = \", nu)\n\nrho =  0.8496143388748167\nnu =  2.0\n\n\n\n\nIII.4.2.b. Simulation de la copule de student\n\nn = u_obs.shape[0]\n_ = StudentTCopula(corr = rho, df=nu).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.2.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de student\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"student\",rho=rho,nu=nu)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAttention, nous avons utilis√© le package copula.api de statsmodels pour impl√©menter les copules. Cependant, la classe correspondant √† la copule de Student ne permet pas d‚Äôobtenir une fonction de r√©partition (voir lien).\nEn utilisant un environnement virtuel, il a √©t√© possible de modifier le fichier statsmodels/distributions/copula/elliptical.py du package afin d‚Äôimpl√©menter la m√©thode pour la fonction de r√©partition. Vous trouverez ce fichier ci-joint afin de garantir le bon fonctionnement du code si vous devez le relancer.\n\n\n\n\nIII.4.2.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_std = adequation_test(X, copula_type=\"student\") # ATTENTION DIFFICULTE POUR ESSTIMER COPULE STUDENT\npprint(result_std)\n\n{'T_obs': np.float64(0.05937889644009823),\n 'copula_type': 'student',\n 'p_value': np.float64(0.85),\n 'params': array([0.84961434, 2.        ])}\n\n\n\n\n\nIII.4.3 Copule de clayton\n\nIII.4.3.a. Estimation des param√®tres de la copule clayton\n\ntheta = fit_copula(u_obs, \"clayton\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  2.4768310546874988\n\n\n\n\nIII.4.3.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = ClaytonCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.3.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de clayton\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nS=1000\nkendall_plot(u_obs,S,copula=\"clayton\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.3.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_clayton= adequation_test(X, copula_type=\"clayton\") \npprint(result_clayton)\n\n{'T_obs': array([0.86477661]),\n 'copula_type': 'clayton',\n 'p_value': np.float64(0.004),\n 'params': array([2.47683105])}\n\n\n\n\n\nIII.4.4 Copule de gumbel\n\nIII.4.4.a. Estimation des param√®tres de la copule gumbel\n\ntheta = fit_copula(u_obs, \"gumbel\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  2.9939208984374996\n\n\n\n\nIII.4.4.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = GumbelCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.4.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de gumbel\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"gumbel\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.4.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_gumbel = adequation_test(X, copula_type=\"gumbel\") \npprint(result_gumbel)\n\n{'T_obs': array([0.03485682]),\n 'copula_type': 'gumbel',\n 'p_value': np.float64(0.988),\n 'params': array([2.9939209])}\n\n\n\n\n\nIII.4.4 Copule de frank\n\nIII.4.4.a. Estimation des param√®tres de la copule frank\n\ntheta = fit_copula(u_obs, \"frank\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  10.0\n\n\n\n\nIII.4.4.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = FrankCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.4.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de frank\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"frank\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.4.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_frank = adequation_test(X, copula_type=\"frank\") \npprint(result_frank)\n\n{'T_obs': array([0.12454173]),\n 'copula_type': 'frank',\n 'p_value': np.float64(0.492),\n 'params': array([10.])}\n\n\n\n\n\nIII.5. R√©sultats\nNos r√©sultats mettent en √©vidence l‚Äôimportance de la structure de d√©pendance dans l‚Äô√©valuation du risque de cr√©dit. Apr√®s avoir test√© plusieurs copules param√©triques, nous avons retenu la copule de Gumbel comme la plus appropri√©e, en raison de son bon ajustement aux donn√©es et de sa capacit√© √† capturer les asym√©tries et les queues de distribution lourdes, essentielles dans un contexte de crise financi√®re.\n\ndict_list = [result_gaussian, result_std, result_clayton, result_gumbel, result_frank]\n\n# Convert to DataFrame\ndf = pd.DataFrame(dict_list)\ndf\n\n\n\n\n\n\n\n\ncopula_type\nparams\nT_obs\np_value\n\n\n\n\n0\ngaussian\n[0.8609765625000002]\n0.054564\n0.892\n\n\n1\nstudent\n[0.8496143388748167, 2.0]\n0.059379\n0.850\n\n\n2\nclayton\n[2.4768310546874988]\n[0.8647766118220173]\n0.004\n\n\n3\ngumbel\n[2.9939208984374996]\n[0.034856823632549765]\n0.988\n\n\n4\nfrank\n[10.0]\n[0.12454172887238589]\n0.492"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/pj_courbe_tx.html",
    "href": "posts/ensai/proc_stochastique/pj_courbe_tx.html",
    "title": "Mod√®les de courbe de taux",
    "section": "",
    "text": "Que ce soit pour les banques, les assureurs ou les fonds de pension, la courbe de taux z√©ro-coupon constitue la brique de base pour la valorisation de nombreux instruments financiers, de la d√©termination du prix des obligations aux produits d√©riv√©s plus complexes tels que les swaps de taux, les caplets, les swaptions ou encore les produits structur√©s de taux. De ce fait, la construction fiable, coh√©rente et r√©guli√®rement mise √† jour d‚Äôune courbe de taux z√©ro-coupon repr√©sente un enjeu majeur.\nDans ce contexte, ce projet propose une m√©thodologie compl√®te de reconstitution de la courbe de taux z√©ro-coupon implicite, √©labor√©e √† partir des cotations de march√© disponibles sur diff√©rents segments : Money Market (march√© mon√©taire), Futures et Swaps. L‚Äôapproche adopt√©e repose sur une combinaison de bootstrapping, permettant d‚Äôextraire les taux z√©ro-coupon pour chaque maturit√© observable, et d‚Äôinterpolations par spline cubique, afin d‚Äôassurer la lissit√© et la continuit√© de la courbe sur l‚Äôensemble de l‚Äô√©ch√©ancier, y compris sur les maturit√©s non directement observ√©es.\nLa courbe obtenue sert ensuite de r√©f√©rence pour la valorisation de produits d√©riv√©s de taux, en particulier les caplets et les swaptions, via le mod√®le classique de Black, largement utilis√© sur les march√©s. Toutefois, afin de mieux capturer la dynamique temporelle des taux d‚Äôint√©r√™t et de prendre en compte la structure temporelle de la volatilit√© implicite, la seconde partie du projet repose sur la calibration d‚Äôun mod√®le de Hull-White, un mod√®le de taux affine avec retour √† la moyenne. La calibration est r√©alis√©e √† partir des cotations de caplets at-the-money (ATM), √† l‚Äôaide d‚Äôune proc√©dure de recherche num√©rique par dichotomie.\nLe projet met √©galement en √©vidence la sensibilit√© de la courbe de taux forward et des prix d‚Äôoptions aux param√®tres de march√©, notamment la volatilit√© et le param√®tre de mean reversion du mod√®le Hull-White. Cette analyse de sensibilit√© illustre comment la structure de la courbe de taux et son √©volution future sont influenc√©es par les hypoth√®ses de mod√©lisation, ce qui est particuli√®rement crucial pour les desks de trading, les gestionnaires d‚Äôactifs ou les √©quipes de gestion actif-passif (ALM).\nEnfin, le projet se prolonge par une extension appliqu√©e aux produits structur√©s : la valorisation de caplets √† barri√®re d√©sactivante (knock-out caplets), qui n√©cessite une approche par simulation Monte-Carlo. Cette extension illustre comment la dynamique du taux court, simul√©e sous la mesure forward neutre, peut √™tre exploit√©e pour √©valuer des produits de plus en plus complexes, r√©pondant √† des besoins sp√©cifiques d‚Äôinvestisseurs ou de gestionnaires de risques.\n\n\n\n\n\n\nImportant\n\n\n\nLe rapport de ce projet est disponible ici."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/pj_courbe_tx.html#introduction",
    "href": "posts/ensai/proc_stochastique/pj_courbe_tx.html#introduction",
    "title": "Mod√®les de courbe de taux",
    "section": "",
    "text": "Que ce soit pour les banques, les assureurs ou les fonds de pension, la courbe de taux z√©ro-coupon constitue la brique de base pour la valorisation de nombreux instruments financiers, de la d√©termination du prix des obligations aux produits d√©riv√©s plus complexes tels que les swaps de taux, les caplets, les swaptions ou encore les produits structur√©s de taux. De ce fait, la construction fiable, coh√©rente et r√©guli√®rement mise √† jour d‚Äôune courbe de taux z√©ro-coupon repr√©sente un enjeu majeur.\nDans ce contexte, ce projet propose une m√©thodologie compl√®te de reconstitution de la courbe de taux z√©ro-coupon implicite, √©labor√©e √† partir des cotations de march√© disponibles sur diff√©rents segments : Money Market (march√© mon√©taire), Futures et Swaps. L‚Äôapproche adopt√©e repose sur une combinaison de bootstrapping, permettant d‚Äôextraire les taux z√©ro-coupon pour chaque maturit√© observable, et d‚Äôinterpolations par spline cubique, afin d‚Äôassurer la lissit√© et la continuit√© de la courbe sur l‚Äôensemble de l‚Äô√©ch√©ancier, y compris sur les maturit√©s non directement observ√©es.\nLa courbe obtenue sert ensuite de r√©f√©rence pour la valorisation de produits d√©riv√©s de taux, en particulier les caplets et les swaptions, via le mod√®le classique de Black, largement utilis√© sur les march√©s. Toutefois, afin de mieux capturer la dynamique temporelle des taux d‚Äôint√©r√™t et de prendre en compte la structure temporelle de la volatilit√© implicite, la seconde partie du projet repose sur la calibration d‚Äôun mod√®le de Hull-White, un mod√®le de taux affine avec retour √† la moyenne. La calibration est r√©alis√©e √† partir des cotations de caplets at-the-money (ATM), √† l‚Äôaide d‚Äôune proc√©dure de recherche num√©rique par dichotomie.\nLe projet met √©galement en √©vidence la sensibilit√© de la courbe de taux forward et des prix d‚Äôoptions aux param√®tres de march√©, notamment la volatilit√© et le param√®tre de mean reversion du mod√®le Hull-White. Cette analyse de sensibilit√© illustre comment la structure de la courbe de taux et son √©volution future sont influenc√©es par les hypoth√®ses de mod√©lisation, ce qui est particuli√®rement crucial pour les desks de trading, les gestionnaires d‚Äôactifs ou les √©quipes de gestion actif-passif (ALM).\nEnfin, le projet se prolonge par une extension appliqu√©e aux produits structur√©s : la valorisation de caplets √† barri√®re d√©sactivante (knock-out caplets), qui n√©cessite une approche par simulation Monte-Carlo. Cette extension illustre comment la dynamique du taux court, simul√©e sous la mesure forward neutre, peut √™tre exploit√©e pour √©valuer des produits de plus en plus complexes, r√©pondant √† des besoins sp√©cifiques d‚Äôinvestisseurs ou de gestionnaires de risques.\n\n\n\n\n\n\nImportant\n\n\n\nLe rapport de ce projet est disponible ici."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "href": "posts/ensai/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "title": "Mod√®les de courbe de taux",
    "section": "I. Reconstitution de la courbe de taux",
    "text": "I. Reconstitution de la courbe de taux\n\nI.1. Formules de valorisation des taux de march√©\nLa courbe interbancaire est une courbe de taux qui repr√©sente les taux d‚Äôint√©r√™t auxquels les banques se pr√™tent de l‚Äôargent entre elles. Elle est utilis√©e pour d√©terminer les taux d‚Äôint√©r√™t des pr√™ts et des emprunts √† court terme. Elle est construite sur le court terme (maturit√©&lt;6M) √† partir des taux du march√©s mon√©taire (Money Market) bas√©s sur les d√©pots non garantis entre banques. Sur le moyen terme (6m - 3y) elle est construite √† partir des contrats futures, i.e.¬†des forwards sur un march√© OTC (Over The Counter) et sur le long terme (&gt;3y) elle est construite √† partir des contrats de swap euribor (Euro Interbank Offered Rate) 3M ou 6M.\nCi dessous, nous disposons de ces donn√©es de taux de march√© cot√©s sur le march√© interbancaire. Nous allons essayer de reconstituer la courbe de taux zero coupon implicite, qui ne cote pas directement sur le march√©. Le fichier de donn√©es contient trois variables : - Type d‚Äôinstruments (Money Market, Futures, Swap) - Maturit√© (en ann√©es) - Taux d‚Äôint√©r√™t\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_excel('data/Data_tx.xlsx', sheet_name='tx_marche')\ndata.columns = ['type', 'T', 'tx']\ndata\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n5\nFUT\n1.50\n0.977094\n\n\n6\nFUT\n1.75\n0.974981\n\n\n7\nFUT\n2.00\n0.972911\n\n\n8\nFUT\n2.25\n0.970984\n\n\n9\nFUT\n2.50\n0.969711\n\n\n10\nFUT\n2.75\n0.968436\n\n\n11\nSWAP\n3.00\n0.026112\n\n\n12\nSWAP\n4.00\n0.028117\n\n\n13\nSWAP\n5.00\n0.029680\n\n\n14\nSWAP\n6.00\n0.031107\n\n\n15\nSWAP\n7.00\n0.032313\n\n\n16\nSWAP\n8.00\n0.033382\n\n\n17\nSWAP\n9.00\n0.034385\n\n\n18\nSWAP\n10.00\n0.035312\n\n\n19\nSWAP\n11.00\n0.036197\n\n\n20\nSWAP\n12.00\n0.037003\n\n\n21\nSWAP\n13.00\n0.037668\n\n\n22\nSWAP\n14.00\n0.038201\n\n\n23\nSWAP\n15.00\n0.038624\n\n\n24\nSWAP\n20.00\n0.039380\n\n\n25\nSWAP\n25.00\n0.038501\n\n\n26\nSWAP\n30.00\n0.037668\n\n\n\n\n\n\n\nEn l‚Äôabsence d‚Äôoppotunit√© d‚Äôarbitrage, les valorisations des instruments de march√© s‚Äôexpriment en fonction des taux z√©ro coupon implicites suivantes :\n\nSur le segment Money Market, on cote en taux mon√©taires :\n\\[\n  L_t(T,T+\\delta) = \\frac{1}{\\delta} \\left( \\frac{B(t,T)}{B(t,T+\\delta)} - 1 \\right),\n  \\]\navec t le temps courant, T la maturit√© et \\(\\delta\\) la p√©riode de capitalisation. Dans notre cas, t=T=0 et \\(\\delta\\) varie en fonction de la maturit√©.\nSur le segment Future, on c√¥te en 1 - tx forward :\n\\[\n  future(T, T+\\delta) = 1 - L_t(T,T+\\delta).\n  \\] Dans notre cas, t=0, T= maturit√© - 3m et \\(\\delta = 3m\\).\nSur le segment Swap, on c√¥te en taux swap :\n\\[\n  Swap(t, T_0, T_n) = B(t, T_0)‚àíB(t, T_n)‚àíK √ólvl(t),\n  \\]\navec K le taux fixe du swap qui √©galise la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de march√© √† la maturit√© \\(T_n\\). Dans notre cas, la date de d√©part est le spot, i.e.¬†\\(T_0=0\\) et \\(T_n\\) est la maturit√© du swap, et t=0 (vu d‚Äôaujourd‚Äôhui).\n\nDe ce fait, le donn√©es ne sont pas homog√®nes en taux du fait de la diff√©rente cotations des instruments. Nous allons donc les transformer en taux mon√©taires pour les homog√©n√©iser.\nRemarques pr√©liminaires : - En zone EURO, les swaps standards c√¥t√©s sur le march√© ont une fr√©quence de paiement semestrielle pour la patte variable et annuelle pour la patte fixe. Ainsi pour le calcul du level du swap, \\(\\delta=1\\) et on ajoute progressivement les taux de march√©. - Pour simplifier les calculs, nous supposerons que les dates de d√©part des taux mon√©taires et des taux de swap sont spot (i.e.¬†T0 = 0 et non 1 ou 2 jours).\nMethode de bootstrapping & stripping :\nPour extraire les taux z√©ro coupon implicites, nous allons utiliser la m√©thode de bootstrapping. Cette m√©thode consiste √† calculer les taux z√©ro coupon implicites √† partir des taux de march√©. Pour cela, nous allons utiliser les formules des taux mon√©taires pr√©sent√©es ci-dessus, qui sont vu comme des fonctions de taux z√©ro coupon implicites.\nComme les taux de swap ne sont pas n√©cessairement disponibles pour toutes les maturit√©s annuelles, il faut interpoler les taux interm√©diaires. Cela permettra de simplifier la m√©thode de bootstrapping. Nous allons utiliser une interpolation par spline cubic afin d‚Äôavoir des taux swap par an. Une interpolation par spline permet d‚Äôavoir des bonnes propri√©t√©s en terme de d√©rivabilit√© et de continuit√© de la courbe de taux.\nIl s‚Äôagira donc de construire une nouvelle courbe de taux de march√© discr√®te avec des cotations annuelles de taux swap √† l‚Äôaide d‚Äôune m√©thode d‚Äôinterpolation par spline, en plus des autres instruments. Par la suite, on supposera que cette nouvelle courbe est la courbe de march√© de r√©f√©rence, i.e.¬†la courbe utilis√©e pour impliciter les taux z√©ro coupon.\nEnfin, nous allons faire du stripping afin de reconstituer une courbe de taux zero coupon implicite plus lisse √† l‚Äôaide de diff√©rentes m√©thodes d‚Äôinterpolations (lin√©aire, spline, etc).\n\n\nI.2. Construction de la courbe des taux z√©ro-coupon\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\n\ndef interpolate_and_update_df(data, col_x, col_y, kind='cubic', start=3, end=30, step=1):\n    \"\"\"\n    Interpole les taux SWAP et met √† jour le DataFrame avec les nouvelles valeurs interpol√©es.\n\n    Param√®tres :\n    - data : DataFrame d'origine contenant une colonne 'type' avec 'SWAP', 'T' et 'tx'.\n    - kind : Type d'interpolation (par d√©faut 'cubic', peut √™tre 'linear', 'quadratic', etc.).\n    - start : Valeur minimale de T pour l'interpolation (par d√©faut 3).\n    - end : Valeur maximale de T pour l'interpolation (par d√©faut 31).\n    - step : Pas d'incr√©mentation pour la grille interpol√©e (par d√©faut 1).\n\n    Retourne :\n    - new_df : DataFrame mis √† jour avec les taux SWAP interpol√©s.\n    \"\"\"\n\n    data = data.copy()\n    x, y = data[col_x].values, data[col_y].values\n\n    f = interp1d(x, y, kind=kind)\n    xnew = np.arange(start, end+step, step)\n    tx_new = f(xnew)\n    df = pd.DataFrame({col_x: xnew, col_y: tx_new})\n\n    return df\n\ndf_interp = interpolate_and_update_df(data, 'T', 'tx') \ndf_interp['type'] = 'SWAP'\ndf_interp.head()\n\n\n\n\n\n\n\n\nT\ntx\ntype\n\n\n\n\n0\n3\n0.026112\nSWAP\n\n\n1\n4\n0.028117\nSWAP\n\n\n2\n5\n0.029680\nSWAP\n\n\n3\n6\n0.031107\nSWAP\n\n\n4\n7\n0.032313\nSWAP\n\n\n\n\n\n\n\n\n# Le rajoiuter dans le df\nnew_df = pd.concat([data[data[\"type\"] != \"SWAP\"], df_interp], ignore_index=True)\nnew_df.head()\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n\n\n\n\n\n\n# Affichage d'une courbe homog√®ne de taux de march√© en fonction de la maturit√©\nnew_df['tx_h'] = new_df.apply(lambda x: 1 - x['tx'] if x['type'] == 'FUT' else x['tx'], axis=1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], marker='o', linestyle='-', color='b')\nplt.xlabel('Maturit√©')\nplt.ylabel('Taux de march√©')\nplt.title('Courbe des taux de march√©')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nExtraction du taux zero coupon sur le segment Money Market\nLes taux z√©ro-coupon continus sont d√©finis par la formule suivante :\n\\[\nr(t,T) = -\\frac{1}{T-t} \\ln B(t,T),\n\\]\no√π B(t,T) est le facteur d‚Äôactualisation (\\(B(t,T) = exp(-r(t,T) \\times  T)\\) ), i.e.¬†le prix d‚Äôune obligation z√©ro-coupon de maturit√© T √† la date t.\nIls sont la brique de base pour la valorisation des produits d√©riv√©s et des obligations. De ce fait, nous allons essayer de reconstituer la courbe des taux z√©ro-coupon implicite √† partir de la courbe des taux de march√© √† l‚Äôaide de la m√©thode du bootstrapping. Cette m√©thode consiste √† calculer les taux z√©ro-coupon implicites √† partir des taux de march√© en utilisant la formule suivante selon le segment Money Market :\n\\[\nL_0(0,\\delta) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0,\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,\\delta) = \\frac{1}{1 + \\delta L_0(0,\\delta)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Money Market\nmm = new_df[new_df['type'] == 'MM']\n\nmm.loc[:, 'B'] = 1 / (1 + mm['tx'] * mm['T'])\nmm.loc[:, 'R'] = - np.log(mm['B']) / mm['T']\n\ndf_ZC = mm\n\n\n\nExtraction du taux zero coupon sur le segment Future\nSur le segment Future, on a :\n\\[\nFuture = 1 - L_0(T,T+\\delta) = 1 - \\frac{1}{\\delta} \\left( \\frac{B(0,T)}{B(0,T+\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,T+\\delta) = \\frac{B(0,T)}{1 + \\delta (1- Future)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Futures\n\nfut = new_df[new_df['type'] == 'FUT']\n\n# concat √† mm\ndf_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\nmm_len = len(mm)\n\nfor i in range(mm_len, len(df_ZC)):\n    df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, 'tx'])* 0.25)\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, 'T']\n\n\n\nExtraction du taux zero coupon sur le segment swap\nPour le segment swap payeur, on a :\n\\[\nSwap(t, T_0, T_n) = B(t, T_0)‚àíB(t, T_n)‚àíK √ólvl(t) = 0,\n\\]\navec K le taux fixe du swap qui fait que la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de march√© √† la maturit√© \\(T_n\\). De ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,T_n) = \\frac{1 - K \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1 + K}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Swaps\nswap = new_df[new_df['type'] == 'SWAP']\nfut_len = len(fut)\n\ndf_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\nfor i in range(mm_len+fut_len, len(df_ZC)):\n    T_n = df_ZC.loc[i, 'T']  # R√©cup√®re la valeur de T actuelle\n    mask = (df_ZC['T'] &lt; T_n) & (df_ZC['T'] % 1 == 0)  # S√©lectionne uniquement les T entiers &lt; T_n\n    df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, 'tx'] * sum(df_ZC.loc[mask, 'B'].fillna(0)))/(1+df_ZC.loc[i, 'tx'])\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC['T'], df_ZC['R'], label='Courbe de taux z√©ro coupon', marker='o')\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de march√©', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux z√©ro coupon discr√©tis√©e')\n\nText(0.5, 1.0, 'Courbe de taux z√©ro coupon discr√©tis√©e')\n\n\n\n\n\n\n\n\n\nComme on peut le constater, le mode d‚Äôinterpolation a un impact significatif sur le calcul des taux de march√© car il affecte la forme de la courbe des taux et donc la valorisation des instruments financiers.\n\n\n\nI.3. Construction de la courbe des taux forward\nA partir de la courbe des taux z√©ro-coupon issue de la m√©thode de bootstrapping, nous souhaitons tracer la courbe des taux forwards de tenor 3M en fonction de la maturit√© √† l‚Äôaide des m√©thodes d‚Äôinterpolation lin√©aire et par spline, avec une discr√©tisation de 0.1 an.\nPour tracer la courbe taux forward, on utilisera la formule suivante pour calculer les taux forward :\n\\[\nL_0(T,T+\\delta) = \\frac{1}{\\delta} (\\frac{B(0,T)}{B(0,T+\\delta)} - 1)\n\\] avec \\(\\delta = 0.25\\).\nPour le segment swap, il s‚Äôagira d‚Äôinterpoler les taux z√©ro-coupon implicites pour avoir des tx forwards 3M. # changer la discretisatio √† 1an ce qui est diff√©rent du t√©nor.\n\ndf_ZC = pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC], ignore_index=True)\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\n\ndef compute_forward_rates(R, T_range, tau):\n    \"\"\" Calcule les taux forward pour chaque maturit√© \"\"\"\n    fwd_rates = []\n    T_values = []\n    for i in range(len(T_range)-1):\n        T = T_range[i]\n        T_tau = T + tau\n        if T_tau &gt;= max(T_range):\n            break  # √âviter d'extrapoler au-del√† des donn√©es disponibles\n        B_T = np.exp(-R[i] * T)\n        R_T_tau = np.interp(T_tau, T_range, R) \n        B_T_tau = np.exp(-R_T_tau * T_tau)\n\n        # Formule du taux forward instantan√©\n        fwd_rate = (B_T / B_T_tau - 1) / tau\n        fwd_rates.append(fwd_rate)\n        T_values.append(T)\n\n    return pd.DataFrame({\"T\": T_values, \"tx_fwd\": fwd_rates})\n\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation lin√©aire')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation lin√©aire')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nLorsqu‚Äôon utilise une interpolation lin√©aire, on obtient une courbe plus discontinue. La courbe a une structure en marches d‚Äôescalier. Il y a des sauts brusques lorsque l‚Äôon passe d‚Äôun intervalle √† un autre. En effet, par nature, l‚Äôinterpolation lin√©aire qui ne prend pas en compte les points interm√©diaires. En interpolant avec une fonction spline, on obtient une courbe plus lisse et continue.Elle est plus coh√©rente avec l‚Äô√©volution naturelle des taux d‚Äôint√©r√™t. En effet, la fonction spline est une fonction polynomiale qui passe par tous les points de la courbe. Elle est plus flexible et permet de mieux capturer les variations des taux d‚Äôint√©r√™t.\nNous sommes int√©ress√©s √† ce qui pourrait se passer lorsque nous shiftons le taux de swap 5Y de 10 points de base. Cela permet de d√©terminer la sensibilit√© de la courbe des taux forward aux variations des taux de swap et donc donner des indications sur comment hedger ce risque.\nNous allons donc calculer le taux forward 3M pour les deux courbes de taux forward et comparer les r√©sultats.\n\nchoc = 10/10000\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx_h']+choc if x['T'] == 5 else x['tx_h'], axis=1)\n\n# plot\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_s\"], label='Taux de march√© shift√©s', color=\"r\")\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de march√©', color=\"b\")\nplt.xlabel('Maturit√©')\nplt.ylabel('Taux de march√©')\nplt.title('Courbe des taux de march√©')\nplt.legend()\nplt.grid()\nplt.show()\n\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx']+choc if x['T'] == 5 else x['tx'], axis=1)\n\n\n\n\n\n\n\n\n\n# Extraction des facteurs d'actualisation pour les Money Market\nimport numpy as np\nimport pandas as pd\n\ndef compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx\"):\n    \"\"\"\n    Calcule les facteurs d'actualisation (B) et les taux z√©ro-coupon (R) \n    √† partir des taux du march√© pour les instruments MM, FUT et SWAP.\n\n    Param√®tres :\n    - new_df : DataFrame contenant les taux du march√© avec les colonnes sp√©cifi√©es.\n    - col_T : Nom de la colonne contenant les maturit√©s (ex: \"T\").\n    - col_tx : Nom de la colonne contenant les taux du march√© (ex: \"tx\").\n\n    Retourne :\n    - df_ZC : DataFrame contenant les facteurs d'actualisation et les taux z√©ro-coupon.\n    \"\"\"\n\n    # --- Extraction des donn√©es du march√© mon√©taire (MM) ---\n    mm = new_df[new_df['type'] == 'MM'].copy()\n    mm.loc[:, 'B'] = 1 / (1 + mm[col_tx] * mm[col_T])\n    mm.loc[:, 'R'] = - np.log(mm['B']) / mm[col_T]\n\n    df_ZC = mm.copy()\n\n    # --- Extraction des donn√©es Futures (FUT) ---\n    fut = new_df[new_df['type'] == 'FUT'].copy()\n    df_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\n    mm_len = len(mm)\n\n    # --- Calcul des facteurs d'actualisation pour les Futures ---\n    for i in range(mm_len, len(df_ZC)):\n        df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, col_tx]) * 0.25)\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, col_T]\n\n    # --- Extraction des donn√©es Swaps (SWAP) ---\n    swap = new_df[new_df['type'] == 'SWAP'].copy()\n    fut_len = len(fut)\n    df_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\n    # --- Calcul des facteurs d'actualisation pour les Swaps ---\n    for i in range(mm_len + fut_len, len(df_ZC)):\n        T_n = df_ZC.loc[i, col_T]  # Maturit√© actuelle\n        mask = (df_ZC[col_T] &lt; T_n) & (df_ZC[col_T] % 1 == 0)  # S√©lection des T entiers &lt; T_n\n\n        sum_B = sum(df_ZC.loc[mask, 'B'].fillna(0))\n        df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, col_tx] * sum_B) / (1 + df_ZC.loc[i, col_tx])\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n    return df_ZC\n\n\ndf_ZC_s = compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx_s\")\ndf_ZC_s=pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC_s], ignore_index=True)\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC_s.loc[1:,'T'], df_ZC_s.loc[1:,'R'], label='Courbe de taux z√©ro coupon shift√©', marker='o')\nplt.plot(df_ZC.loc[1:,\"T\"], df_ZC.loc[1:,\"R\"], label='Courbe de taux z√©ro coupon non shift√©', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux z√©ro coupon discr√©tis√©e')\n\nText(0.5, 1.0, 'Courbe de taux z√©ro coupon discr√©tis√©e')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nEn shiftant le taux de swap 5Y, la courbe de forward baisse brusquement pour T=5Y. Il y a une d√©formation locale de la courbe des taux forward. Cela signifie que la courbe des taux forward est sensible aux variations des taux de swap. En effet, les taux swap sont des instruments financiers qui permettent de se couvrir contre les variations des taux d‚Äôint√©r√™t."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "href": "posts/ensai/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "title": "Mod√®les de courbe de taux",
    "section": "II. Valorisation de swaptions et de caplets",
    "text": "II. Valorisation de swaptions et de caplets\n\nPour coter les caplets/floorlets et swaptions, le mod√®le de Black est souvent utilis√©. Ce mod√®le est bas√© sur l‚Äôhypoth√®se que les taux d‚Äôint√©r√™t sont log-normalement distribu√©s. Il permet de calculer le prix d‚Äôun caplet/floorlet et d‚Äôun swaption en fonction des taux d‚Äôint√©r√™t et de la volatilit√© implicite.\nL‚ÄôEDS (Equation Diff√©rentielle Stochastique) de Black est donn√©e par :\n\n\\[\ndL(t) = \\sigma L(t) dW(t)\n\\]\navec \\(L(t)\\) le taux, \\(\\sigma\\) la volatilit√© du taux et \\(W(t)\\) un mouvement brownien. En utilisant le changement de num√©raire, ce taux est une martingale sous la mesure du numeraire (probabilit√© risque neutre). Cela permet de calculer le prix d‚Äôun caplet/floorlet ou d‚Äôune swaption.\nCaplets et Floorlets\n\nPour un caplet, le prix est donn√© par la formule suivante :\n\n\\[\nCaplet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ L_i(t) \\phi(d) - K \\phi (d - \\sigma_i \\sqrt{T_{i-1}-t} )\\right]\n\\]\navec \\(d = \\frac{1}{\\sigma \\sqrt{T_{i-1}-t}} \\left( \\ln \\left( \\frac{L_i(t)}{K} \\right) + \\frac{\\sigma^2(T_{i-1}-t)}{2} \\right)\\), \\(L_i(t)\\) le taux forward 3M √† la date t, \\(K\\) le strike du caplet, \\(N\\) le nominal, \\(\\delta_i\\) la p√©riode de capitalisation, \\(B(t,T_i)\\) le facteur d‚Äôactualisation √† la maturit√© \\(T_i\\), \\(\\sigma_i\\) la volatilit√© du taux forward 3M √† la maturit√© \\(T_i\\) et \\(\\phi\\) la fonction de r√©partition de la loi normale standard.\nPour un floorlet, le prix est donn√© par la formule suivante :\n\\[\nFloorlet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ K \\phi (d - \\sigma_i \\sqrt(T_{i-1}-t) ) - L_i(t) \\phi(d) \\right]\n\\]\nSwaptions\nPour un swaption donneur, le prix est donn√© par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ F_S(t) \\Phi(d) - K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) \\right]\n\\]\navec \\(F_S(t)\\) le taux swap √† la date t, \\(K\\) le strike du swaption, \\(N\\) le nominal, \\(\\delta\\) la p√©riode de capitalisation, \\(B(t,T_j)\\) le facteur d‚Äôactualisation √† la maturit√© \\(T_j\\), \\(\\sigma_S\\) la volatilit√© du taux swap et \\(\\Phi\\) la fonction de r√©partition de la loi normale standard.\nd est donn√© par la formule suivante :\n\\[\nd = \\frac{1}{\\sigma_S \\sqrt{T_0 - t}} \\left( \\ln \\left( \\frac{F_S(t)}{K} \\right) + \\frac{\\sigma_S^2(T_0 - t)}{2} \\right)\n\\]\nPour un swaption receveur, le prix est donn√© par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) - F_S(t) \\Phi(d) \\right]\n\\]\nIl s‚Äôagit, √† partir des cotations d√©crites dans le tableau ci-dessous et de la courbe des taux z√©ro-coupon construite pr√©c√©demment, calculer les prix de march√© de caplets sur euribor12M, ce qui implique une p√©riode de capitalisation annuelle, de maturit√© T = 5Y, i.e.¬†pay√© √† 6Y, et de strikes K associ√©s au tableau. Nous souhaitons ainsi calculer : - Le prix des caplets Caplet(t, 5Y, 6Y) pour les strikes du tableau ci-dessous. - Le prix des swaptions Swaption(t, 5Y, 6Y) pour les strikes du tableau ci-dessous.\n\nvol_data = pd.read_excel('data/Data_tx.xlsx', sheet_name='vol')\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\n\n\n\n\n0\n-100\n0.311859\n0.311859\n\n\n1\n-50\n0.283274\n0.283274\n\n\n2\n-25\n0.265921\n0.265921\n\n\n3\n0\n0.250000\n0.250000\n\n\n4\n25\n0.243451\n0.243451\n\n\n5\n50\n0.249019\n0.249019\n\n\n6\n100\n0.271828\n0.271828\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet'):\n    \"\"\"\n    Calcule la valeur d'un caplet selon le mod√®le de Black.\n\n    Param√®tres :\n    - N : Notional\n    - delta_i : P√©riode du caplet \n    - B_t_Ti : Facteur d'actualisation B(t, Ti)\n    - L_i_t : Taux forward Li(t)\n    - K : Strike du caplet\n    - sigma_i : Volatilit√© implicite\n    - Ti_1 : Date de d√©but de la p√©riode\n    - Ti : Date de fin de la p√©riode\n    - t : Temps actuel\n\n    Retourne :\n    - Valeur du caplet\n    \"\"\"\n    Ti_1 = Ti - delta_i\n    d1 = (np.log(L_i_t / K) + 0.5 * sigma_i**2 * (Ti_1 - t)) / (sigma_i * np.sqrt(Ti_1 - t))\n    d2 = sigma_i * np.sqrt(Ti_1 - t) - d1\n    if option_type == 'caplet':\n        price = N * delta_i * B_t_Ti * (L_i_t * norm.cdf(d1) - K * norm.cdf(-d2))\n    elif option_type == 'floorlet':\n        price = N * delta_i * B_t_Ti * (K * norm.cdf(d2) - L_i_t * norm.cdf(-d1))\n    return price\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # P√©riode du caplet\n    delta_i = 1 \n\n    # Maturit√© du caplet\n    T=5 \n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == T, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    # Volatilit√© implicite\n    sigma_i = vol_data.loc[i, \"Vols Caplets\"]  \n\n    # Date de d√©but de la p√©riode\n    Ti = T + delta_i\n\n    # Temps actuel\n    t = 0  \n\n    caplet_price = price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet')\n    vol_data.loc[i, 'Caplet Price MKT'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n\n\n\n\n\n\n\n\n# pricer les swaptions"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/pj_courbe_tx.html#iii.-mod√®le-de-hull-white",
    "href": "posts/ensai/proc_stochastique/pj_courbe_tx.html#iii.-mod√®le-de-hull-white",
    "title": "Mod√®les de courbe de taux",
    "section": "III. Mod√®le de Hull-White",
    "text": "III. Mod√®le de Hull-White\n\nIII.1 Du mod√®le HJM vers le mod√®le Hull&White\nPour une maturit√© \\(T\\) fix√©e, Heath, Jarrow et Morton ont suppos√© que le taux forward instantan√© √©volue selon la dynamique suivante :\n\\[\ndf(t, T) = \\alpha(t, T)\\, dt + \\sigma(t, T)\\, dW_t \\quad (1)\n\\]\nLa dynamique (1) ne se place pas forc√©ment dans un cadre sans opportunit√© d‚Äôarbitrage. Les auteurs ont montr√© que le processus \\(\\alpha\\) ne pouvait pas √™tre choisi arbitrairement et que, pour qu‚Äôil existe une unique mesure martingale √©quivalente, \\(\\alpha\\) devait √™tre li√© √† la volatilit√© du z√©ro coupon.\nSupposons donc l‚Äôexistence d‚Äôune unique mesure martingale √©quivalente \\(\\mathbb{Q}\\) (mesure risque-neutre) dont le num√©raire est l‚Äôactif sans risque.\nOn suppose que le prix du z√©ro coupon (payant une unit√© de devise en date \\(T\\)) √©volue sous \\(\\mathbb{Q}\\) selon l‚ÄôEDS :\n\\[\n\\frac{dB(t, T)}{B(t, T)} = r_t\\, dt + \\Gamma(t, T)\\, dW_t^Q \\quad (2)\n\\]\nPar d√©finition, on sait que $ B(t, T) = e^{-_t^T f(t,s), ds}$ et que $ f(t, T) = -_T (B(t, T)). $\nEn appliquant le lemme d‚ÄôIt√¥, on obtient : $ df(t, T) = (t, T), _T (t, T), dt - _T (t, T), dW_t^Q. $\nEn posant $ -_T (t, T) = (t, T), $ nous obtenons :\n\\[\ndf(t, T) = \\gamma(t, T) \\int_t^T \\gamma(t, u)\\, du \\, dt + \\gamma(t, T)\\, dW_t^Q \\quad (3)\n\\]\nApr√®s int√©gration, on retrouve finalement :\n\\[\nf(t, T) = f(0, T) + \\int_0^t \\gamma(s, T) \\left( \\int_s^T \\gamma(s, u)\\, du \\right) ds + \\int_0^t \\gamma(s, T)\\, dW_s^Q \\quad (4)\n\\]\n\n\nIII.2 Hypoth√®ses du mod√®le Hull&White\nOn suppose que le mod√®le HJM est gaussien, lin√©aire et calibrable. Ces hypoth√®ses permettent d‚Äô√©crire :\n\\[\n\\gamma(t, T) = \\sigma(t)\\, e^{-\\lambda (T-t)}  \\quad \\text{ et} \\quad \\Gamma(t, T) = \\frac{\\sigma(t)}{\\lambda}\\Bigl(e^{-\\lambda (T-t)} - 1\\Bigr)\n\\]\no√π la fonction de volatilit√© instantan√©e \\(\\sigma(t)\\) est constante par morceaux.\n\n\nIII.3 Construction de la formule z√©ro-coupon\nDans le cadre du mod√®le Hull&White, la dynamique du taux court instantan√© \\(r_t\\) s‚Äô√©crit :\n\\[\ndr_t = \\left[\\lambda\\bigl(f(0,t) - r_t\\bigr) + \\partial_t f(0,t) + \\int_0^t \\sigma^2(s)\\, e^{-2\\lambda (t-s)} ds \\right] dt + \\sigma(t)\\, dW_t^Q \\quad (5)\n\\]\nOn introduit alors une nouvelle variable d‚Äô√©tat : $ X_t = r_t - f(0,t)$\nLa dynamique de \\(X_t\\) devient :\n\\[\ndX_t = \\left[\\varphi(t) - \\lambda X_t\\right] dt + \\sigma(t)\\, dW_t^Q \\quad (8)\n\\]\navec $ (t) = _0^t ^2(s), e^{-2(t-s)} ds. $\nLa formule du prix du z√©ro coupon s‚Äôexprime alors comme une fonction d√©terministe de \\(X_t\\) :\n\\[\nB(t, T) = \\frac{B(0,T)}{B(0,t)} \\exp\\!\\Biggl\\{ -\\frac{1}{2\\beta^2(t,T)} \\varphi(t) - \\beta(t,T) X_t \\Biggr\\} \\quad (9)\n\\]\no√π $ (t, T) = . $\n\nA quelle cat√©gorie de mod√®le appartient le mod√®le Hull&White? Justifier.\n\nLe mod√®le Hull & White est un mod√®le √† structure √† terme aÔ¨Éne, i.e.¬†un mod√®le de taux d‚Äôint√©r√™t pour lequel le taux z√©ro-coupon continu R(t, T ) est une fonction aÔ¨Éne du taux court r (t).\nIl ressemble √† un processus d‚ÄôOrnstein-Uhlenbeck ou mean reversing process, qui est un processus gaussien d√©finit de la mani√®re suivante :\n\\[\ndY_t = - \\theta \\left[Y_t - \\mu \\right] dt + \\sigma dW_t,\n\\]\no√π \\(\\theta, \\mu, \\sigma\\) sont des param√®tres d√©terministes et \\(W_t\\) est le processus de Wiener.\nDans notre cas, on a \\(\\theta = \\lambda\\), \\(\\mu = \\frac{\\phi(t)}{\\lambda}\\) et \\(\\sigma = \\sigma(t)\\). De ce fait, la moyenne et la variance d√©pend du temps et le param√®tre de vitesse de retour √† la moyenne est constant.\n\nD√©terminer la loi du processus \\(X_t|X_s\\)?\n\nSous la probabilit√© risque neute \\(\\mathbb{Q}\\), le processus \\(X_t\\) s‚Äôecrit :\n\\[\ndX_t = \\left(\\phi(t)- \\lambda X_t \\right) dt + \\sigma(t) dW_t^Q,\n\\]\navec \\(\\phi(t) = \\int_0^t \\sigma^2(s) e^{-2\\lambda(t-s)} ds\\).\nPosons \\(K_t = e^{\\lambda t} X_t  = f(X_t, t)\\implies X_t = e^{-\\lambda t} K_t\\), par la formule d‚ÄôIt√¥, on a :\n\\[\\begin{aligned}\ndf(X_t, t) &= e^{\\lambda t} dX_t + de^{\\lambda t} X_t \\\\\n&= e^{\\lambda t} \\left( \\phi(t) - \\lambda X_t \\right) dt + e^{\\lambda t} \\sigma(t) dW_t^Q + e^{\\lambda t} X_t dt \\\\\n&= e^{\\lambda t}\\phi(t) dt + e^{\\lambda t}\\sigma(t)dW_t^Q \\\\\n&\\implies f(X_t, t) = K_t = e^{- \\lambda t}K_s  + \\int_s^t e^{-\\lambda(t-u)}\\phi(u) du + \\int_s^t e^{-\\lambda(t-u)}\\sigma(u)dW_u^Q \\\\\n&\\Leftrightarrow X_t = X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\phi(u)  du + \\int_s^t e^{-\\lambda (t-u)} \\sigma(u) dW_u^T\n\\end{aligned}\\]\nDe ce fait, on en d√©duit que $X_t|X_s ( X_s e^{-(t-s)} + _s^t e^{-(t-u)} (u) du, _s^t e^{-2 (t-u)} (u)^2 d ) $.\n\n\nIII.4 Dynamique des taux forwards\nOn note ensuite \\(L_i(t)\\) le taux LIBOR forward √† la date \\(t\\) qui fixe en \\(T_i\\) et paie en \\(T_{i+1}\\). Sous l‚Äôhypoth√®se d‚Äôabsence d‚Äôopportunit√© d‚Äôarbitrage, ce taux s‚Äôexprime √† partir de la courbe de taux : √é \\[\nL_i(t) = \\frac{1}{\\delta_i}\\left(\\frac{B(t, T_i)}{B(t, T_{i+1})} - 1\\right)  = \\frac{1}{\\delta_i}\\left(Z_t- 1\\right) ,\n\\]\nPour connaitre la dynamique des taux forwards, on applique le lemme d‚ÄôIt√¥ au processus :\n\\[\nZ_t = \\frac{B(t, T_i)}{B(t, T_{i+1})}.\n\\]\n\nRappel du lemme d‚ÄôIt√¥ : Consid√©rons deux actifs \\(X\\) et \\(Y\\) et posons \\(Z = \\frac{X}{Y}\\) (la valeur de \\(X\\) exprim√©e en num√©raire \\(Y\\)). Le lemme d‚ÄôIt√¥ nous donne l‚Äô√©volution de \\(Z\\) par :\n\\[\n\\frac{dZ}{Z} = \\left(\\frac{dX}{X} - \\frac{dY}{Y}\\right) - \\left\\langle \\frac{dX}{X} - \\frac{dY}{Y},\\, \\frac{dY}{Y}\\right\\rangle.\n\\]\n\nEn appliquant le lemme d‚ÄôIt√¥ √† \\(Z_t\\), on obtient :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= \\frac{dB(t, T_i)}{B(t, T_i)} - \\frac{dB(t, T_{i+1})}{B(t, T_{i+1})} \\\\\n&- \\frac{1}{\\cancel{B(t,T_{i+1})^2}} \\cancel{B(t,T_{i+1})^2} \\, \\Gamma(t,T_i,T_{i+1})^2 \\, dt \\\\\n&-  \\frac{-1}{\\cancel{B(t,T_i)B(t,T_{i+1})}} \\cancel{B(t,T_i)B(t,T_{i+1})}\\,\\Gamma(t,T_i)\\Gamma(t,T_{i+1})\\,dt\\\\\n&= \\Gamma(t,T_i,T_{i+1})\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i)\\right)dt + \\left(\\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\right)dW_t^Q\\\\\n&\\implies \\frac{dZ_t}{Z_t} = \\mu(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\n\\end{aligned}\\]\navec\n\\[\\begin{cases}\n\\sigma(t,T_i,T_{i+1}) = \\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\\\\n\\mu(t,T_i,T_{i+1}) = \\Gamma(t,T_i)\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i) \\right) = - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\n\\end{cases}\\]\nDe ce fait, on a :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\\\\\n&= \\sigma(t,T_i,T_{i+1}) \\underbrace{\\left( - \\Gamma(t,T_i)\\,dt + dW_t^Q \\right)}_{d\\tilde{W_t}}\\\\\n&= \\sigma(t,T_i,T_{i+1}) d\\tilde{W_t}\n\\end{aligned}\\]\no√π \\(d\\tilde{W_t}\\) est un mouvement brownien selon le th√©or√®me de Girsanov.\nLa diffusion de \\(Z_t\\) est une loi log-normale, sans drift sous la probabilit√© risque forward. De ce fait, il suit le mod√®le de Black pour la valorisation des options.\nOn peut √©crire ainsi la dynamique du taux forward \\(L_i(t)\\) sous la probabilit√© risque forward :\n\\[\\begin{aligned}\ndL_i(t) &= \\frac{Z_t}{\\delta_i} \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}  \\\\\n&= (L_i(t) + \\frac{1}{\\delta_i}) \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}\n\\end{aligned}\\]\n\n\nIII.5 Valorisation des instruments de calibration\n\nPayoff d‚Äôun caplet vanille :\nLe payoff d‚Äôun caplet sur le taux LIBOR \\(L_i(T_i)\\), de maturit√© \\(T_i\\), avec paiement en \\(T_{i+1}\\) et de strike \\(K\\) est donn√© par :\n\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr).\n\\]\n\nFormule de valorisation dans le cadre du mod√®le H&W :\nIl peut √™tre d√©montr√© que la formule de valorisation de ce caplet s‚Äôexprime de la mani√®re suivante :\n\n\\[\nC\\Bigl( Z_t,\\, \\tilde{K},\\, T_i,\\, \\sigma_i^*,\\, B(t, T_{i+1}) \\Bigr) \\quad (11)\n\\]\navec :\n\n$ Z_t = $,\n$ (_i^*)^2 = t^{T_i} ( (s, T_i) - (s, T{i+1}) )^2 ds = ^2(T_i, T_{i+1}), (T_i) $,\n$ = 1 + _i K $,\n$ C() $ d√©signe le prix d‚Äôun Call selon le cadre Black, en fonction du forward, du strike, de la maturit√©, de la volatilit√© et du facteur d‚Äôactualisation.\n\nEn effet, on peut r√©√©crire le payoff d‚Äôun caplet sous la forme d‚Äôun Call sur \\(Z_t\\) qui suit un mod√®le de Black :\n\\[\\begin{aligned}\n\\text{Payoff} &= \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t}{\\delta_i} - 1 - K; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t - 1 - \\delta_i K}{\\delta_i}; 0 \\Bigr) \\\\\n&= \\max\\Bigl(Z_t - 1 - \\delta_i K; 0 \\Bigr) \\\\\n\\text{Payoff} &= \\max\\Bigl( Z_t - \\tilde{K},\\; 0 \\Bigr) \\\\\n\\end{aligned}\\]\n\n\nIII.6. Calibration du mod√®le\nLe mod√®le de Hull White permet d‚Äôavoir une formule ferm√©e pour le prix des caplets. De fait, puisqu‚Äôon a calcul√© les prix de march√© de caplets sur euribor12M, ce qui implique une p√©riode de capitalisation annuelle, de maturit√© T = 5Y, nous pouvons desormais calibrer le param√®tre de volatilit√© \\(\\sigma_i^*\\) avec la m√©thode de dichotomie et aussi extraire de mani√®re analytique la volatilit√© instantan√©e \\(\\sigma(t)\\) du mod√®le Hull&White, qu‚Äôon supposera constante, i.e.¬†\\(\\sigma(t) = \\sigma\\).\nOn pose √©galement, pour la calibration, \\(\\lambda = 5\\%\\).\nPour extraire la volatilit√© spot, nous utiliserons uniquement le prix de marche ATM.\n\ncaplet_price_MKT = vol_data.loc[3, 'Caplet Price MKT']\nprint(f\"Le prix de march√© caplet sur euribor 12M de maturit√© T=5Y est de {caplet_price_MKT:.4%}\")\n\nLe prix de march√© caplet sur euribor 12M de maturit√© T=5Y est de 0.7137%\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW'):\n    if model == \"HW\" :\n        Z_t = delta_i * L_i_t + 1\n        K = 1 + delta_i * K\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=Z_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n    else:\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=L_i_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n\n\ndef Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT,option_type='caplet', model=\"HW\",tol=1e-6, sigma_low=1/10000, sigma_high=1):\n    \"\"\"\n    Extrait la volatilit√© implicite sigma en utilisant la m√©thode de dichotomie.\n    \"\"\"\n    fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n    fmax = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_high,option_type=option_type, model=model)\n    price = caplet_price_MKT\n    if fmin&gt;price :\n        return sigma_low\n    elif fmax&lt;price :\n        return sigma_high\n    else:\n        while sigma_high-sigma_low&gt;tol:\n            sigma_mid = (sigma_low + sigma_high) / 2\n            fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n            fmid = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_mid,option_type=option_type, model=model)\n            if ((fmin - price) * (fmid - price) &gt; 0) : # jette la moiti√© de gauche\n                sigma_low = sigma_mid\n            else: # jette la moiti√© de droite\n                sigma_high = sigma_mid\n        sigma_mid = (sigma_low + sigma_high) / 2\n        return sigma_mid\n    \n\n# Notional\nN = 1  \n\n# P√©riode du caplet\ndelta_i = 1 \n\n# Maturit√© du caplet\nTi=6\n\n# Facteur d'actualisation B(t, Ti)\nB_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n# Taux forward Li(t)\nL_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n# Strike du caplet\nK = L_i_t + vol_data.loc[3, 'Strike en bps et en rel. / fwd']/10000\n\nlambda_ = 5/100\n\nsigma_i = Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT, option_type='caplet',model=\"HW\")\nprint(\"Volatilit√© implicite (en %) :\", sigma_i*100) \n\nVolatilit√© implicite (en %) : 0.9271045541763303\n\n\n\nbeta_Ti_Ti_1 =  ((1 - np.exp(- lambda_ * delta_i))/lambda_)**2\nTi_1 = Ti - delta_i\nphi = (1 - np.exp(-2*lambda_*(Ti_1-t)))/(2*lambda_)\nsigma = np.sqrt((sigma_i**2 * Ti_1)/ (beta_Ti_Ti_1 * phi))\n\nprint(\"Volatilit√© instantan√©e (en %) :\", sigma*100) \n\nVolatilit√© instantan√©e (en %) : 1.071446257025021\n\n\n\nCette volatilit√© spot nous permet de valoriiser les caplets pour des strikes diff√©rents de l‚ÄôATM √† l‚Äôaide de la formule de valorisation ferm√©e du mod√®le Hull et White.\n\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # P√©riode du caplet\n    delta_i = 1 \n\n    # Maturit√© du caplet\n    Ti=6\n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    sigma_i = sigma_i\n\n    lambda_ = 5/100\n\n    caplet_price = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW')\n    vol_data.loc[i, 'Caplet Price HW'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n\n\n\n\n\n\n\nL‚Äôune des faiblesses du mod√®le de Hull et White est le fait qu‚Äôil n‚Äôarrive pas √† capter le smile de volatilit√©. En effet, la volatilit√© implicite extraite est un skew. Pour constater ce ph√©nom√®ne, nous inverserons la formule de Black pour les caplets et nous en d√©duirons la volatilit√© implicite pour chaque strike. Nous utiliserons toujours la m√©thode de dichotomie pour trouver la volatilit√© implicite.\n\nfor i in vol_data.index:\n    N = 1\n    delta_i = 1\n    Ti = 6\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n    L_i_t = (1 / delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0] / df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd'] / 10000\n    lambda_ = 5 / 100\n    caplet_price_MKT = vol_data.loc[i, 'Caplet Price HW']\n\n    sigma_extracted =  Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, caplet_price_MKT, option_type='caplet', model='Black')\n    vol_data.loc[i, 'Sigma_HW'] = sigma_extracted\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\nSigma_HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n0.288669\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n0.267386\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n0.258287\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n0.250009\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n0.242433\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n0.235466\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n0.223060\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Sigma_HW\"], label='Volatilit√© extraite', marker='o')\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Vols Caplets\"], label='Volatilit√© de march√©', marker='o')\nplt.legend()\nplt.title('Comparaison de la volatilit√© extraite par le mod√®le HW et la volatilit√© de march√©')\nplt.xlabel('Strike en bps et en rel. / fwd ')\nplt.ylabel('Volatilit√©')\n\nText(0, 0.5, 'Volatilit√©')\n\n\n\n\n\n\n\n\n\n\nATM  = L_i_t\n\n\n\n3.7 Valorisation d‚Äôun produit structur√©\n\nRemarques pr√©liminaires :\n\nNous garderons dans un premier temps la calibration ATM effectu√©e avec \\(\\lambda = 5\\%\\).\nPour la partie Monte-Carlo, nous admettrons que l‚ÄôEDS pour le processus \\(X_t\\) sous la probabilit√© forward neutre \\(Q^T\\) associ√©e au num√©raire \\(B(t,T)\\) s‚Äô√©crit comme :\n\n\\[\ndX_t = \\left[\\phi(t) + \\sigma(t)\\Gamma(t,T) - \\lambda X_t\\right] dt + \\sigma(t) dW_t^T \\tag{12}\n\\]\nO√π \\(W_t^T\\) est un brownien sous \\(Q^T\\).\n\nNous souhaitons valoriser un caplet de strike \\(K\\), de dates de fixing \\(T_i = 5Y\\) et de paiement \\(T_{i+1} = 6Y\\) et de barri√®re d√©sactivante \\(B\\) (avec \\(B &gt; K\\)).\n\n√âcrire le payoff de l‚Äôoption et tracer la fonction de payoff en fonction de \\(L_i(T_i)\\). Cette option est-elle plus ou moins ch√®re qu‚Äôun simple caplet de strike \\(K\\) ?\n\nLe payoff de l‚Äôoption est donn√© par :\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &lt; B}\n\\]\n\n\n\nimage-2.png\n\n\nIl est possible de d√©composer le payoff √† partir d‚Äôoptions vanilles et digitales :\n\\[\n\\text{Payoff} = C_K - CB - (B-K) \\times D_B\n\\]\n\nUne option digitale est une option qui paie 1 si le sous-jacent est au-dessus d‚Äôun certain seuil et 0 sinon. De ce fait, le payoff de l‚Äôoption est donn√© par :\n\\[\nD_B = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &gt; K}\n\\]\n\n\n\nimage.png\n\n\nUne option vanille est un contrat financier standardis√© qui donne le droit, mais non l‚Äôobligation, d‚Äôacheter (call) ou de vendre (put) un actif sous-jacent √† un prix fix√© (strike) √† une date donn√©e (maturit√©). De ce fait, dans le cas d‚Äôun call, le payoff de l‚Äôoption est donn√© par :\n\\[\nC_K = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr)\n\\]\n\n\n\nCapture d‚ÄôeÃÅcran 2025-03-05 aÃÄ 00.17.09.png\n\n\n\n\nRappeler les principes du pricing par m√©thode de Monte-Carlo.\n\nLa m√©thode de Monte-Carlo est une m√©thode num√©rique qui permet de pricer des produits financiers complexes lorsque les formules ferm√©es ne sont pas disponibles. Elle consiste √† simuler un grand nombre \\(N\\) de trajectoires du processus stochastique et √† calculer la moyenne empiriques des payoffs actualis√©s pour obtenir le prix de l‚Äôoption.\n\nRappeler comment on simule une loi gaussienne √† partir d‚Äôune loi uniforme.\n\nPour simuler une variable al√©atoire suivant une loi gaussienne standard \\(\\mathcal{N}(0,1)\\) √† partir d‚Äôune variable uniforme \\(U\\) sur \\([0,1]\\), on applique l‚Äôinverse de la fonction de r√©partition de la loi gaussienne standard (aussi appel√©e la fonction quantile de la loi normale) :\n\\[\nX = F^{-1}(U)\n\\]\no√π \\(F\\) est la fonction de r√©partition de la loi normale standard.\n\nOn consid√®re un caplet sur euribor12M √† barri√®re d√©sactivante de strike $ K = ATM - 100 bps$, de barri√®re \\(B = ATM + 100 bps\\) et de maturit√© \\(T_i = 5Y\\). Pour valoriser cette option, nous allons utiliser une m√©thode num√©rique de type Monte-Carlo. Pour cela, il est necessaire de connaire la loi de X_t sachant X_s. En nous aidant de la question pr√©c√©dente, on peut d√©duire que la loi de \\(X_t|X_s\\) est une loi normale de param√®tres :\n\n\\[\nX_t|X_s \\sim \\mathcal{N}\\left( X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\left( \\phi(u) - \\sigma \\Gamma(u,T) \\right) du, \\quad \\int_s^t e^{-2 \\lambda (t-u)} \\sigma^2 d \\right)\n\\]\nPour valoriser cette option, nous pouvons directement utiliser la loi de \\(X_5|X_0\\) pour simuler les trajectoires du taux court et calculer le payoff de l‚Äôoption ou diffuser progressivement le taux court en utilisant la loi de \\(X_t|X_s\\) pour chaque pas de temps. Ensuite, il s‚Äôagira de calculer le payoff de l‚Äôoption √† chaque date, en faire la moyenne et l‚Äôactualiser pour obtenir le prix de l‚Äôoption.\n\nM√©thode 1 : Simulation de la loi de \\(X_5|X_0\\)\n\nimport numpy as np\nfrom scipy.integrate import quad\n\n# Fonction phi(t) - variance cumul√©e\ndef phi(t, sigma, lambda_):\n    return (sigma**2 / (2 * lambda_)) * (1 - np.exp(-2 * lambda_ * t))\n\n# Fonction beta(t, T)\ndef beta(t, T, lambda_):\n    return (1 - np.exp(-lambda_ * (T - t))) / lambda_\n\n# Fonction gamma(t, T)\ndef gamma(t, T, sigma, lambda_):\n    return (sigma / lambda_) * (np.exp(-lambda_ * (T - t)) - 1)\n\n# Fonction B(t, T)\ndef B_t_T(t, T, B0_T, B0_t, X_t, sigma, lambda_):\n    beta_t_T = beta(t, T, lambda_)**2\n    phi_t = phi(t, sigma, lambda_)\n    exponent = -0.5 * beta_t_T * phi_t - beta_t_T * X_t\n    return (B0_T / B0_t) * np.exp(exponent)\n\n# Fonction d'int√©gration avec param√®tres suppl√©mentaires\ndef integrand_mean(u, t, Xs, s, sigma, lambda_, T):\n    # t = borne sup\n    # s = borne inf\n    # T = maturit√©\n    Xt = Xs * np.exp(-lambda_ * (t - s))\n    exp_part = np.exp(-lambda_ * (t - u))\n    return Xt + exp_part * (phi(u, sigma, lambda_) + sigma * gamma(u, T, sigma, lambda_))\n\n# Param√®tres\nT = Ti_1  = 5\nXs = X0 = 0  \ns = 0\nt = Ti_1 \n\n# Calcul de la moyenne conditionnelle\nmean_5_given_0, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\nprint(f\"Moyenne conditionnelle de X_5 | X_0 : {mean_5_given_0:.6f}\")\n\n# Calcul de la variance conditionnelle (ind√©pendant de gamma ici)\n\ndef compute_variance(sigma, lambda_, t, s):\n    return (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nvar_5_given_0 = compute_variance(sigma, lambda_, t, s)\n# var_5_given_0 = (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nprint(f\"Variance conditionnelle de X_5 | X_0 : {var_5_given_0:.6f}\")\n\nMoyenne conditionnelle de X_5 | X_0 : 0.000000\nVariance conditionnelle de X_5 | X_0 : 0.000452\n\n\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003007\n\n\n\n\nM√©thode 2 : Methode de diffusion\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[i-1]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.002987\n\n\nNous constatons qu‚Äôavec les deux m√©thodes, nous obtenons des prix d‚Äôoptions similaires (diff√©rent de 0.2bps). Cela confirme que les deux m√©thodes convergent vers le m√™me r√©sultat.\nEn d√©g√©n√©rant le produit en faisant tendre la barri√®re √† \\(+\\infty\\), nous constatons que le prix de l‚Äôoption call est √©gal au prix de march√© du forward. En d√©g√©n√©rant le produit en faisant tendre la barri√®re √† 0, nous constatons que le prix de l‚Äôoption call est √©gal √† 0.\nCela est coh√©rent car lorsque la barri√®re est tr√®s √©lev√©e, le produit est √©quivalent √† un forward et lorsque la barri√®re est nulle, le produit est √©quivalent √† un call classique. La fonction que nous avons impl√©ment√© est donc coh√©rente et bien impl√©ment√©e.\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM # - bp\n\n    # Barri√®re\n    B = np.inf#ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.007089\n\n\nNous rendons la barri√®re ‚Äòbermud√©enne‚Äô en √©tendant la condition de d√©sactivation aux dates 1Y, 2Y, 3Y, 4Y et 5Y. De ce fait, le payoff de cette option s‚Äô√©crit :\n\\[\n\\text{Payoff} = \\delta_i \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{max_{i=1,\\dots,5}(L_i(T_i) &lt; B)}\n\\]\n\n# Option bermudienne\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n    L_i_t = np.zeros(Ti_1)\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[s]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n        # Calcul du prix B(5,6) selon Hull-White\n        Bi_t = df_ZC.loc[df_ZC['T'] == t, 'B'].values[0] # B(0,6)\n        Bi_s = df_ZC.loc[df_ZC['T'] == s, 'B'].values[0] # B(0,5)\n\n        B_s_t = B_t_T(s, t, Bi_t, Bi_s, X[i], sigma, lambda_)\n        B_s_s = B_t_T(s, s, Bi_s, Bi_s, X[i], sigma, lambda_)\n\n        # Calcul du taux forward L_i_t\n        L_i_t[i-1] = (1 / (t-s)) * ((B_s_s / B_s_t) - 1)\n    \n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_t - strike, 0) * (np.max(L_i_t) &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003046\n\n\nL‚Äôun des param√®tres important du mod√®le de Hull et White est la mean reversion \\(\\lambda\\), qui caract√©rise la force de rappel √† la moyenne du processus. Ce param√®tre a un impact positive sur la valorisation de l‚Äôoption, comme nous pouvons l‚Äôobserver dans la figure ci dessous .\n\n# Liste des lambda √† tester\nlambdas = np.linspace(0.01, 1, 15)  # Exemple de grille de lambda\nresults = []\n\nTi_1 = 5\nTi = 6\n# Boucle principale sur les lambdas\nfor lambda_ in lambdas:\n    n_touched = 0\n    payoffs = np.zeros(n_simulations)\n\n    for sim in range(n_simulations):\n        phi_ = phi(Ti_1, sigma, lambda_)\n        gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n        # Moyenne\n        mu_X = mean_5_given_0\n\n        # Ecart-type\n        sigma_X = np.sqrt(var_5_given_0)\n\n        # X_5|X_0\n        X = np.random.normal(mu_X, sigma_X)  \n\n        # Calcul du prix B(5,6) selon Hull-White\n        B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n        B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n        B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n        B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n        L_i_t = ((B_5_5 / B_5_6) - 1)\n        bp = 100 / 10000\n        strike = ATM - bp\n        B = ATM + bp\n\n        payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n        payoffs[sim] = payoff\n\n        # V√©rification de la barri√®re\n        if np.any(L_i_t&gt;= B):\n            n_touched += 1\n\n    prob_toucher_barriere = n_touched / n_simulations\n\n    call_price = B0_6 * np.mean(payoffs)\n    results.append((lambda_, call_price,prob_toucher_barriere))\n\n\n# Optionnel : Graphique de la sensibilit√©\nimport matplotlib.pyplot as plt\n\nlambdas, prices, probabilities = zip(*results)\nplt.plot(lambdas, prices, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Prix de l\\'option')\nplt.title('Sensibilit√© du prix √† la mean reversion (Œª)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nDe plus, plus le param√®tre de mean reversion \\(\\lambda\\) est √©lev√©, plus la probabilit√© de toucher la barri√®re est faible. Inversement, une faible mean reversion laisse plus de libert√© au processus pour explorer des valeurs extr√™mes, augmentant ainsi la probabilit√© de franchir la barri√®re.\n\nplt.plot(lambdas, probabilities, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Probabilit√© de toucher la barri√®re')\nplt.title('Probabilit√© de toucher la barri√®re en fonction de la mean reversion (Œª)')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html",
    "href": "posts/ensai/proc_stochastique/APF_filter.html",
    "title": "APF filter",
    "section": "",
    "text": "Le filtre particulaire APF (Auxiliary Particle Filter) est un filtre particulaire qui utilise des particules auxiliaires pour estimer la densit√© de probabilit√© de l‚Äô√©tat cach√©. Il est utilis√© pour estimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. Dans cet article, nous allons √©tudier la performance du filtre APF en utilisant un exemple simple.\nComme tout filtre particulaire, il est necessaire de sp√©cifier la distribution a priori de l‚Äô√©tat, i.e.¬†\\(p(x_0)\\), la distribution de transition, i.e.¬†\\(p(v_t|v_{t-1})\\) et la vraisemblance, i.e.¬†\\(p(y_t|v_t)\\).\nDans le mod√®le de Heston sp√©cifi√© en (1), on consid√®re que : \\[\np(v_t|v_{t-1})=2c\\chi^2(2cx_k; 2q + 2; 2ce^{-\\kappa \\Delta} x_{k‚àí1}),\n\\]\n\\[p(v_1) = \\Gamma(v_1; a,b)\\] o√π \\(a = \\frac{2 \\kappa \\theta}{\\sigma^2}\\) et \\(b = \\frac{2 \\kappa}{\\sigma^2}\\),\net \\(p(y_t)|v_t) = N(0,h)\\).\n\\[\n\\begin{cases}\ndS_t = S_t \\left( rds + \\sqrt{v_t} dW_t^1 \\right) \\\\\ndv_t = \\kappa (\\theta - v_t) ds + \\sigma \\sqrt{v_t} dW_t^2 \\\\\ndW_t^1 dW_t^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\nPour tester la pertinence de l‚ÄôAPF, nous allons utiliser les param√®tres suivants \\(\\Phi = (\\theta = 0.03, \\kappa = 4, \\sigma = 0.4, \\kappa = -0.87, \\rho = 0.5)\\). Pour passer en temps discret et assurer la positivit√© de la volatilit√©, nous utilisosn le schema d‚Äôeuler (√† \\(|v_t|\\)) suivant :\n\\[\n\\begin{cases}\ny_t = C(t, \\theta, v_t, S_t, K, \\tau) + \\varepsilon_t, \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\theta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}\\Delta} (\\rho w_t^1 + \\sqrt{1-\\rho^2}w_t^2) \\right|, \\\\[10pt]\nS_t = S_{t-1} \\left(1 + \\mu \\Delta + \\sqrt{\\Delta v_t} w_t^1 \\right), \\\\[10pt]\n\\end{cases}\n\\]\no√π \\(\\varepsilon_t \\sim N(0,h=0.01)\\), et \\(w_t^1, w_t^2\\) sont des variables al√©atoires gaussiennes et ind√©pendantes.\n\nNous avons utilis√© un sch√©ma d‚Äôeuler modifi√© pour garantir la positivit√© de la volatilit√©. En effet, la volatilit√© doit √™tre positive dans le mod√®le de Heston, et le sch√©ma d‚Äôeuler standard peut produire des valeurs n√©gatives. En prenant la valeur absolue de la volatilit√© √† chaque √©tape, nous nous assurons que les valeurs restent positives. Il aurait √©t√© √©galement possible d‚Äôutiliser le sch√©ma d‚Äôeuler √† \\(ln(v_t)\\) (via le lemme de ito) pour garantir la positivit√© de la volatilit√©.\n\n\nrm(list=ls())\n\n################ Simulation de la trajectoire de St et vt ################ \nHeston_sim &lt;- function(N, kappa, theta, sigma, rho, v0, mu, tau, S0){\n  # N: Number of time steps\n  dt &lt;- tau / N  # Time step\n\n  # Store stock prices and volatilities\n  S &lt;- numeric(N+1)\n  v &lt;- numeric(N+1)\n  \n  S[1] &lt;- S0\n  v[1] &lt;- v0\n  \n  for (t in 1:N){\n    # Generate correlated Brownian motions\n    W1 &lt;- rnorm(1)\n    W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n    \n    # Euler discretization of variance process (ensure non-negativity)\n    v[t+1] &lt;- abs(v[t] + kappa * (theta - v[t]) * dt + sigma * sqrt(v[t] * dt) * W2)\n    \n    # Euler discretization of the stock price process \n    S[t+1] &lt;- S[t] * (1+ mu*dt + sqrt(v[t+1] * dt) * W1)\n  }\n  \n  return(list(v_t = v, S_t=S))\n}\n\nAvec les √©tapes 1 et 2, nous obtenons les trajectoires de volatilit√© instantan√©e et de prix d‚Äôaction suivantes :\n\nset.seed(123)\nN &lt;- 300\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nrho &lt;- 0.5\nv0 &lt;- 0.03\nmu &lt;- 0.1\ntau&lt;-1\nS0 &lt;- 100\n\nres &lt;- Heston_sim(N=N, kappa=kappa, theta=theta, sigma=sigma, rho=rho, v0=v0, mu=mu, tau=tau, S0=S0)\n\npar(mfrow=c(1,2))\nplot(res$v_t, type=\"l\", main=\"Processus de volatilit√© simul√©\", xlab = \"Time step\", ylab = \"Volatilit√©\") \nplot(res$S_t, type=\"l\", main=\"Processus de prix du sous-jacent simul√©\", xlab = \"Time step\", ylab = \"Prix\") \n\n\n\n\n\n\n\n\n\n\nLa proc√©dure de simulation est la suivante :\n\nTout d‚Äôabord, une trajectoire de 300 pas de temps de la variance instantan√©e sera simul√©e pour un pas de 1 jour, en commen√ßant par \\(v_0\\) = 0,03.\nConditionnellement √† cette trajectoire, une trajectoire correspondante du prix de l‚Äôaction sera alors g√©n√©r√©e.\nNous calculons, √† l‚Äôaide du mod√®le de Heston, les prix d‚Äôoptions bruit√©s pour trois types d‚Äôoptions diff√©rents : une option √† la monnaie (ATM), une option dans la monnaie (ITM) et une option mixte (50 % ITM / 50 % OTM).\n\nNous avons choisi trois types d‚Äôoptions afin d‚Äôobserver comment le filtre APF se comporte dans diff√©rentes conditions de march√© :\n\nOption √† la monnaie (ATM) :\n\nPrix d‚Äôexercice : K = 1 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ATM sont souvent utilis√©es pour l‚Äôestimation de la volatilit√© implicite, car elles sont les plus liquides et pr√©sentent un delta proche de 0.5, ce qui les rend sensibles aux variations du sous-jacent.\n\nOption dans la monnaie (ITM) :\n\nPrix d‚Äôexercice : K = 0.95 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ITM ont une valeur intrins√®que √©lev√©e et une volatilit√© implicite plus stable. Elles sont moins sensibles aux fluctuations imm√©diates du march√© mais permettent d‚Äô√©valuer l‚Äôimpact du filtre APF dans des conditions de faible variance du prix d‚Äôoption.\n\nOption mixte (50 % ITM / 50 % OTM) :\n\nPrix d‚Äôexercice : K = 117\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Cette approche permet de tester le filtre APF dans un sc√©nario r√©aliste de portefeuille d‚Äôoptions o√π des positions ITM et OTM sont combin√©es. L‚Äôobjectif est d‚Äôanalyser si le filtre reste stable lorsque l‚Äôon m√©lange des options avec des sensibilit√©s diff√©rentes aux mouvements du sous-jacent et aux variations de volatilit√©.\n\n\nPourquoi tester diff√©rentes configurations d‚Äôoptions ?\nL‚Äôobjectif de cette analyse est de v√©rifier comment le filtre APF se comporte en pr√©sence de conditions de march√© vari√©es :\n\nOptions ATM : impact fort de la volatilit√©, mais moins sujettes au risque de gamma.\nOptions ITM : faible sensibilit√© √† la volatilit√© implicite, mais risque de couverture plus limit√©.\nOptions mixtes : √©valuation de la robustesse du filtre lorsque plusieurs types d‚Äôoptions coexistent dans un m√™me portefeuille.\n\nCes tests permettent de comparer la pr√©cision du filtre en fonction de la position de l‚Äôoption par rapport au prix du sous-jacent.\n\n################ Simulation du prix des options ATM ################ \nsource(\"data/Heston_Call_Function.R\")\nset.seed(123)\n\nv_t &lt;- res$v_t \nS_t &lt;- res$S_t\nh &lt;- 0.01\n\n# Prix d'option K=1*S, tau = 0.5 =&gt; A la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 1\nATM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nATM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\n\nres$ATM &lt;- ATM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options ITM ################ \n# Prix d'option K=0.95*S, tau = 0.5 =&gt; Hors de la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 0.95\n\nITM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nITM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\nres$ITM &lt;- ITM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options mixte ################ \nsummary(S_t)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  99.49  104.36  117.23  119.24  128.88  156.89 \n\n# Prix d'option K=117, tau = 0.5  =&gt; moiti√© ITM et moiti√© OTM\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 117\n\nMIXTE &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nMIXTE[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K)\n}\nres$MIXTE &lt;- MIXTE + rnorm(1,mean=0,sd=sqrt(h))\n\n\n\nEn simulant le prix de ces options, nous constatons sans surprise que les options mixte ont un prix plus √©lev√© plus on se rapproche de la maturit√©. Cependant, les options ITM et ATM ont des prix plus stables, avec une l√©g√®re augmentation pour les options ITM.\n\nN &lt;- length(res$ITM)  \ndt &lt;- tau / N\ntime_axis &lt;- seq(0, tau, length.out = N)  # Axe des temps, de 0 √† tau\n\nplot(time_axis,res$ITM, type=\"l\", main = \"Evolution du prix des options \", ylim = c(0,50), col = \"red\", ylab = \"Prix\", xlab = \"Temps\")\nlines(time_axis,res$ATM, type = \"l\",col=\"blue\")\nlines(time_axis,res$MIXTE, type='l', col= \"darkgreen\")\n# legend\nlegend(\"topleft\", legend=c(\"ITM\", \"ATM\", \"Mixte\"), col=c(\"red\", \"blue\",'darkgreen'), lty=1:1, cex=0.8, title=\"Types d'option\")\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôobjectif est de comparer les r√©sultats de l‚ÄôAPF sur du filtre particulaire bootstrap en termes d‚Äôerreurs d‚Äôajustement, mesur√©es par les erreurs quadratiques moyennes (RMSE) de l‚Äôajustement de la variance et de l‚Äôajustement des prix des options, dans diff√©rents cas de donn√©es.\nFiltre bootstrap :\nLe filtre boostrap fonctionne de la mani√®re suivante :\n\n\n\nBootstrap filter\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre bootstrap de la mani√®re suivante :\n\nBootstrapParticleFilter &lt;- function(y, S, v, K, tau = 0.5, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, rho = 0.5, r = 0.05, dt = 1, h = 0.01) {\n  \n  # Param√®tres suppl√©mentaires\n  sigma_epsilon &lt;- sqrt(h)\n\n  # Param√®tres de la loi stationnaire de v\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation\n  n &lt;- length(y)\n  v_hat &lt;- numeric(n)\n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire bootstrap\n  for (t in 1:n) {\n    if (t == 1) {\n      # Initialisation des particules √† t = 0\n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      \n      # Poids initiaux bas√©s sur la densit√© de la loi stationnaire\n      w[t, ] &lt;- dgamma(v_particle[t, ], shape = alpha1, rate = 1/alpha2)\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # Estimation initiale\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    } else {\n      # √âtape de pr√©diction (propagation des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                    mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                    sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n      \n      #  Calcul du prix du Call pour chaque particule\n      C &lt;- numeric(M)\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # Calcul pour chaque particule avec le bon strike\n      for (i in 1:M) {\n          C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                       v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n\n      \n      # Mise √† jour des poids (vraisemblance observation-conditionnelle)\n      w[t, ] &lt;- dnorm(y[t], mean = C, sd = sqrt(sigma_epsilon))\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # R√©√©chantillonnage\n      index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n      v_particle[t, ] &lt;- v_particle[t, index]\n      \n      # Poids uniformes apr√®s resampling\n      w_normalized[t, ] &lt;- rep(1 / M, M)\n\n      # Estimation de la volatilit√© instantan√©e\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  return(v_hat)\n}\n\nFiltre APF :\nLe filtre particulaire auxilaire fonctionne de la mani√®re suivante :\n\n\n\nAPF\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre APF de la mani√®re suivante :\n\nAPF_Heston &lt;- function(y, S, v,K, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, \n                       lambda = -0.87, rho = 0.5, r = 0.05, tau = 0.5, dt = 1, h = 0.01) {\n  \n  # Param√®tres de la loi stationnaire de v\n  sigma_epsilon &lt;- sqrt(h)\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation des param√®tres\n  n &lt;- length(y)\n  \n  # Initialisation des vecteurs/matrices\n  v_hat &lt;- numeric(n)                   \n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  mu &lt;- matrix(nrow = n, ncol = M)        \n  py_mu &lt;- matrix(nrow = n, ncol = M) \n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire APF\n  for (t in 1:n) {\n    if (t == 1) {\n      \n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      w[t, ] &lt;- rep(1 / M, M)\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n      \n    } else {\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # 1 - Pr√©-s√©lection\n      mu[t, ] &lt;- abs(v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt)\n      \n      mean_pymu &lt;- numeric(M)\n      for (i in 1:M) {\n        mean_pymu[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                             v0 = mu[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      py_mu[t, ] &lt;- dnorm(y[t], mean = mean_pymu, sd = sigma_epsilon)\n\n      # Poids pour la pr√©-s√©lection\n      w[t-1, ] &lt;- py_mu[t, ] * w_normalized[t-1, ]\n      \n      # 2 - Resampling (√©chantillonnage)\n      index &lt;- sample(1:M, size = M, replace = TRUE, prob = w[t-1, ])\n\n      # Mise √† jour des particules\n      v_particle[t-1, ] &lt;- v_particle[t-1, index]\n      \n      # 3 - Propagation (√©volution des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                   mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                   sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n\n      # Mise √† jour des poids\n      C &lt;- numeric(M)\n      for (i in 1:M) {\n        C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                     v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      likelihood &lt;- dnorm(y[t], mean = C, sd = sigma_epsilon)\n      w[t, ] &lt;- likelihood / (py_mu[t, index] + 1e-12)  # Protection pour √©viter la division par z√©ro\n      \n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  # Retourner les r√©sultats\n  return(v_hat)\n}\n\nDiff√©rence entre le filtre bootstrap et le filtre APF :\nLe filtre bootstrap et le filtre APF sont deux m√©thodes de filtrage particulaire qui permettent d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. La principale diff√©rence entre ces deux m√©thodes r√©side dans la mani√®re dont elles mettent √† jour les poids des particules, et ainsi comme la distribution d‚Äôimportance q est construite. Par d√©finition, les poids sont d√©finis par : \\[\nw(x_t) = w(x_{t-1}) \\frac{p(y_t|x_t)p(x_t|x_{t-1})}{q(x_t|x_{t-1},y_t)},\n\\] o√π \\(p(y_t|x_t)\\) est la vraisemblance, \\(p(x_t|x_{t-1})\\) est la distribution de transition, et \\(q(x_t|x_{t-1},y_t)\\) est la distribution d‚Äôimportance.\nDans le cadre du filtre bootstrap, la distribution d‚Äôimportance est d√©finie comme suit : \\[\nq(x_t|x_{t-1},y_t) = p(x_t|x_{t-1}).\n\\] De ce fait, \\(w(x_t) = w(x_{t-1}) p(y_t|x_t)\\).Cela signifie que les poids sont mis √† jour en fonction de la distribution de transition et de la vraisemblance.\nDans le cadre du filtre APF, la distribution d‚Äôimportance se rapproche de la distribution optimale, vu dans la litt√©rature comme √©tant \\(q(x_t|x_{t-1},y_t) = p(x_t|x_{t-1},y_t)\\). De ce fait, les poids sont mis √† jour en fonction de la vraisemblance conditionnelle, ce qui permet d‚Äôam√©liorer la pr√©cision de l‚Äôestimation de l‚Äô√©tat cach√©, et donc de l‚Äôestimation de la volatilit√© dans notre cas."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html#mod√®le-utilis√©",
    "href": "posts/ensai/proc_stochastique/APF_filter.html#mod√®le-utilis√©",
    "title": "APF filter",
    "section": "",
    "text": "Le filtre particulaire APF (Auxiliary Particle Filter) est un filtre particulaire qui utilise des particules auxiliaires pour estimer la densit√© de probabilit√© de l‚Äô√©tat cach√©. Il est utilis√© pour estimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. Dans cet article, nous allons √©tudier la performance du filtre APF en utilisant un exemple simple.\nComme tout filtre particulaire, il est necessaire de sp√©cifier la distribution a priori de l‚Äô√©tat, i.e.¬†\\(p(x_0)\\), la distribution de transition, i.e.¬†\\(p(v_t|v_{t-1})\\) et la vraisemblance, i.e.¬†\\(p(y_t|v_t)\\).\nDans le mod√®le de Heston sp√©cifi√© en (1), on consid√®re que : \\[\np(v_t|v_{t-1})=2c\\chi^2(2cx_k; 2q + 2; 2ce^{-\\kappa \\Delta} x_{k‚àí1}),\n\\]\n\\[p(v_1) = \\Gamma(v_1; a,b)\\] o√π \\(a = \\frac{2 \\kappa \\theta}{\\sigma^2}\\) et \\(b = \\frac{2 \\kappa}{\\sigma^2}\\),\net \\(p(y_t)|v_t) = N(0,h)\\).\n\\[\n\\begin{cases}\ndS_t = S_t \\left( rds + \\sqrt{v_t} dW_t^1 \\right) \\\\\ndv_t = \\kappa (\\theta - v_t) ds + \\sigma \\sqrt{v_t} dW_t^2 \\\\\ndW_t^1 dW_t^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\nPour tester la pertinence de l‚ÄôAPF, nous allons utiliser les param√®tres suivants \\(\\Phi = (\\theta = 0.03, \\kappa = 4, \\sigma = 0.4, \\kappa = -0.87, \\rho = 0.5)\\). Pour passer en temps discret et assurer la positivit√© de la volatilit√©, nous utilisosn le schema d‚Äôeuler (√† \\(|v_t|\\)) suivant :\n\\[\n\\begin{cases}\ny_t = C(t, \\theta, v_t, S_t, K, \\tau) + \\varepsilon_t, \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\theta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}\\Delta} (\\rho w_t^1 + \\sqrt{1-\\rho^2}w_t^2) \\right|, \\\\[10pt]\nS_t = S_{t-1} \\left(1 + \\mu \\Delta + \\sqrt{\\Delta v_t} w_t^1 \\right), \\\\[10pt]\n\\end{cases}\n\\]\no√π \\(\\varepsilon_t \\sim N(0,h=0.01)\\), et \\(w_t^1, w_t^2\\) sont des variables al√©atoires gaussiennes et ind√©pendantes.\n\nNous avons utilis√© un sch√©ma d‚Äôeuler modifi√© pour garantir la positivit√© de la volatilit√©. En effet, la volatilit√© doit √™tre positive dans le mod√®le de Heston, et le sch√©ma d‚Äôeuler standard peut produire des valeurs n√©gatives. En prenant la valeur absolue de la volatilit√© √† chaque √©tape, nous nous assurons que les valeurs restent positives. Il aurait √©t√© √©galement possible d‚Äôutiliser le sch√©ma d‚Äôeuler √† \\(ln(v_t)\\) (via le lemme de ito) pour garantir la positivit√© de la volatilit√©.\n\n\nrm(list=ls())\n\n################ Simulation de la trajectoire de St et vt ################ \nHeston_sim &lt;- function(N, kappa, theta, sigma, rho, v0, mu, tau, S0){\n  # N: Number of time steps\n  dt &lt;- tau / N  # Time step\n\n  # Store stock prices and volatilities\n  S &lt;- numeric(N+1)\n  v &lt;- numeric(N+1)\n  \n  S[1] &lt;- S0\n  v[1] &lt;- v0\n  \n  for (t in 1:N){\n    # Generate correlated Brownian motions\n    W1 &lt;- rnorm(1)\n    W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n    \n    # Euler discretization of variance process (ensure non-negativity)\n    v[t+1] &lt;- abs(v[t] + kappa * (theta - v[t]) * dt + sigma * sqrt(v[t] * dt) * W2)\n    \n    # Euler discretization of the stock price process \n    S[t+1] &lt;- S[t] * (1+ mu*dt + sqrt(v[t+1] * dt) * W1)\n  }\n  \n  return(list(v_t = v, S_t=S))\n}\n\nAvec les √©tapes 1 et 2, nous obtenons les trajectoires de volatilit√© instantan√©e et de prix d‚Äôaction suivantes :\n\nset.seed(123)\nN &lt;- 300\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nrho &lt;- 0.5\nv0 &lt;- 0.03\nmu &lt;- 0.1\ntau&lt;-1\nS0 &lt;- 100\n\nres &lt;- Heston_sim(N=N, kappa=kappa, theta=theta, sigma=sigma, rho=rho, v0=v0, mu=mu, tau=tau, S0=S0)\n\npar(mfrow=c(1,2))\nplot(res$v_t, type=\"l\", main=\"Processus de volatilit√© simul√©\", xlab = \"Time step\", ylab = \"Volatilit√©\") \nplot(res$S_t, type=\"l\", main=\"Processus de prix du sous-jacent simul√©\", xlab = \"Time step\", ylab = \"Prix\") \n\n\n\n\n\n\n\n\n\n\nLa proc√©dure de simulation est la suivante :\n\nTout d‚Äôabord, une trajectoire de 300 pas de temps de la variance instantan√©e sera simul√©e pour un pas de 1 jour, en commen√ßant par \\(v_0\\) = 0,03.\nConditionnellement √† cette trajectoire, une trajectoire correspondante du prix de l‚Äôaction sera alors g√©n√©r√©e.\nNous calculons, √† l‚Äôaide du mod√®le de Heston, les prix d‚Äôoptions bruit√©s pour trois types d‚Äôoptions diff√©rents : une option √† la monnaie (ATM), une option dans la monnaie (ITM) et une option mixte (50 % ITM / 50 % OTM).\n\nNous avons choisi trois types d‚Äôoptions afin d‚Äôobserver comment le filtre APF se comporte dans diff√©rentes conditions de march√© :\n\nOption √† la monnaie (ATM) :\n\nPrix d‚Äôexercice : K = 1 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ATM sont souvent utilis√©es pour l‚Äôestimation de la volatilit√© implicite, car elles sont les plus liquides et pr√©sentent un delta proche de 0.5, ce qui les rend sensibles aux variations du sous-jacent.\n\nOption dans la monnaie (ITM) :\n\nPrix d‚Äôexercice : K = 0.95 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ITM ont une valeur intrins√®que √©lev√©e et une volatilit√© implicite plus stable. Elles sont moins sensibles aux fluctuations imm√©diates du march√© mais permettent d‚Äô√©valuer l‚Äôimpact du filtre APF dans des conditions de faible variance du prix d‚Äôoption.\n\nOption mixte (50 % ITM / 50 % OTM) :\n\nPrix d‚Äôexercice : K = 117\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Cette approche permet de tester le filtre APF dans un sc√©nario r√©aliste de portefeuille d‚Äôoptions o√π des positions ITM et OTM sont combin√©es. L‚Äôobjectif est d‚Äôanalyser si le filtre reste stable lorsque l‚Äôon m√©lange des options avec des sensibilit√©s diff√©rentes aux mouvements du sous-jacent et aux variations de volatilit√©.\n\n\nPourquoi tester diff√©rentes configurations d‚Äôoptions ?\nL‚Äôobjectif de cette analyse est de v√©rifier comment le filtre APF se comporte en pr√©sence de conditions de march√© vari√©es :\n\nOptions ATM : impact fort de la volatilit√©, mais moins sujettes au risque de gamma.\nOptions ITM : faible sensibilit√© √† la volatilit√© implicite, mais risque de couverture plus limit√©.\nOptions mixtes : √©valuation de la robustesse du filtre lorsque plusieurs types d‚Äôoptions coexistent dans un m√™me portefeuille.\n\nCes tests permettent de comparer la pr√©cision du filtre en fonction de la position de l‚Äôoption par rapport au prix du sous-jacent.\n\n################ Simulation du prix des options ATM ################ \nsource(\"data/Heston_Call_Function.R\")\nset.seed(123)\n\nv_t &lt;- res$v_t \nS_t &lt;- res$S_t\nh &lt;- 0.01\n\n# Prix d'option K=1*S, tau = 0.5 =&gt; A la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 1\nATM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nATM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\n\nres$ATM &lt;- ATM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options ITM ################ \n# Prix d'option K=0.95*S, tau = 0.5 =&gt; Hors de la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 0.95\n\nITM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nITM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\nres$ITM &lt;- ITM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options mixte ################ \nsummary(S_t)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  99.49  104.36  117.23  119.24  128.88  156.89 \n\n# Prix d'option K=117, tau = 0.5  =&gt; moiti√© ITM et moiti√© OTM\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 117\n\nMIXTE &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nMIXTE[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K)\n}\nres$MIXTE &lt;- MIXTE + rnorm(1,mean=0,sd=sqrt(h))\n\n\n\nEn simulant le prix de ces options, nous constatons sans surprise que les options mixte ont un prix plus √©lev√© plus on se rapproche de la maturit√©. Cependant, les options ITM et ATM ont des prix plus stables, avec une l√©g√®re augmentation pour les options ITM.\n\nN &lt;- length(res$ITM)  \ndt &lt;- tau / N\ntime_axis &lt;- seq(0, tau, length.out = N)  # Axe des temps, de 0 √† tau\n\nplot(time_axis,res$ITM, type=\"l\", main = \"Evolution du prix des options \", ylim = c(0,50), col = \"red\", ylab = \"Prix\", xlab = \"Temps\")\nlines(time_axis,res$ATM, type = \"l\",col=\"blue\")\nlines(time_axis,res$MIXTE, type='l', col= \"darkgreen\")\n# legend\nlegend(\"topleft\", legend=c(\"ITM\", \"ATM\", \"Mixte\"), col=c(\"red\", \"blue\",'darkgreen'), lty=1:1, cex=0.8, title=\"Types d'option\")\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôobjectif est de comparer les r√©sultats de l‚ÄôAPF sur du filtre particulaire bootstrap en termes d‚Äôerreurs d‚Äôajustement, mesur√©es par les erreurs quadratiques moyennes (RMSE) de l‚Äôajustement de la variance et de l‚Äôajustement des prix des options, dans diff√©rents cas de donn√©es.\nFiltre bootstrap :\nLe filtre boostrap fonctionne de la mani√®re suivante :\n\n\n\nBootstrap filter\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre bootstrap de la mani√®re suivante :\n\nBootstrapParticleFilter &lt;- function(y, S, v, K, tau = 0.5, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, rho = 0.5, r = 0.05, dt = 1, h = 0.01) {\n  \n  # Param√®tres suppl√©mentaires\n  sigma_epsilon &lt;- sqrt(h)\n\n  # Param√®tres de la loi stationnaire de v\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation\n  n &lt;- length(y)\n  v_hat &lt;- numeric(n)\n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire bootstrap\n  for (t in 1:n) {\n    if (t == 1) {\n      # Initialisation des particules √† t = 0\n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      \n      # Poids initiaux bas√©s sur la densit√© de la loi stationnaire\n      w[t, ] &lt;- dgamma(v_particle[t, ], shape = alpha1, rate = 1/alpha2)\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # Estimation initiale\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    } else {\n      # √âtape de pr√©diction (propagation des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                    mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                    sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n      \n      #  Calcul du prix du Call pour chaque particule\n      C &lt;- numeric(M)\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # Calcul pour chaque particule avec le bon strike\n      for (i in 1:M) {\n          C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                       v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n\n      \n      # Mise √† jour des poids (vraisemblance observation-conditionnelle)\n      w[t, ] &lt;- dnorm(y[t], mean = C, sd = sqrt(sigma_epsilon))\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # R√©√©chantillonnage\n      index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n      v_particle[t, ] &lt;- v_particle[t, index]\n      \n      # Poids uniformes apr√®s resampling\n      w_normalized[t, ] &lt;- rep(1 / M, M)\n\n      # Estimation de la volatilit√© instantan√©e\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  return(v_hat)\n}\n\nFiltre APF :\nLe filtre particulaire auxilaire fonctionne de la mani√®re suivante :\n\n\n\nAPF\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre APF de la mani√®re suivante :\n\nAPF_Heston &lt;- function(y, S, v,K, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, \n                       lambda = -0.87, rho = 0.5, r = 0.05, tau = 0.5, dt = 1, h = 0.01) {\n  \n  # Param√®tres de la loi stationnaire de v\n  sigma_epsilon &lt;- sqrt(h)\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation des param√®tres\n  n &lt;- length(y)\n  \n  # Initialisation des vecteurs/matrices\n  v_hat &lt;- numeric(n)                   \n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  mu &lt;- matrix(nrow = n, ncol = M)        \n  py_mu &lt;- matrix(nrow = n, ncol = M) \n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire APF\n  for (t in 1:n) {\n    if (t == 1) {\n      \n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      w[t, ] &lt;- rep(1 / M, M)\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n      \n    } else {\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # 1 - Pr√©-s√©lection\n      mu[t, ] &lt;- abs(v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt)\n      \n      mean_pymu &lt;- numeric(M)\n      for (i in 1:M) {\n        mean_pymu[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                             v0 = mu[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      py_mu[t, ] &lt;- dnorm(y[t], mean = mean_pymu, sd = sigma_epsilon)\n\n      # Poids pour la pr√©-s√©lection\n      w[t-1, ] &lt;- py_mu[t, ] * w_normalized[t-1, ]\n      \n      # 2 - Resampling (√©chantillonnage)\n      index &lt;- sample(1:M, size = M, replace = TRUE, prob = w[t-1, ])\n\n      # Mise √† jour des particules\n      v_particle[t-1, ] &lt;- v_particle[t-1, index]\n      \n      # 3 - Propagation (√©volution des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                   mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                   sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n\n      # Mise √† jour des poids\n      C &lt;- numeric(M)\n      for (i in 1:M) {\n        C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                     v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      likelihood &lt;- dnorm(y[t], mean = C, sd = sigma_epsilon)\n      w[t, ] &lt;- likelihood / (py_mu[t, index] + 1e-12)  # Protection pour √©viter la division par z√©ro\n      \n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  # Retourner les r√©sultats\n  return(v_hat)\n}\n\nDiff√©rence entre le filtre bootstrap et le filtre APF :\nLe filtre bootstrap et le filtre APF sont deux m√©thodes de filtrage particulaire qui permettent d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. La principale diff√©rence entre ces deux m√©thodes r√©side dans la mani√®re dont elles mettent √† jour les poids des particules, et ainsi comme la distribution d‚Äôimportance q est construite. Par d√©finition, les poids sont d√©finis par : \\[\nw(x_t) = w(x_{t-1}) \\frac{p(y_t|x_t)p(x_t|x_{t-1})}{q(x_t|x_{t-1},y_t)},\n\\] o√π \\(p(y_t|x_t)\\) est la vraisemblance, \\(p(x_t|x_{t-1})\\) est la distribution de transition, et \\(q(x_t|x_{t-1},y_t)\\) est la distribution d‚Äôimportance.\nDans le cadre du filtre bootstrap, la distribution d‚Äôimportance est d√©finie comme suit : \\[\nq(x_t|x_{t-1},y_t) = p(x_t|x_{t-1}).\n\\] De ce fait, \\(w(x_t) = w(x_{t-1}) p(y_t|x_t)\\).Cela signifie que les poids sont mis √† jour en fonction de la distribution de transition et de la vraisemblance.\nDans le cadre du filtre APF, la distribution d‚Äôimportance se rapproche de la distribution optimale, vu dans la litt√©rature comme √©tant \\(q(x_t|x_{t-1},y_t) = p(x_t|x_{t-1},y_t)\\). De ce fait, les poids sont mis √† jour en fonction de la vraisemblance conditionnelle, ce qui permet d‚Äôam√©liorer la pr√©cision de l‚Äôestimation de l‚Äô√©tat cach√©, et donc de l‚Äôestimation de la volatilit√© dans notre cas."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html#option-√†-la-monnaie-atm",
    "href": "posts/ensai/proc_stochastique/APF_filter.html#option-√†-la-monnaie-atm",
    "title": "APF filter",
    "section": "2.1 Option √† la monnaie (ATM)",
    "text": "2.1 Option √† la monnaie (ATM)\n\n2.1.1 Filtre bootstrap\n\n##################### D√©finition des param√®tres #####################\n# D√©finition des param√®tres phi\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nlambda &lt;- -0.87 # Prime de risque\nrho &lt;- 0.5\n\n# Param√®tres de Heston \nh &lt;- 0.01\nr &lt;- 0.05\n\n# Prix de call (y), du sous-jacent (S), et volatilit√© instantan√©e (v)\ny &lt;- res$ATM\nS &lt;- res$S_t\nv&lt;- res$v_t\n\n# Initialisation des param√®tres\nn &lt;- length(y)  # Nombre d'observations\nM &lt;- 200      # Nombre de particules\ndt &lt;- 1\n\n\nset.seed(123)\n\n# Param√®tres de l'option \nK &lt;- 1 * S\ntau&lt;- 0.5\n\n# Exemple d'appel avec tes donn√©es \"res\"\net_boot_atm &lt;- system.time({\nboot_atm &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_atm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_atm)\n\nutilisateur     syst√®me      √©coul√© \n     12.028       0.739      12.971 \n\nrmse_boot_atm &lt;- sqrt(mean((boot_atm - v)^2))\ncat(\"RMSE :\", rmse_boot_atm)\n\nRMSE : 0.001402773\n\n\n\n\n2.1.2 Filtre APF\n\nset.seed(123)\net_apf_atm &lt;- system.time({\napf_atm &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_atm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_atm)\n\nutilisateur     syst√®me      √©coul√© \n     23.965       1.410      25.782 \n\nrmse_apf_atm &lt;- sqrt(mean((apf_atm - v)^2))\ncat(\"RMSE :\", rmse_apf_atm)\n\nRMSE : 0.001889304"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html#option-√†-la-monnaie-itm",
    "href": "posts/ensai/proc_stochastique/APF_filter.html#option-√†-la-monnaie-itm",
    "title": "APF filter",
    "section": "2.2 Option √† la monnaie (ITM)",
    "text": "2.2 Option √† la monnaie (ITM)\n\n2.2.1 Filtre bootstrap\n\nset.seed(123)\ny &lt;- res$ITM\nK &lt;- 0.95 * S\n\net_boot_itm &lt;- system.time({\nboot_itm &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_itm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_itm)\n\nutilisateur     syst√®me      √©coul√© \n     11.420       0.646      12.123 \n\nrmse_boot_itm &lt;- sqrt(mean((boot_itm - v)^2))\ncat(\"RMSE :\", rmse_boot_itm)\n\nRMSE : 0.00152023\n\n\n\n\n2.2.2 APF\n\net_apf_itm &lt;- system.time({\napf_itm &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_itm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_itm)\n\nutilisateur     syst√®me      √©coul√© \n     22.690       1.271      24.052 \n\nrmse_apf_itm &lt;- sqrt(mean((apf_itm - v)^2))\ncat(\"RMSE :\", rmse_apf_itm)\n\nRMSE : 0.001850903"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html#options-mixtes",
    "href": "posts/ensai/proc_stochastique/APF_filter.html#options-mixtes",
    "title": "APF filter",
    "section": "2.3 Options mixtes",
    "text": "2.3 Options mixtes\n\n2.3.1 Filtre bootstrap\n\nset.seed(123)\ny &lt;- res$MIXTE\nK &lt;- 117\n\net_boot_mixte &lt;- system.time({\nboot_mixte &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_mixte, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_mixte)\n\nutilisateur     syst√®me      √©coul√© \n     12.057       0.692      12.879 \n\nrmse_boot_mixte&lt;- sqrt(mean((boot_mixte - v)^2))\ncat(\"RMSE :\", rmse_boot_mixte)\n\nRMSE : 0.01595637\n\n\n\n\n2.3.2 APF\n\net_apf_mixte &lt;- system.time({\napf_mixte &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_mixte, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_mixte)\n\nutilisateur     syst√®me      √©coul√© \n     23.758       1.300      25.226 \n\nrmse_apf_mixte &lt;- sqrt(mean((apf_mixte - v)^2))\ncat(\"RMSE :\", rmse_apf_mixte)\n\nRMSE : 0.01475175"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/APF_filter.html#comparaison-des-performances",
    "href": "posts/ensai/proc_stochastique/APF_filter.html#comparaison-des-performances",
    "title": "APF filter",
    "section": "2.4 Comparaison des performances",
    "text": "2.4 Comparaison des performances\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Cr√©ation du data.frame avec les colonnes demand√©es\ncomp_perf &lt;- data.frame(\n    M√©thode = c(\"APF\", \"APF\", \"APF\", \"BOOT\", \"BOOT\", \"BOOT\"),\n    Type_Option = c(\"ATM\", \"ITM\", \"MIXTE\", \"ATM\", \"ITM\", \"MIXTE\"),\n    RMSE = c(rmse_apf_atm, rmse_apf_itm, rmse_apf_mixte, rmse_boot_atm, rmse_boot_itm, rmse_boot_mixte)\n)\n\n# Conversion : M√©thode comme colonne\ncomp_perf_wide &lt;- pivot_wider(\n    comp_perf,\n    names_from = M√©thode,   # Ce qui devient les colonnes\n    values_from = RMSE      # Ce qui remplit les cases\n)\n\ncomp_perf_wide &lt;- bind_rows(\n    comp_perf_wide,\n    data.frame(\n        Type_Option = \"Moyenne\",\n        APF = mean(c(rmse_apf_atm, rmse_apf_itm, rmse_apf_mixte), na.rm = TRUE),\n        BOOT = mean(c(rmse_boot_atm, rmse_boot_itm, rmse_boot_mixte), na.rm = TRUE)\n    )\n)\n# Affichage du r√©sultat\ncomp_perf_wide\n\n# A tibble: 4 √ó 3\n  Type_Option     APF    BOOT\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 ATM         0.00189 0.00140\n2 ITM         0.00185 0.00152\n3 MIXTE       0.0148  0.0160 \n4 Moyenne     0.00616 0.00629\n\n\n\n# Comparaison des temps d'ex√©cution\ncomp_temps &lt;- data.frame(\n    M√©thode = c(\"APF\", \"APF\", \"APF\", \"BOOT\", \"BOOT\", \"BOOT\"),\n    Type_Option = c(\"ATM\", \"ITM\", \"MIXTE\", \"ATM\", \"ITM\", \"MIXTE\"),\n    Temps = c(et_apf_atm[3], et_apf_itm[3], et_apf_mixte[3], et_boot_atm[3], et_boot_itm[3], et_boot_mixte[3])\n)\n\n# Conversion : M√©thode comme colonne\ncomp_temps_wide &lt;- pivot_wider(\n    comp_temps,\n    names_from = M√©thode,   # Ce qui devient les colonnes\n    values_from = Temps      # Ce qui remplit les cases\n)\n\ncomp_temps_wide &lt;- bind_rows(\n    comp_temps_wide,\n    data.frame(\n        Type_Option = \"Moyenne\",\n        APF = mean(c(et_apf_atm[3], et_apf_itm[3], et_apf_mixte[3]), na.rm = TRUE),\n        BOOT = mean(c(et_boot_atm[3], et_boot_itm[3], et_boot_mixte[3]), na.rm = TRUE)\n    )\n)\n# Affichage du r√©sultat\ncomp_temps_wide\n\n# A tibble: 4 √ó 3\n  Type_Option   APF  BOOT\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 ATM          25.8  13.0\n2 ITM          24.1  12.1\n3 MIXTE        25.2  12.9\n4 Moyenne      25.0  12.7\n\n\nEn comparant les performances de l‚ÄôAPF √† celles du filtre bootstrap, nous constatons que les deux filtres ont des performances assez similaires en termes d‚Äôerreurs d‚Äôajustement (RMSE) pour les options ATM et ITM. Cependant, l‚ÄôAPF semble √™tre plus efficace pour les options mixtes, avec des erreurs d‚Äôajustement plus faibles que le filtre bootstrap. Cependant, en termes de temps d‚Äôex√©cution, le filtre bootstrap est plus rapide que l‚ÄôAPF pour les trois types d‚Äôoptions. En moyenne, l‚ÄôAPF est plus lent que le filtre bootstrap, mais offre une meilleure pr√©cision tout option confondue.\nPou connaitre la robustesse de ces estimations, nous avons fait du bootstrap pour estimer la distribution des RMSE, et ainsi obtenir des intervalles de confiance pour ces estimations, de m√™me que des p-valeurs. Le code ayant servi √† la simulation est disponible ici.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggpubr)\n\n# Charger les r√©sultats du bootstrap\nres_ATM &lt;- readRDS(\"results_sim/res_ATM_min.rds\")\nres_ITM &lt;- readRDS(\"results_sim/res_ITM_min.rds\")\nres_MIXTE &lt;- readRDS(\"results_sim/res_MIXTE_min.rds\")\n\n# Extraire les RMSE\nrmse_bootstrap_atm &lt;- res_ATM$t[, 1]\nrmse_apf_atm_b &lt;- res_ATM$t[, 2]\n\nrmse_bootstrap_itm &lt;- res_ITM$t[, 1]\nrmse_apf_itm_b &lt;- res_ITM$t[, 2]\n\nrmse_bootstrap_mixte &lt;- res_MIXTE$t[, 1]\nrmse_apf_mixte_b &lt;- res_MIXTE$t[, 2]\n\n# Cr√©er un dataframe avec les valeurs RMSE pour les trois sc√©narios\ndf_rmse_boxplot &lt;- data.frame(\n  M√©thode = rep(c(\"Bootstrap Filter\", \"APF-Heston\"), each = c(length(rmse_bootstrap_atm), length(rmse_apf_atm_b), length(rmse_bootstrap_itm), length(rmse_apf_itm_b),length(rmse_bootstrap_mixte), length(rmse_apf_mixte_b))),\n  RMSE = c(rmse_bootstrap_atm, rmse_apf_atm_b,\n           rmse_bootstrap_itm, rmse_apf_itm_b,\n           rmse_bootstrap_mixte, rmse_apf_mixte_b),\n  Sc√©nario = rep(c(\"ATM\", \"ATM\", \"ITM\", \"ITM\", \"MIXTE\", \"MIXTE\"), each = c(length(rmse_bootstrap_atm), length(rmse_apf_atm_b),\nlength(rmse_bootstrap_itm), length(rmse_apf_itm_b),\nlength(rmse_bootstrap_mixte), length(rmse_apf_mixte_b)))\n)\n\nWarning in rep(c(\"Bootstrap Filter\", \"APF-Heston\"), each =\nc(length(rmse_bootstrap_atm), : seul le premier √©l√©ment de l'argument 'each'\nest utilis√©\n\n\nWarning in rep(c(\"ATM\", \"ATM\", \"ITM\", \"ITM\", \"MIXTE\", \"MIXTE\"), each =\nc(length(rmse_bootstrap_atm), : seul le premier √©l√©ment de l'argument 'each'\nest utilis√©\n\n# Calcul des p-valeurs\n\n# kruskall wallis\ntest_atm &lt;- kruskal.test(list(rmse_bootstrap_atm, rmse_apf_atm_b))$p.value\ntest_itm &lt;- kruskal.test(list(rmse_bootstrap_itm, rmse_apf_itm_b))$p.value\ntest_mixte &lt;- kruskal.test(list(rmse_bootstrap_mixte, rmse_apf_mixte_b))$p.value\n\n# test_atm &lt;- t.test(rmse_bootstrap_atm, rmse_apf_atm_b)$p.value\n# test_itm &lt;- t.test(rmse_bootstrap_itm, rmse_apf_itm_b)$p.value\n# test_mixte &lt;- t.test(rmse_bootstrap_mixte, rmse_apf_mixte_b)$p.value\n\n# Cr√©ation du boxplot avec facet_wrap et ajout des p-valeurs en caption\nggplot(df_rmse_boxplot, aes(x = M√©thode, y = RMSE, fill = M√©thode)) +\n  geom_boxplot(alpha = 0.6) +\n  facet_wrap(~Sc√©nario, scales = \"free_y\") +\n  labs(title = \"Comparaison des RMSE (Bootstrap vs APF) selon les sc√©narios\",\n       x = \"M√©thode de Filtrage\",\n       y = \"RMSE\",\n       caption = paste(\"p-values t-test: ATM =\", formatC(test_atm, format = \"e\", digits = 2),\n                       \"| ITM =\", formatC(test_itm, format = \"e\", digits = 2),\n                       \"| MIXTE =\", formatC(test_mixte, format = \"e\", digits = 2))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n# Concat√©ner toutes les RMSE\ntotal_rmse_bootstrap &lt;- c(rmse_bootstrap_atm, rmse_bootstrap_itm, rmse_bootstrap_mixte)\ntotal_rmse_apf &lt;- c(rmse_apf_atm_b, rmse_apf_itm_b, rmse_apf_mixte_b)\n\n# Kruskal wallis test\nglobal_pval &lt;- kruskal.test(list(total_rmse_bootstrap, total_rmse_apf))$p.value\n\n# global_pval &lt;- t.test(total_rmse_bootstrap, total_rmse_apf)$p.value\n\n# Boxplot global\nggboxplot(df_rmse_boxplot, x = \"M√©thode\", y = \"RMSE\", fill = \"M√©thode\",\n          ylab = \"RMSE\", xlab = \"M√©thode de Filtrage\",\n          title = \"Comparaison des RMSE (Bootstrap vs APF) pour les trois sc√©narios\") +\n  labs(caption = paste(\"p-values: Global =\", formatC(global_pval, format = \"e\", digits = 2))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nApproche pvalue :\n\nrmse_diff_atm &lt;- rmse_bootstrap_atm - rmse_apf_atm_b\nrmse_diff_atm_obs &lt;- rmse_boot_atm - rmse_apf_atm\n\nmean(rmse_diff_atm_obs &gt; rmse_diff_atm)\n\n[1] 0.84\n\nrmse_diff_itm &lt;- rmse_bootstrap_itm - rmse_apf_itm_b\nrmse_diff_itm_obs &lt;- rmse_boot_itm - rmse_apf_itm\n\nmean(rmse_diff_itm_obs &gt; rmse_diff_itm)\n\n[1] 0.38\n\nrmse_diff_mixte &lt;- rmse_bootstrap_mixte - rmse_apf_mixte_b\nrmse_diff_mixte_obs &lt;- rmse_boot_mixte - rmse_apf_mixte\n\nmean(rmse_diff_mixte_obs &gt; rmse_diff_mixte)\n\n[1] 1\n\n\n\nConclusion : L‚ÄôAPF est une m√©thode plus pr√©cise que le filtre bootstrap pour estimer la volatilit√© instantan√©e dans le cadre d‚Äôun mod√®le de Heston o√π le prix est bruit√© \\(h=0.01\\), mais elle est √©galement plus lente en termes de temps d‚Äôex√©cution. Le choix entre ces deux m√©thodes d√©pendra donc des besoins sp√©cifiques de l‚Äôanalyse, en fonction de la pr√©cision et de la vitesse d‚Äôex√©cution requises.\nPour tester la robustesse de ces estimations, il aurait fallu faire du bootstrap afin d‚Äôestimer la distribution des RMSE, et ainsi obtenir des intervalles de confiance pour ces estimations.\nIl aurait √©t√© interessant de tester les filtres dans d‚Äôautres cas de figures."
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part1.html",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part1.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le mod√®le √† volatilit√© stochastique de Taylor est un mod√®le de volatilit√© stochastique qui est utilis√© pour mod√©liser la volatilit√© des actifs financiers. Le mod√®le est d√©fini par l‚Äô√©quation suivante :\n\\[\n\\begin{aligned}\nr_t &= \\exp(x_t/2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim \\text{N}(0,1) \\\\\nx_t &= \\mu + \\phi \\cdot x_{t-1} + \\sigma_t \\cdot \\eta_t\n\\end{aligned}\n\\]\no√π \\(r_t\\) est le rendement de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(\\mu\\) est la moyenne de la volatilit√©, \\(\\phi\\) est le coefficient d‚Äôautor√©gression, \\(\\sigma_t\\) est l‚Äô√©cart-type de la volatilit√© √† l‚Äôinstant \\(t\\), \\(\\eta_t\\) est un bruit blanc gaussien, et \\(\\varepsilon_t\\) est un bruit blanc gaussien.\nPour extraire la volatilit√©, nous utilisons le filtre de Kalman sur le logarithme des rendements au carr√© \\(y_t = \\log(r_t^2)\\), afin de lin√©ariser le mod√®le. Le mod√®le lin√©aris√© est d√©fini par l‚Äô√©quation suivante :\n\\[\ny_t = x_t + \\varepsilon_t\n\\]\no√π \\(y_t\\) est le logarithme des rendements au carr√© √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), et \\(\\varepsilon_t\\) est un bruit blanc de loi log-\\(\\chi^2\\).\nDe ce fait, nous pouvons utiliser le filtre de Kalman pour estimer la volatilit√© de l‚Äôactif financier en utilisant les rendements observ√©s. En effet, le filtre de Kalman est un algorithme r√©cursif qui permet d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique √† partir d‚Äôobservations bruit√©es. Il s‚Äôapplique √† des mod√®les lin√©aires dont le bruit est gaussien. Dans notre cas, nous avons lin√©aris√© le mod√®le pour qu‚Äôil soit compatible avec le filtre de Kalman. Cependant, le bruit n‚Äôest pas gaussien, mais log-\\(\\chi^2\\). L‚Äôobjectif de ce notebook est d‚Äôobserver le comportement du filtre de Kalman o√π le bruit n‚Äôest pas gaussien.\nNous poss√©dons d√©j√† d‚Äôun fichier avec les rendements de l‚Äôactif financier et la vraie volatilit√© simul√©s avec les param√®tres suivants \\(\\mu = -0.8\\), \\(\\phi = 0.9\\), \\(\\sigma = 0.09\\). Nous allons donc utiliser ces donn√©es pour estimer la volatilit√© de l‚Äôactif financier en utilisant le filtre de Kalman. N√©anmoins, le code est √©galement fourni pour simuler les donn√©es si vous souhaitez tester le filtre de Kalman sur d‚Äôautres param√®tres.\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\n\n# r_t = exp(x_t/2)*eps_t (eps_t iid N(0,1))\n# x_t = mu + phi * x_{t-1} + sigma_t * eta_t  (eta_t iid N(0,1))\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Param√®tres\nn = 252\nmu = -0.8\nphi = 0.9\nsigma_squared = 0.09\n\n# Simulation\n# np.random.seed(0)\n\n# x = np.zeros(n)\n# r = np.zeros(n)\n\n# for t in range(0, n):\n#     if t == 0:\n#         x[t] = np.random.normal(loc= mu/(1-phi), scale=np.sqrt(sigma_squared/(1-phi**2))) # Densit√© de transition stationnaire de x_t\n#     else:\n#         x[t] = mu + phi * x[t-1] + np.sqrt(sigma_squared) * np.random.normal(loc=0, scale=1)\n#     r[t] = np.exp(x[t]/2) * np.random.normal(loc=0, scale=1)\n\ndata  = pd.read_csv('true_sv_taylor.csv')\nr = data['r']\nx = data['x']\n\n\n# Affichage des trajectoires\nimport matplotlib.pyplot as plt\n\nplt.plot(r, color=\"black\")\nplt.title(\"Trajectoire des rendements\")\nplt.show()\n\nplt.plot(x, color='red')\nplt.title(\"Trajectoire de log-volatilit√©\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Construction du mod√®le log-SV (mod√®le de Taylor)\n\n# Pour appliquer le filtre de Kalman, il faut que les bruits soient centr√©s.\nmu_r_squared = -1.27 # car log(eps**2) suit une log chi-deux\ny = np.log(r**2) - mu_r_squared\n\n\n\nLe filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649"
  },
  {
    "objectID": "posts/ensai/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "href": "posts/ensai/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Dans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp3.html",
    "href": "posts/ensai/apprentisage-stat/Tp3.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "AdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\nHence, for M weak learners, the final prediction is given by \\(g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)\\) where \\(g_m(x)\\) is the m-th weak learner and \\(\\alpha_m\\) is the weight associated with the m-th weak learner. The optimisation problem of AdaBoost is given by:\n\\[ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) \\]\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function \\(l(y,f(x))=\\exp(-yf(x))\\), \\(y \\in \\{-1,+1\\}\\). It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n\nInitialize the observation weights \\(w_i^{(1)} = 1/n\\) for \\(i=1,\\dots,n\\)\n\n\nFor m=1 to M:\n\n\n\nFit a weak learner \\(g_m(x)\\) to the training data using weights \\(w_i^{(m)}\\)\n\n\nCompute the error rate \\(err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))\\) where \\(I\\) is the indicator function\n\n\nCompute the weight \\(\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)\\)\n\n\nUpdate the weights \\(w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)\\)\n\n\n\nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems. It is a generalization of the AdaBoost algorithm to multi-class classification problems.\nTo inspect how the errors and the weights vary with the number of iterations, we will the function make_gaussian_quantiles from sklearn. This function generates a multi-dimensional standard normale distribution with a given number of samples \\(n\\) per class \\(K\\). We will generate a dataset of size \\(n=2000\\) with \\(K=3\\) classes and \\(d=10\\) features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.475\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Documents/Repositories/website/venv/lib/python3.14/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n  warnings.warn(\n\n\nAccuracy:  0.715\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp3.html#i.-decision-tree",
    "href": "posts/ensai/apprentisage-stat/Tp3.html#i.-decision-tree",
    "title": "Gradient boosting",
    "section": "",
    "text": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.475"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "href": "posts/ensai/apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "title": "Gradient boosting",
    "section": "",
    "text": "# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Documents/Repositories/website/venv/lib/python3.14/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n  warnings.warn(\n\n\nAccuracy:  0.715\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html",
    "href": "posts/ensai/apprentisage-stat/Tp1.html",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]\n\n\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html#ridge-regression",
    "href": "posts/ensai/apprentisage-stat/Tp1.html#ridge-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html#lasso-regression",
    "href": "posts/ensai/apprentisage-stat/Tp1.html#lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "If we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html#ridge-regression-1",
    "href": "posts/ensai/apprentisage-stat/Tp1.html#ridge-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "1. Ridge regression",
    "text": "1. Ridge regression\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter \\(\\lambda\\) by cross-validation. We will use the function RidgeCV from the sklearn library to perform the cross-validation. Since we didn‚Äôt center the covariables, we will set the parameter fit_intercept to True in order to include an intercept in the model.\nBy default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter Œª. This is called regularization path. We can do this by fitting the model for different values of Œª and store the coefficients.\n\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'Œª = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter \\(\\lambda\\) wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients.\n In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. \n\na. Check on the intercept value of the model using lambda found by cross validation\n\nprint(f'Intercept value : {ridge_cv.intercept_}')\n\nIntercept value : 152.28937186306015\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features."
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html#lasso-regression-1",
    "href": "posts/ensai/apprentisage-stat/Tp1.html#lasso-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "2. Lasso regression",
    "text": "2. Lasso regression\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation. It is important to perform a cross-validation in order to choose the best value of the regularization parameter \\(\\lambda\\) as we have demonstrated in the first activity. For the lasso regression, we will use the function LassoCV from the sklearn library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n\n\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Œª = {lasso_cv.alpha_:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter \\(\\lambda\\) given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of \\(\\lambda\\), we can select more or less variables.\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n\na. Check on the intercept value of the model using lambda found by cross validation\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n\nIntercept value : 151.95282341561403"
  },
  {
    "objectID": "posts/ensai/apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "href": "posts/ensai/apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "3. Quality of the models (ridge regression vs lasso regression)",
    "text": "3. Quality of the models (ridge regression vs lasso regression)\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\\[\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2\\]\nwhere \\(\\hat g(x)\\) is the predicted value of the output variable y given the input variable x, \\(\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j\\).\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model."
  },
  {
    "objectID": "posts/m2mo/derivatives/00_references.html",
    "href": "posts/m2mo/derivatives/00_references.html",
    "title": "Introduction to financial markets",
    "section": "",
    "text": "In the financial sector, a market refers to the place where the supply and demand for financial products and services meet. It brings together numerous participants, including banks, financial institutions, corporations, and individual investors.\nAmong the various financial instruments traded, one finds equities, bonds, derivatives, currencies, and commodities.\nIn this article, we examine three major types of financial markets: the equity market, the bond market, and the derivatives market, along with their characteristics and operating mechanisms."
  },
  {
    "objectID": "posts/m2mo/derivatives/00_references.html#equity-markets",
    "href": "posts/m2mo/derivatives/00_references.html#equity-markets",
    "title": "Introduction to financial markets",
    "section": "Equity markets",
    "text": "Equity markets\nThe equity market enables companies to raise capital by issuing ownership securities (shares) that investors can purchase. These shares are subsequently traded on stock exchanges such as euronext paris or the new york stock exchange.\nShare prices fluctuate according to supply and demand, influenced by corporate performance, macroeconomic conditions, and market sentiment.\nThe main participants in this market include issuing companies, institutional investors such as pension funds and insurance companies, and retail investors. To buy or sell shares, investors typically use online brokers or investment banks. They submit buy orders (bid) or sell orders (ask), which can be observed in the order book and are then executed on the market.\nA market order corresponds to an immediate purchase or sale at the best available price.\nA limit order allows the purchase or sale of a specified quantity of securities subject to a price condition. It provides control over the execution price and enables investors to set a target buying or selling price.\nA stop order is executed only if the price reaches a specified threshold. It is often used to limit losses or secure gains. For a purchase, it is generally placed above the current market price; for a sale, below.\nA stop-limit order combines a stop trigger with a limit condition and may include time restrictions.\nA trailing stop order functions similarly to a stop order but is defined as a percentage variation rather than a fixed price level.\nAdditional tactical order types allow the implementation of more sophisticated investment strategies.\nIn france, the autorit√© des march√©s financiers supervises financial markets to ensure transparency and proper functioning.\n\nPrice formation\nThe price of a share is determined by the interaction of supply and demand. When demand increases, the price tends to rise. Conversely, when supply exceeds demand, the price falls. The traded price corresponds to the effective intersection between a bid and an ask, that is, where an order is matched.\nFundamental factors also influence prices, including corporate earnings, dividend distributions, growth prospects, and general economic conditions.\nAt market close, the closing price corresponds to the last traded price of the session. This price may differ from the next opening price due to events occurring outside trading hours."
  },
  {
    "objectID": "posts/m2mo/derivatives/00_references.html#bond-market",
    "href": "posts/m2mo/derivatives/00_references.html#bond-market",
    "title": "Introduction to financial markets",
    "section": "Bond market",
    "text": "Bond market\nThe bond market is where investors buy and sell debt securities. Bonds are debt instruments issued by governments or corporations to raise capital. A bond is a contract between an issuer and an investor in which the issuer commits to repaying the principal at maturity while paying periodic interest payments, known as coupons.\nThrough bond issuance, the issuer contracts debt on the market. The investor lends funds by purchasing the bond and receives interest payments until maturity, at which point the principal, also called the face value, is repaid.\nBonds are generally considered lower-risk investments compared to equities, as they typically provide fixed returns and exhibit lower volatility. However, their expected return is usually lower than that of stocks.\nBonds can be classified according to their issuer, maturity, and interest structure. Government bonds are issued by sovereign states. Corporate bonds are issued by private or public companies. Short-term bonds typically have maturities below five years, whereas long-term bonds exceed ten years. Interest rates may be fixed or variable. Bonds are also categorized according to credit rating, which assesses the issuer‚Äôs ability to repay its debt.\n\nBond valuation\nBond valuation consists of determining its present value, that is, the amount an investor is willing to pay today. The price of a bond is given by\n\\[B(t,T) = \\sum_{u=1}^{T} \\frac{C}{(1+r)^u} + \\frac{N}{(1+r)^T}.\n\\]\nThe bond value depends on the market interest rate (r), the remaining maturity (T-t), the coupon rate, and the issuer‚Äôs credit quality. When market interest rates increase, existing bond prices decline, as new bonds offer higher yields.\nA zero-coupon bond does not pay periodic interest. Its value simplifies to\n\\(B(t,T) = \\frac{N}{(1+r)^{T-t}}.\\)"
  },
  {
    "objectID": "posts/m2mo/derivatives/00_references.html#derivatives-market",
    "href": "posts/m2mo/derivatives/00_references.html#derivatives-market",
    "title": "Introduction to financial markets",
    "section": "Derivatives market",
    "text": "Derivatives market\nDerivatives are financial instruments whose value depends on an underlying asset. Examples include options, futures, forwards, and swaps. These instruments are used for hedging, speculation, and arbitrage.\nHedging involves taking a derivative position to offset an existing exposure. Speculation consists of taking a position based on expected future price movements without prior exposure. Arbitrage involves exploiting price discrepancies across markets to generate risk-free profit.\nDerivatives may be traded on organized exchanges or over-the-counter markets. Organized markets are centralized and regulated, offering standardized contracts and daily settlement through clearinghouses. Margin requirements and marking-to-market procedures reduce counterparty risk.\nOver-the-counter markets are decentralized and allow customized contracts between counterparties.\nPrices are determined by bid and ask quotes. The bid-ask spread reflects market liquidity.\n\nForward contracts\nA forward contract is an agreement between two parties to buy or sell an asset at a price (K) at a future date (T). The payoff for a long position is \\(S_T - K,\\) and for a short position, \\(K - S_T.\\)\nForwards are primarily used for hedging purposes.\n\n\nOptions\nAn option gives the holder the right, but not the obligation, to buy or sell an underlying asset at a strike price (K) before or at maturity (T).\nThe payoff of a european call is \\(\\max(S_T - K, 0),\\) while the payoff of a european put is \\(\\max(K - S_T, 0).\\)\nOptions may be used for hedging, speculation, or income generation.\n\n\nFutures\nFutures contracts are standardized agreements traded on organized exchanges. They are marked to market daily, and margin accounts are adjusted accordingly. Futures prices converge to spot prices at maturity to prevent arbitrage.\n\n\nSwaps\nA swap is a derivative contract in which two parties exchange cash flows based on a notional amount over a specified period. The most common type is the interest rate swap, in which one party pays a fixed rate while receiving a floating rate, typically linked to libor or a similar benchmark.\nThe swap can be viewed as a combination of a fixed-rate bond and a floating-rate bond. Its value equals the difference between the present values of fixed and floating cash flows:\n\\[V_{\\text{swap}} = B_{\\text{fixed}} - B_{\\text{floating}}.\n\\]\nWhen the fixed rate equals the swap rate, the contract initially has zero net value.\nSwap markets provide quoted bid and ask fixed rates across maturities, which are used to construct the zero-coupon yield curve through bootstrapping techniques.\nAlthough swap rates are not strictly risk-free, they are often used as reference discount rates for long maturities under normal market conditions.\nSource: john c.¬†hull, options, futures, and other derivatives, 11th edition."
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html",
    "href": "posts/m2mo/edp/edp_american_opt.html",
    "title": "Finite difference Methods for American Options",
    "section": "",
    "text": "Dans ce TP, on √©tudie des sch√©mas num√©riques permettant d‚Äôapprocher la solution de l‚Äô√©quation de Black-Scholes pour les options am√©ricaines. On considerera en particulier deux cas de payoff pour les options am√©ricaines :\nDans les deux cas, l‚ÄôEDP correspond √† un syst√®me non lin√©aire d‚Äô√©quations aux d√©riv√©es partielles suivant : \\[\n\\begin{cases}\n\\min \\left( \\frac{\\partial v}{\\partial t} - \\frac{1}{2} \\sigma^2 s^2 \\frac{\\partial^2 v}{\\partial s^2} - r s \\frac{\\partial v}{\\partial s} + r v, v - \\phi \\right) = 0, \\quad (t,s) \\in (0,T) \\times (S_{min}, S_{max}), \\\\\nv(t, S_{min}) = v_l(t), \\quad t \\in (0,T), \\\\\nv(t, S_{max}) = v_r(t) := 0, \\quad t \\in (0,T), \\\\\nv(0,s) = g(s), \\quad s \\in (S_{min}, S_{max}),\n\\end{cases}\n\\]\nDans le cas du payoff 1, on a \\(v_l(t) := \\phi(S_{min}) = K - S_{min}\\), tandis que dans le cas du payoff 2, on a \\(v_l(t) := \\phi(S_{min}) = 0\\).\nCes sch√©mas nous permettront d‚Äôobtenir une approximation num√©rique de la fonction de prix d‚Äôun put europ√©en \\(v(t,s)\\), avec \\(t \\in [0,T]\\) et \\(s \\in [S_{min}, S_{max}]\\).\nLes sch√©mas num√©riques permettant d‚Äôapprocher cette EDP abord√©s ici sont : 1. le sch√©ma d‚ÄôEuler explicite 2. le sch√©ma d‚ÄôEuler implicite (et plus g√©n√©ralement pour tout sch√©ma implicite pour les options am√©ricaines), on est conduit √† un systeme non lin√©aire discret √† r√©soudre √† chaque pas de temps, qui √† la forme d‚Äôun ‚Äúprobl√®me d‚Äôobstacle‚Äù. 2. On s‚Äôint√©ressera √† diverses m√©thodes num√©riques de r√©solution de ce syst√®me, telles que la m√©thode PSOR (projected successive over-relaxation), une m√©thode de type Newton."
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#explicit-euler-scheme-or-euler-forward-scheme",
    "href": "posts/m2mo/edp/edp_american_opt.html#explicit-euler-scheme-or-euler-forward-scheme",
    "title": "Finite difference Methods for American Options",
    "section": "Explicit Euler Scheme or Euler Forward Scheme",
    "text": "Explicit Euler Scheme or Euler Forward Scheme\nLe sch√©ma d‚ÄôEuler explicite est un sch√©ma bas√© sur une discr√©tisation explicite en temps. La discr√©tisation de l‚ÄôEDP est bas√©e sur des approximations centr√©es. D√®s lors, on approxime les d√©riv√©es partielles de la mani√®re suivante :\n\\[\n\\begin{cases}\nmin(\\frac{U_j^{n+1} - U_j^n}{\\Delta t} - \\frac{1}{2} \\sigma^2 s_j^2 \\frac{U_{j+1}^n - 2 U_j^n + U_{j-1}^n}{h^2} - r s_j \\frac{U_{j+1}^n - U_{j-1}^n}{2 h} + r U_j^n, U^{n+1}_j - \\phi(s_j))= 0, \\quad j = 1, \\ldots, J,\\quad n = 0, \\ldots, N-1. \\\\\nU_0^n = v_l(t_n), \\quad n = 0, \\ldots, N-1. \\\\\nU_{J+1}^n = v_r(t_n), \\quad n = 0, \\ldots, N-1.\n\\end{cases}\n\\]\nOn peut la r√©√©crire sous la forme matricielle afin d‚Äôextraire une solution num√©rique dite explicite : Sous forma matricielle, le sch√©ma s‚Äô√©crit :\n\\[\n\\begin{cases}\nmin(\\frac{U^{n+1} - U^n}{\\Delta t} +  A U^n + q(t_n), U^{n+1} - g) = 0, \\quad n = 0, \\ldots, N-1, \\\\\nU^0 = (\\phi(s_i))_{1 \\leq i \\leq J},\n\\end{cases}\n\\]\no√π - g est un vecteur de \\(\\mathbb{R}^J\\) d√©fini par \\(g_j = \\phi(s_j)\\) pour \\(j = 1, \\ldots, J\\),\n\n\\(A\\) est une matrice carr√©e tridiagonale de taille \\(J \\times J\\). En posant \\(\\alpha_j = \\frac{\\sigma^2}{2} \\frac{s_j^2}{h^2}\\) et \\(\\beta_j = r \\frac{s_j}{2 h}\\), les coefficients de la matrice \\(A\\) sont donn√©s par :\n\\[\n\\begin{cases}\nA_{j,j-1} = -\\alpha_j + \\beta_j, \\quad j= 2, \\ldots, J, \\\\\nA_{j,j} = 2\\alpha_j + r, \\quad j = 1, \\ldots, J, \\\\\nA_{j,j+1} = -\\alpha_j - \\beta_j, \\quad j = 1, \\ldots, J, \\\\\n\\end{cases}\n\\]\n\\(q(t_n)\\) un vecteur de \\(\\mathbb{R}^J\\) qui d√©pendent des param√®tres du mod√®le et de la discr√©tisation spatiale donn√© par :\n\\[\nq_j(t_n) =\n\\begin{cases}\n(-\\alpha_1 + \\beta_1) U_0^n, \\quad j = 1, \\\\\n0, \\quad j = 2, \\ldots, J-1, \\\\\n(-\\alpha_J + \\beta_J) U_{J+1}^n, \\quad j = J.\n\\end{cases}\n\\]\n\nDe fait, on obtient la relation de r√©currence explicite permettant de calculer \\(U^{n+1}\\) en fonction de \\(U^n\\) :\n\\[\nU^{n+1} = max(U^n - \\Delta t ( A U^n + q(t_n)), g)), \\quad n = 0, \\ldots, N-1,\n\\]"
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#implicit-euler-scheme",
    "href": "posts/m2mo/edp/edp_american_opt.html#implicit-euler-scheme",
    "title": "Finite difference Methods for American Options",
    "section": "Implicit Euler Scheme",
    "text": "Implicit Euler Scheme\nPour des raisons de stabilit√©, on peut √©galement utiliser un sch√©ma d‚ÄôEuler implicite, qui est bas√© sur une discr√©tisation implicite en temps.\n\nSplitting scheme\nUne premi√®re approche est de consid√©rer le ‚Äúsplitting scheme‚Äù, qui consiste √† s√©parer la partie lin√©aire de l‚ÄôEDP de la partie non lin√©aire : (i) On calcule \\(U^{n+1,(1)}\\) tel que \\(U^{n+1,(1)} - U^n + \\Delta t (A U^{n+1,(1)} + q(t_{n+1})) = 0\\), (ii) On calcule \\(U^{n+1}\\) tel que \\(U^{n+1} = max(U^{n+1,(1)}, g)\\).\n\n\nM√©thodes num√©riques de r√©solution\nUne seconde approche est de consid√©rer un sch√©ma d‚ÄôEuler implicite ‚Äúfully implicit‚Äù, qui consiste √† discr√©tiser l‚ÄôEDP de mani√®re implicite en temps, ce qui conduit √† un syst√®me non lin√©aire √† r√©soudre √† chaque pas de temps : \\[\\begin{cases}\nmin(\\frac{U^{n+1} - U^n}{\\Delta t} +  A U^{n+1} + q(t_{n+1}), U^{n+1} - g) = 0, \\quad n = 0, \\ldots, N-1, \\\\\nU^0 = (\\phi(s_i))_{1 \\leq i \\leq J}.\n\\end{cases}\\]\nEn posant \\(B = I + \\Delta t A\\) et \\(b = U^n - \\Delta t q(t_{n+1})\\), le probl√®me se transforme en la r√©solution d‚Äôun probl√®me d‚Äôobstacle √† chaque pas de temps ou il s‚Äôagit de trouver \\(x \\in \\mathbb{R}^J\\) tel que \\(min(B x - b, x - g) = 0\\). De fait, on a recours √† des m√©thodes num√©riques de r√©solution de ce type de probl√®me, telles que la m√©thode PSOR (projected successive over-relaxation) ou une m√©thode de type Newton.\n\nPSOR Algorithm\nLa m√©thode PSOR est une m√©thode it√©rative de r√©solution de probl√®mes d‚Äôobstacle. Elle consiste √† it√©rer sur les composantes du vecteur \\(x\\) en appliquant une relaxation projet√©e.\nTout d‚Äôabord, on d√©compose la matrice \\(B\\) en une partie triangulaire inf√©rieure \\(L\\), et une partie triangulaire strictement sup√©rieure \\(U\\), de sorte que \\(B = L + U\\). Ensuite, pour un temps fix√© \\(n\\), on cherche √† trouver une solution \\(x\\) du probl√®me d‚Äôobstacle en it√©rant sur les composantes du vecteur \\(x\\) au temps fix√© en appliquant la formule suivante : \\[\nx_j^{(k+1)} = max\\left(g_j, \\frac{\\omega}{B_{j,j}} \\left(b_j - \\sum_{i=1}^{j-1} B_{j,i} x_i^{(k+1)} - \\sum_{i=j+1}^{J} B_{j,i} x_i^{(k)} \\right) + (1 - \\omega) x_j^k \\right) , \\quad j = 1, \\ldots, J,\n\\]\no√π \\(k\\) est l‚Äôindice de l‚Äôit√©ration. La m√©thode converge vers la solution du probl√®me d‚Äôobstacle, et le taux de convergence d√©pend du choix du param√®tre de relaxation, i.e.¬†du choix de la valeur de \\(\\omega \\in (0,2)\\) dans la formule de relaxation. Lorsque \\(\\omega = 1\\), la m√©thode PSOR correspond √† la m√©thode classique de Gauss-Seidel projet√©e. Lorsque \\(\\omega \\neq 1\\), la m√©thode PSOR peut acc√©l√©rer la convergence, mais le choix optimal de \\(\\omega\\) d√©pend du probl√®me sp√©cifique et peut n√©cessiter une exp√©rimentation.\n\n\nSemi-smooth Newton‚Äôs method\nLa m√©thode de type Newton est une m√©thode it√©rative de r√©solution de probl√®mes non lin√©aires. Elle consiste √† lin√©ariser le probl√®me d‚Äôobstacle autour d‚Äôune solution approximative \\(x^{(k)}\\) √† chaque it√©ration, et √† r√©soudre le probl√®me lin√©aris√© pour obtenir une nouvelle approximation \\(x^{(k+1)}\\).\nOn cherche √† r√©soudre le probl√®me d‚Äôobstacle \\(F(x) = min(B x - b, x - g) = 0\\) en it√©rant sur les approximations successives de la solution. On lin√©arise le probl√®me d‚Äôobstacle autour de l‚Äôapproximation \\(x^{(k)}\\) en utilisant la formule de Taylor : \\[\nF(x) \\approx F(x^{(k)}) + F'(x^{(k)})(x - x^{(k)}),\n\\]\nOn cherche √† r√©soudre \\(F(x) = 0\\). D√®s lors, √† chaque it√©ration, on r√©sout le probl√®me lin√©aris√© \\(F'(x^{(k)})(x - x^{(k)}) = -F(x^{(k)})\\) pour obtenir une nouvelle approximation \\(x^{(k+1)}\\). La m√©thode de type Newton converge quadratiquement vers la solution du probl√®me d‚Äôobstacle, ce qui en fait une m√©thode tr√®s efficace pour r√©soudre ce type de probl√®me.\n\\(F'(x)\\) est la matrice jacobienne de \\(F\\) √† l‚Äôapproximation \\(x\\). La matrice jacobienne de \\(F\\) est donn√©e par : \\[\nF'(x)_{i,j} := \\begin{cases}\nB_{i,j}, \\quad \\text{si } B x - b &lt; x - g, \\\\\n\\delta_{i,j}, \\quad \\text{sinon}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#higher-order-schemes",
    "href": "posts/m2mo/edp/edp_american_opt.html#higher-order-schemes",
    "title": "Finite difference Methods for American Options",
    "section": "Higher order schemes",
    "text": "Higher order schemes\nEnfin, nous envisagerons de comparer les sch√©mas d‚ÄôEuler implicite, les sch√©mas de type Crank-Nicolson, et un sch√©ma d‚Äôordre 2 tel que le BDF2."
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#euler-explicit-scheme",
    "href": "posts/m2mo/edp/edp_american_opt.html#euler-explicit-scheme",
    "title": "Finite difference Methods for American Options",
    "section": "Euler Explicit Scheme",
    "text": "Euler Explicit Scheme\nNous √©tudions ici le comportement du sch√©ma d‚ÄôEuler explicite appliqu√© √† l‚Äô√©quation de Black‚ÄìScholes pour une option am√©ricaine de type put. Les simulations sont r√©alis√©es en faisant varier les param√®tres de discr√©tisation spatiale J et temporelle N, tout en imposant la contrainte d‚Äôobstacle √† chaque pas de temps.\nLes figures pr√©sent√©es correspondent aux cas suivants (pour les deux payoffs consid√©r√©s) :\n\nN = 20, J = 50\nN = 20, J = 20\nN = 50, J = 20\n\nPour chaque configuration, le prix num√©rique de l‚Äôoption est compar√© au payoff, et la valeur associ√©e √† la condition de CFL est calcul√©e.\n\nr_ = 0.1\nsigma_ = 0.3\nK_ = 100\nT_ = 1\nSmin_ = 50\nSmax_ = 250\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\n# Definition des param√®tres dans un dictionnaire\nparams = dict(\n    r=r_,\n    sigma=sigma_,\n    K=K_,\n    T=T_,\n    N=None, # Valeur √† d√©finir plus tard\n    J=None, # Valeur √† d√©finir plus tard\n    Smin=Smin_,\n    Smax=Smax_\n)\n\nParam√®tres financiers:\nr=0.10 sigma=0.30 K=100 T=1\n\n\n\nJ_values = [50, 20, 20]\nN_values = [20, 20, 50]\n\ncfl_records1 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, (N_, J_) in enumerate(zip(N_values, J_values)):\n    params['N'] = N_\n    params['J'] = J_\n\n    ee = SchemeEE(**params, payoff=1)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    #CFL condition\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    # Enregistrement dans la table\n    cfl_records1.append({\n        \"N\": N_,\n        \"J\": J_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = {N_}, J = {J_}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du put am√©ricain -- Scheme {ee.scheme_name}\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\nJ_values = [50, 20, 20]\nN_values = [20, 20, 50]\n\ncfl_records2 = []\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=False)\nfor j, (N_, J_) in enumerate(zip(N_values, J_values)):\n    params['N'] = N_\n    params['J'] = J_\n\n    ee = SchemeEE(**params, payoff=2)\n    U, t = ee.solve()\n    s = ee.s\n    dt = ee.dt\n\n    #CFL condition\n    CFL = dt / (ee.h ** 2) * (ee.sigma ** 2) * (ee.Smax ** 2)\n\n    # Enregistrement dans la table\n    cfl_records2.append({\n        \"N\": N_,\n        \"J\": J_,\n        \"CFL\": CFL\n    })\n\n    ax = axes[j]\n    ax.plot(s, U, label=\"Prix option\")\n    ax.plot(s, ee.phi(s), 'k--', label=\"Payoff\")\n\n    ax.set_title(f\"N = {N_}, J = {J_}\")\n    ax.set_xlabel(\"s\")\n    if j == 0:\n        ax.set_ylabel(\"u(t,s)\")\n    ax.legend()\n\nplt.suptitle(\n    f\"Evolution du prix du put am√©ricain -- Scheme {ee.scheme_name}, N fix√© √† {N_}\",\n    fontsize=16\n)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nLes figures ci - dessus repr√©sentent le prix num√©rique de l‚Äôoption au temps final, compar√© au payoff, pour diff√©rentes configurations de maillage.\nLes r√©sultats montrent que le comportement du sch√©ma d√©pend fortement du choix de N et J. Lorsque le maillage spatial est fin ( J=50) et que le nombre de pas de temps est relativement faible ( N=20), la solution num√©rique devient instable et pr√©sente de fortes oscillations, avec des valeurs non physiques. Ce ph√©nom√®ne traduit une instabilit√© du sch√©ma explicite. En revanche, pour un maillage spatial plus grossier ( J=20) et un pas de temps suffisamment petit ( N=20 ou N=50), la solution obtenue est plus r√©guli√®re et respecte correctement la contrainte d‚Äôoption am√©ricaine, la solution restant au-dessus du payoff.\nCes observations sugg√®rent que la stabilit√© du sch√©ma d‚ÄôEuler explicite est √©troitement li√©e √† la relation entre le pas de temps et le pas d‚Äôespace, ce qui motive l‚Äôanalyse de la condition de stabilit√© de type CFL.\n\ncfl_df1 = pd.DataFrame(cfl_records1)  # Cas 1 : N fix√©\ncfl_df2 = pd.DataFrame(cfl_records2)  # Cas 2 : N = J\n\ncfl_df1 = cfl_df1.rename(columns={\"CFL\": \"CFL (N=10)\"})\ncfl_df2 = cfl_df2.rename(columns={\"CFL\": \"CFL (N=J)\"})\n\nprint(\"Payoff standard:\")\nprint(\"=\"*20)\nprint(cfl_df1)\nprint(\"Payoff tronqu√©:\")\nprint(\"=\"*20)\nprint(cfl_df2)\n\nPayoff standard:\n====================\n    N   J  CFL (N=10)\n0  20  50   18.288281\n1  20  20    3.100781\n2  50  20    1.240312\nPayoff tronqu√©:\n====================\n    N   J  CFL (N=J)\n0  20  50  18.288281\n1  20  20   3.100781\n2  50  20   1.240312\n\n\nL‚Äô√©tude de la condition de CFL met en √©vidence le caract√®re conditionnellement stable du sch√©ma d‚ÄôEuler explicite. Comme observ√© sur les figures, lorsque le maillage spatial est raffin√© sans diminution suffisante du pas de temps, la condition de stabilit√© est viol√©e et la solution num√©rique pr√©sente des oscillations marqu√©es. √Ä l‚Äôinverse, lorsque N et J sont augment√©s conjointement, la condition de CFL est mieux respect√©e et le comportement du sch√©ma s‚Äôam√©liore.\nOn remarque par ailleurs que les valeurs calcul√©es de la CFL sont identiques pour le payoff standard et pour le payoff tronqu√©. Ce r√©sultat est attendu, puisque la condition de stabilit√© d√©pend uniquement des param√®tres num√©riques et du mod√®le, en particulier du pas de temps Œît, du pas d‚Äôespace h, de la volatilit√© œÉ et de la borne sup√©rieure Smax et non de la r√©gularit√© du payoff.\n\ndef get_convergence_table(N_grid, J_grid, params, Sval, scheme_class):\n\n    est_prices = []\n    errex = []\n    errors = []\n    cpu_times = []\n\n    for N, J in zip(N_grid, J_grid):\n        params['N'] = N\n        params['J'] = J\n\n        start = time.time()\n        scheme = scheme_class(**params)\n        U, _ = scheme.solve()\n        tcpu = time.time() - start\n\n        price_est = scheme.interpolate(Sval, U)\n\n        est_prices.append(price_est)\n        cpu_times.append(tcpu)\n\n    est_prices = np.array(est_prices)\n    errors = np.zeros(len(est_prices))\n    errors[1:] = np.abs(np.diff(est_prices))\n    cpu_times = np.array(cpu_times)\n\n    # Ordre de convergence global\n    alpha = np.zeros(len(errors))\n    h_step = (params[\"Smax\"] - params[\"Smin\"]) / (J_grid + 1)\n    alpha[1:] = np.where(\n        (errors[:-1] &gt; 0) & (errors[1:] &gt; 0) & (h_step[:-1] &gt; h_step[1:]),\n        np.log(errors[:-1] / errors[1:]) / np.log(h_step[:-1] / h_step[1:]),\n        0\n    )\n    df = pd.DataFrame({\n        \"J\": J_grid,\n        \"N\": N_grid,\n        \"U(s)\": est_prices,\n        \"error\": errors,\n        \"alpha\": alpha,\n        \"tcpu\": cpu_times\n    })\n\n    return df.round(6)\n\n\nSval = 90\nJ_grid = np.array([20, 40, 80, 160, 320])\n\nN_grid = np.array([2*(j**2)/10 for j in J_grid]).astype(int)\n\nprint(\"Cas N = J\\n\", \"=\"*75)\n\nprint(\"Convergence Table for Scheme EE:\")\nprint(get_convergence_table(N_grid, J_grid-1, params, Sval, SchemeEE))\n\nCas N = J\n ===========================================================================\nConvergence Table for Scheme EE:\n     J      N       U(s)     error     alpha      tcpu\n0   19     80  12.947098  0.000000  0.000000  0.000573\n1   39    320  13.064717  0.117619  0.000000  0.001415\n2   79   1280  13.109572  0.044855  1.390781  0.005432\n3  159   5120  13.117805  0.008233  2.445784  0.019646\n4  319  20480  13.119987  0.002183  1.915301  0.082647\n\n\nordre h^2 + dt ==&gt; division de l‚Äôerreur par 4.\n\n\nPour compl√©ter l‚Äôanalyse de la stabilit√©, il est √©galement int√©ressant d‚Äô√©tudier la convergence du sch√©ma. Pour √©tudier l‚Äôordre de convergence du sch√©ma, nous avons utilis√© une grille pour J qui varie en doublant √† chaque fois, et une grille pour N qui varie selon la r√®gle \\(2 \\times \\frac{J^2}{10}\\). Etant donn√© le fait que l‚Äôordre th√©orique de convergence du sch√©ma d‚ÄôEuler explicite pour l‚Äô√©quation de Black-Scholes est de 2 en espace et de 1 en temps, on s‚Äôattend √† ce que l‚Äôerreur diminue d‚Äôun facteur de 4 √† chaque √©volution de la grille. La table de convergence pr√©sent√©e ci-dessous montre l‚Äô√©volution de la solution num√©rique U(s) lorsque le nombre de points spatiaux J et le nombre de pas temporels N sont augment√©s.\nEn respectant la r√®gle de variation des grilles pour J et N, on s‚Äôassure que les deux contributions √† l‚Äôerreur, i.e.¬†l‚Äôerreur spatiale et l‚Äôerreur temporelle, diminuent de mani√®re coh√©rente, ce qui permet d‚Äôobserver une convergence plus r√©guli√®re du sch√©ma et donc un ordre de convergence plus clair.\nOn observe que l‚Äôerreur diminue progressivement au fur et √† mesure que le maillage spatial et temporel est raffin√©, tandis que le facteur Œ± (estimant l‚Äôordre de convergence en espace) tend vers la valeur th√©orique attendue (2), ce qui confirme que le sch√©ma d‚ÄôEuler explicite converge correctement sous la condition de CFL.\nLa colonne tcpu montre que ce raffinement s‚Äôaccompagne d‚Äôun co√ªt computationnel croissant, le temps de calcul augmentant rapidement lorsque le maillage spatial et temporel est affin√©."
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#schemas-implicites",
    "href": "posts/m2mo/edp/edp_american_opt.html#schemas-implicites",
    "title": "Finite difference Methods for American Options",
    "section": "Schemas implicites",
    "text": "Schemas implicites\nApr√®s avoir √©tudi√© le sch√©ma d‚ÄôEuler explicite, nous nous int√©ressons √† des sch√©mas implicites et semi-implicites (Euler implicite, Crank-Nicolson‚Ä¶). Ce passage est principalement motiv√© par le fait que le sch√©ma explicite est conditionnellement stable et impose une limitation relativement stricte sur le pas de temps Œît et le maillage via la condition CFL. Les sch√©mas implicites, eux, offrent une stabilit√© plus grande, permettant d‚Äôutiliser des pas de temps plus importants.\nDans le cas du sch√©ma d‚ÄôEuler implicite (et plus g√©n√©ralement pour tout sch√©ma implicite appliqu√© aux options am√©ricaines), on est conduit √† r√©soudre √† chaque pas de temps un syst√®me non lin√©aire discret, correspondant √† un probl√®me d‚Äôobstacle v‚â•œï. Pour traiter ce syst√®me, nous utilisons diverses m√©thodes num√©riques, telles que la m√©thode PSOR (Projected Successive Over-Relaxation) ou une m√©thode de type Newton semi-smooth.\nCes sch√©mas compl√©mentaires nous permettent donc de comparer pr√©cision, stabilit√© et co√ªt computationnel, tout en g√©rant correctement la contrainte inh√©rente aux options am√©ricaines.\n\nSplitting Euler Implicit scheme & Splitting Crank-Nicolson Scheme\nPour √©tudier le comportement du sch√©ma splitting d‚ÄôEuler implicite, nous nous sommes directement plac√©s dans le cas o√π N=20 et J=50, qui avait montr√© des oscillations dans le sch√©ma explicite. Les r√©sultats obtenus sont pr√©sent√©s dans le graphique ci-dessous. Comme on peut le constater, le sch√©ma d‚ÄôEuler implicite produit une approximation stable et sans oscillations du prix du put europ√©en, m√™me pour des valeurs √©lev√©es de N et J. Cela confirme la stabilit√© inconditionnelle du sch√©ma implicite, qui ne d√©pend pas de la relation entre le pas de temps et le pas d‚Äôespace.\n\nparams['J'] = 20\nparams['N'] = 50\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.30 K=100 T=1\nParam√®tres num√©riques:\nJ=20 N=50\n\n\n\nei_split = SplittingScheme(**params)\nU,t = ei_split.solve()\ns = ei_split.s\ndt = ei_split.dt\n\nplt.plot(s,U,label=\"t=%.2f\" %(t+dt))\nplt.plot(s,ei_split.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put au cours du temps [Splitting Euler Implicite]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLe sch√©ma de splitting d‚ÄôEuler implicite est un sch√©ma implicite qui permet de s√©parer la partie lin√©aire de l‚ÄôEDP de la partie non lin√©aire. En appliquant ce sch√©ma, on observe, ci dessus, une √©volution du prix du put au cours du temps qui est plus stable que celle obtenue avec le sch√©ma d‚ÄôEuler explicite.\nBien qu‚Äôils puissent √™tre moins pr√©cis qu‚Äôun vrai sch√©ma d‚ÄôEuler implicite ‚Äúfully implicit‚Äù, les sch√©mas de splitting d‚ÄôEuler implicite et de splitting de Crank-Nicolson sont plus simples √† impl√©menter et peuvent offrir une bonne approximation du prix du put am√©ricain, tout en √©tant plus stables que le sch√©ma d‚ÄôEuler explicite. Par ailleurs, en ce qui concerne les ordres de convergence, les sch√©mas de splitting d‚ÄôEuler implicite et de splitting de Crank-Nicolson peuvent pr√©senter des ordres de convergence similaires √† ceux des sch√©mas d‚ÄôEuler implicite et de Crank-Nicolson classiques.\n\nN_grid = J_grid = np.array([20*(2**k) for k in range(5)]).astype(int)\n\nprint(\"Splitting scheme\")\nprint(\"=\"*75, \"\\nConvergence Table for Scheme EI:\")\nprint(get_convergence_table(N_grid, J_grid-1, params, Sval, SplittingScheme))\n\nprint(\"=\"*75, \"\\nConvergence Table for Scheme CN:\")\nprint(get_convergence_table(N_grid, J_grid-1, params, Sval, SchemeCN))\n\nSplitting scheme\n=========================================================================== \nConvergence Table for Scheme EI:\n     J    N       U(s)     error     alpha      tcpu\n0   19   20  12.771151  0.000000  0.000000  0.001442\n1   39   40  12.958929  0.187778  0.000000  0.001753\n2   79   80  13.054834  0.095904  0.969364  0.004257\n3  159  160  13.089790  0.034957  1.456030  0.010195\n4  319  320  13.105599  0.015809  1.144811  0.026634\n=========================================================================== \nConvergence Table for Scheme CN:\n     J    N       U(s)     error     alpha      tcpu\n0   19   20  12.888852  0.000000  0.000000  0.000767\n1   39   40  13.021900  0.133049  0.000000  0.001050\n2   79   80  13.090984  0.069084  0.945528  0.002674\n3  159  160  13.107807  0.016823  2.037952  0.006654\n4  319  320  13.114979  0.007172  1.229912  0.019724\n\n\n\n\nPSOR Algorithm\nComme attendu, le sch√©ma PSOR prend plus de temps √† converger que les sch√©mas d‚ÄôEuler implicite et de Crank-Nicolson, en raison de la nature it√©rative de la m√©thode PSOR. Cependant, il offre une bonne approximation du prix du put am√©ricain, tout en √©tant plus stable que le sch√©ma d‚ÄôEuler explicite.\n\nparams['J'] = 100 -1\nparams['N'] = 10\n\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.30 K=100 T=1\nParam√®tres num√©riques:\nJ=99 N=10\n\n\n\npsor = PSOR(**params, payoff=1)\nU,t = psor.solve() # omega = 1.5 pour une convergence rapide\ns = psor.s\ndt = psor.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t))\nplt.plot(s,psor.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put europ√©en au cours du temps [PSOR]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCependant, elle peut √™tre acc√©l√©r√©e en choisissant un param√®tre de relaxation \\(\\omega\\) appropri√©, ce qui peut r√©duire le nombre d‚Äôit√©rations n√©cessaires pour atteindre la convergence. Nous constatons que pour des valeurs de \\(\\omega\\) proches de 1.5, la m√©thode PSOR converge plus rapidement que pour des valeurs plus proches de 1, ce qui sugg√®re que l‚Äôover-relaxation peut √™tre b√©n√©fique pour acc√©l√©rer la convergence de la m√©thode PSOR.\n\n# comparing time\n\nomegas = [1.2, 1.5, 1.8, 1.9]\n\nfor omega in omegas:\n    psor = PSOR(**params, payoff=1)\n    start_time = time.time()\n    U, t = psor.solve(omega=omega)\n    end_time = time.time()\n    print(f\"Omega: {omega}, Time taken: {end_time - start_time:.4f} seconds\")\n\nOmega: 1.2, Time taken: 50.6897 seconds\nOmega: 1.5, Time taken: 44.9621 seconds\nOmega: 1.8, Time taken: 27.1073 seconds\nOmega: 1.9, Time taken: 50.4578 seconds\n\n\n\n\nSemi-smooth Newton‚Äôs method\nEn g√©n√©ral, la m√©thode PSOR est une m√©thode efficace pour r√©soudre les probl√®mes d‚Äôobstacle associ√©s aux options am√©ricaines, mais elle peut √™tre moins rapide que les m√©thodes de type Newton pour des probl√®mes de grande taille ou pour des sch√©mas d‚Äôordre sup√©rieur.\nPour ce faire, nous avons impl√©ment√© une m√©thode de type Newton pour r√©soudre le probl√®me d‚Äôobstacle √† chaque pas de temps. Cette m√©thode consiste √† lin√©ariser le probl√®me d‚Äôobstacle autour d‚Äôune solution approximative √† chaque it√©ration, et √† r√©soudre le probl√®me lin√©aris√© pour obtenir une nouvelle approximation.\n\nparams['J'] = 100 -1\nparams['N'] = 10\n\n\nprint(\"Param√®tres financiers:\")\nprint(\"r=%.2f\" %r_, \"sigma=%.2f\" %sigma_, \"K=%.0f\" %K_, \"T=%.0f\" %T_)\n\nprint(\"Param√®tres num√©riques:\")\nprint(\"J=%.0f\" %params['J'], \"N=%.0f\" %params['N'])\n\nParam√®tres financiers:\nr=0.10 sigma=0.30 K=100 T=1\nParam√®tres num√©riques:\nJ=99 N=10\n\n\n\nnewtonss = NewtonSemiSmooth(**params, payoff=1)\nU,t = newtonss.solve()\ns = newtonss.s\ndt = newtonss.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t))\nplt.plot(s,newtonss.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put europ√©en au cours du temps [Newton Semi-Smooth]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMini-conclusion sur les sch√©ma implicites\nL‚Äô√©tude des sch√©mas implicites et semi-implicites montre que, lorsque le maillage spatial J et le pas temporel N sont raffin√©s, toutes les m√©thodes test√©es produisent des solutions num√©riques de plus en plus pr√©cises. Les ordres de convergence estim√©s (Œ±) sont coh√©rents avec les pr√©dictions th√©oriques : proches de 1 pour Euler implicite et proches de 2 pour Crank-Nicolson.\nLe raffinement du maillage et du pas de temps s‚Äôaccompagne d‚Äôune augmentation notable du temps de calcul, ce que mettent en √©vidence les tables de convergence et qui illustre clairement le compromis entre pr√©cision et co√ªt computationnel.\nEn termes de qualit√© des solutions, les m√©thodes num√©riques explor√©es (PSOR et Newton semi-smooth) produisent des r√©sultats lisses et coh√©rents, respectant correctement la contrainte d‚Äôobstacle."
  },
  {
    "objectID": "posts/m2mo/edp/edp_american_opt.html#higher-order-schemes-1",
    "href": "posts/m2mo/edp/edp_american_opt.html#higher-order-schemes-1",
    "title": "Finite difference Methods for American Options",
    "section": "Higher order schemes",
    "text": "Higher order schemes\nApr√®s avoir √©tudi√© les sch√©mas implicites classiques et les m√©thodes de r√©solution pour le probl√®me d‚Äôobstacle, nous consid√©rons √©galement un sch√©ma d‚Äôordre 2, le BDF2, qui am√©liore la pr√©cision temporelle tout en prenant en compte l‚Äôobstacle.\n\nBDF Scheme\nLe sch√©ma BDF2 est test√© avec les m√™mes param√®tres de maillage spatial et de pas de temps que les sch√©mas pr√©c√©dents. Le graphique ci-dessous montre que le prix du put europ√©en reste lisse et coh√©rent, respectant correctement la contrainte d‚Äôobstacle, de mani√®re similaire aux autres m√©thodes implicites √©tudi√©es.\n\nparams['J'] = 100 -1\nparams['N'] = 10\n\nbdf2 = BdfScheme(**params, payoff=1)\nU,t = bdf2.solve()\ns = bdf2.s\ndt = bdf2.dt\n\nplt.figure(figsize=(6, 5))\nplt.plot(s,U,label=\"t=%.2f\" %(t))\nplt.plot(s,bdf2.phi(s), 'k--', label=\"payoff\")\nplt.xlabel(\"s\")\nplt.ylabel(\"u(t,s)\")\nplt.title(\"Evolution du prix du put europ√©en au cours du temps [BDF2]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheryl Kouadio, Msc",
    "section": "",
    "text": "Check out the latest ¬†Published articles¬†, ¬†M2MO¬†, ¬†Ensai¬†, and ¬†More ¬ª\n\n\n\n\n\n\n\n\n\n\n\nTerm structure interest rate modelling\n\n\n\nFeb 21, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Markets\n\n\n\nFeb 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDP in finance\n\n\n\nFeb 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Markets\n\n\n\nFeb 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDP in finance\n\n\n\nJan 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nAll Posts ¬ª"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Cheryl Kouadio, Msc",
    "section": "",
    "text": "Check out the latest ¬†Published articles¬†, ¬†M2MO¬†, ¬†Ensai¬†, and ¬†More ¬ª\n\n\n\n\n\n\n\n\n\n\n\nTerm structure interest rate modelling\n\n\n\nFeb 21, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Markets\n\n\n\nFeb 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDP in finance\n\n\n\nFeb 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Markets\n\n\n\nFeb 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDP in finance\n\n\n\nJan 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nAll Posts ¬ª"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Cheryl Kouadio, Msc",
    "section": "About me",
    "text": "About me\n\n  \n    \n\n    \n  \n    \n     E-mail\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \nI am a graduate student in statistics and quantitative modeling, with academic and professional interests at the intersection of probability theory, stochastic modeling, data analysis, and financial risk management. I am currently completing a Master‚Äôs degree in Mod√©lisation Al√©atoire et Finance Quantitative (M2MO) at Universit√© Paris Cit√©, a selective program focused on advanced probability, stochastic processes, numerical methods, and applications to finance and risk.\nMy academic training is grounded in mathematical statistics and applied data science. I previously studied Statistics and Decision Informatics, where I developed a strong foundation in statistical inference, multivariate analysis, time series, optimization, and programming for data analysis. Throughout my studies, I have built solid technical skills in Python, R, SAS, and SQL, and I regularly use data-visualization tools to communicate quantitative results clearly and effectively.\nAlongside my academic training, I have gained hands-on experience in applied statistics and data analysis through professional assignments in institutional and administrative environments. My work has involved data cleaning and processing, statistical reporting, automation of analyses, and the development of dashboards to support data-driven decision-making. These experiences have strengthened my ability to bridge rigorous statistical theory with real-world applications.\nMy current academic focus centers on stochastic processes, probabilistic modeling, numerical methods for finance, and quantitative risk analysis. I am particularly interested in how mathematically sound models can be used to quantify uncertainty, assess financial risk, and support regulatory and strategic decision frameworks. I place strong emphasis on methodological rigor, model validation, and the transparent interpretation of results.\nMore broadly, I am building a profile oriented toward quantitative finance, risk modeling, and advanced statistical research, combining strong theoretical foundations with practical implementation skills. My goal is to use mathematics and statistics as tools to better understand complex systems, manage risk, and produce reliable and interpretable quantitative analyses."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Cheryl Kouadio, Msc, Subodh Selukar, PhD, Megan Othus, PhD, and Sylvie Chevret, MD, PhD, 2025 Detecting the Cure Model Appropriateness in Randomized Clinical Trials With Long-Term Survivors. JCO Clinical Cancer Informatics 9: e210084, 2025."
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Cheryl Kouadio, Msc, Subodh Selukar, PhD, Megan Othus, PhD, and Sylvie Chevret, MD, PhD, 2025 Detecting the Cure Model Appropriateness in Randomized Clinical Trials With Long-Term Survivors. JCO Clinical Cancer Informatics 9: e210084, 2025."
  },
  {
    "objectID": "publications.html#report",
    "href": "publications.html#report",
    "title": "Publications",
    "section": "Report¬†",
    "text": "Report¬†\nKouadio Cheryl, Kougoum Moko Mani Marilene, and Ouyassin Mariyam, 2025 Impl√©mentation de la VaR √† l‚Äôaide de la th√©orie des valeurs extr√™mes et comparaison des m√©thodes d‚Äôimpl√©mentation.\nKouadio Cheryl, and Wane Mountaga, 2025 Stress testing du CAC 40 √† l‚Äôaide de la th√©oried des valeurs extr√™mes.\nMathis Bouillon ,Kouadio Cheryl, Kougoum Moko Mani Marilene, Ouyassin Mariyam, and Wane Mountaga, 2025 Stock price predictions using time series.\nKouadio Cheryl, and Ouyassin Mariyam, 2025 Introduction aux mod√®les de courbe de taux.\nKouadio Cheryl, and Ouyassin Mariyam, 2025 Mod√©lisation des risques multiples √† l‚Äôaide de la th√©orie des copules.\nKouadio Cheryl, 2024 Understand better econometrics applied to microeconomy."
  },
  {
    "objectID": "publications.html#applications-launched",
    "href": "publications.html#applications-launched",
    "title": "Publications",
    "section": "Applications launched",
    "text": "Applications launched\nStock price prediction : CAC40 streamlit app\nAsset pricing and management github project"
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html",
    "href": "notes/new-york-state-climate-policy.html",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "",
    "text": "New York State is recognized as a climate leader, yet navigating its policies can be challenging without a deep understanding of the policymaking landscape. Here, I‚Äôve aimed to link key policy goals with relevant legislation, agencies, and provide direct access to the original legislative text. I hope you find these resources helpful.\nNotes: ECL: Environmental Conservation Law; Emission reductin baseline: 1990 level; Renewable energy: solar thermal, PV, land and offshore wind, hydroelctric, geothermal electric, geothermal ground source heat, tidal energy, wave energy, ocean thermal, and fuel cells which do not utilize a fossil fuel resource in the process1; ZEV(zero emission vehicles): Battery Electric Vehicles, Fuel Cell Vehicles, Plug-in Hybrid Electric Vehicles (PHEV)2.\nUpdates: Distributed PV: On October 17, 2024, Governor Kathy Hochul announced that 6 gigawatts (GW) of distributed solar have been installed across New York, a year ahead of schedule3. Storage: On June 20, 2024, the New York Public Service Commission approved the Order Establishing Updated Energy Storage Goal and Deployment Policy which expands the State‚Äôs goal to 6GW of energy storage to be installed by 20304."
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#new-york-state-climate-leadership-and-community-protection-act",
    "href": "notes/new-york-state-climate-policy.html#new-york-state-climate-leadership-and-community-protection-act",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "New York State Climate Leadership and Community Protection Act",
    "text": "New York State Climate Leadership and Community Protection Act\n\nGHG emissions goals\n\n2030: Reduce GHG by 40% below 1990 levels\n\n2050: Reduce GHG by 85% below 1990 levels\n\n\n¬ß 75-0107. Statewide greenhouse gas emissions limits.\n1. No later than one year after the effective date of this article, the department shall, pursuant to rules and regulations promulgated after at least one public hearing, establish a statewide greenhouse gas emissions limit as a percentage of 1990 emissions, as estimated pursuant to section 75-0105 of this article, as follows:\na. 2030: 60% of 1990 emissions.\nb. 2050: 15% of 1990 emissions.\n2. Greenhouse gas emission limits shall be measured in units of carbon dioxide equivalents and identified for each individual type of green-house gas.\n\n\n\nPower sector and renewable goals\n\n2030: Generate 70% of electricity from renewable energy sources\n\n2040: Achieve 100% zero-emission electricity\n\n\n¬ß 66-p.¬†Establishment of a renewable energy program.\n1. As used in this section:\n(a) ‚Äújurisdictional load serving entity‚Äù means any entity subject to the jurisdiction of the commission that secures energy to serve the electrical energy requirements of end-use customers in New York state;\n(b) ‚Äúrenewable energy systems‚Äù means systems that generate electricity or thermal energy through use of the following technologies: solar thermal, photovoltaics, on land and offshore wind, hydroelectric, geothermal electric, geothermal ground source heat, tidal energy, wave energy, ocean thermal, and fuel cells which do not utilize a fossil fuel resource in the process of generating electricity.\n2. No later than June thirtieth, two thousand twenty-one, the commission shall establish a program to require that: (a) a minimum of seventy percent of the state wide electric generation secured by jurisdictional load serving entities to meet the electrical energy requirements of all end-use customers in New York state in two thousand thirty shall be generated by renewable energy systems; and (b) that by the year two thousand forty (collectively, the ‚Äútargets‚Äù) the statewide electrical demand system will be zero emissions.\n\n\nTechnological specific goals:\n\n2025: Install 6 GW solar PV\n\n2030: Install 3 GW energy storage\n\n2035: Install 9 GW of offshore wind\n\n\n\n\nNo later than July first, two thousand twenty-four, the commission shall establish programs to require the procurement by the state‚Äôs load serving entities of at least nine gigawatts of offshore wind electricity generation by two thousand thirty-five and six gigawatts of photovoltaic solar generation by two thousand twenty-five, and to support three giga-watts of statewide energy storage capacity by two thousand thirty.\n\n\n\n\nWhat also in the bill?\n\nEstablish the New York state climate action council (‚Äúcouncil‚Äù)\nAt least 35% of the benefits directed to disadvantaged communities with a goal to achieve 40%\n\n\n75-0117. Investment of funds.\nState agencies, authorities and entities, in consultation with the environmental justice working group and the climate action council, shall, to the extent practicable, invest or direct available and relevant programmatic resources in a manner designed to achieve a goal for disadvantaged communities to receive forty percent of overall benefits of spending on clean energy and energy efficiency programs, projects or investments in the areas of housing, workforce development, pollution reduction, low income energy assistance, energy, transportation and economic development, provided however, that disadvantaged communities shall receive no less than thirty-five percent of the overall benefits of spending on clean energy and energy efficiency programs, projects or investments and provided further that this section shall not alter funds already contracted or committed as of the effective date of this section.\n\nSource Document: Senate Bill S6599"
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#amendaments-to-envirnmental-coservation-law",
    "href": "notes/new-york-state-climate-policy.html#amendaments-to-envirnmental-coservation-law",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "Amendaments to Envirnmental Coservation Law",
    "text": "Amendaments to Envirnmental Coservation Law\n\nEV sales goals\n\n2035: 100% new sale of passager cars and trucks shall be zero emission\n2045: 100% new sale of medium-duty and heavy-duty vehicles shall be zero emission\n\n\nSection 1. The environmental conservation law is amended by adding a new section 19-0306-b to read as follows:\n¬ß 19-0306-b. Zero-emissions cars and trucks.\n1. It shall be a goal of the state that one hundred percent of new passenger cars and trucks offered for sale or lease, or sold, or leased, for registration in the state shall be zero-emissions by two thousand thirty-five. It shall be a further goal of the state that one hundred percent of medium-duty and heavy-duty vehicles offered for sale or lease, or sold, or leased, for registration in the state be zero-emissions by two thousand forty-five for all operations where feasible. It shall be further a goal of the state to transition to one hundred percent zero-emissions off-road vehicles and equipment by two thousand thirty-five where feasible.\n\nSource Document: Senate Bill S2758\nNote: This is an ongoing policy issue, more updates are expected."
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#footnotes",
    "href": "notes/new-york-state-climate-policy.html#footnotes",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nysenate.gov/legislation/bills/2019/S6599‚Ü©Ô∏é\nhttps://dec.ny.gov/environmental-protection/air-quality/controlling-motor-vehicle-pollution/low-and-zero-emission-vehicles‚Ü©Ô∏é\nhttps://www.nyserda.ny.gov/About/Newsroom/2024-Announcements/2024-10-17-Governor-Hochul-Announces-New-York-State-Has-Achieved-Major-Solar-Milestone‚Ü©Ô∏é\nhttps://www.nyserda.ny.gov/All-Programs/Energy-Storage-Program‚Ü©Ô∏é"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What‚Äôs New & Updated",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroducing the Vasicek model\n\n\nTerm structure interest rate modelling\n\n\n\n\n\n\n\n\nFeb 21, 2026\n\n\nCheryl KOUADIO\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical foundations of financial market modeling\n\n\nFinancial Markets\n\n\n\n\n\n\n\n\nFeb 10, 2026\n\n\nCheryl KOUADIO\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFinite difference Methods for American Options\n\n\nEDP in finance\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\nCheryl KOUADIO, Marilene Kougoum\n\n\n39 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to financial markets\n\n\nFinancial Markets\n\n\n\n\n\n\n\n\nFeb 1, 2026\n\n\nCheryl KOUADIO\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nFinite difference Methods for European Options\n\n\nEDP in finance\n\n\n\n\n\n\n\n\nJan 14, 2026\n\n\nCheryl KOUADIO, Marilene Kougoum\n\n\n40 min\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting the Cure Model Appropriateness in Randomized Clinical Trials With Long-Term Survivors\n\n\n\n\n\nTo evaluate the appropriateness of a cure model when analyzing right-censored end points of a randomized clinical trial (RCT) in malignancy in the presence of long-term survivors. We aim to derive how the ratio estimation of censored cured subjects (RECeUS), previously proposed for a homogeneous population, could be extended for use in RCTs.\n\n\n\n\n\nDec 15, 2025\n\n\nCheryl Kouadio, Msc, Subodh Selukar, PhD, Megan Othus, PhD, and Sylvie Chevret, MD, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nRisque de valorisation d‚Äôobligation\n\n\nRisque de cr√©dit\n\n\n\n\n\n\n\n\nMar 23, 2025\n\n\nCheryl KOUADIO\n\n\n33 min\n\n\n\n\n\n\n\n\n\n\n\n\nProjet de gestion de risques multiples\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2025\n\n\nCheryl Kouadio, Mariyam Ouyassin\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nMod√®les de courbe de taux\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\n43 min\n\n\n\n\n\n\n\n\n\n\n\n\nTP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nCheryl Kouadio\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nTP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nCheryl Kouadio\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nTP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nCheryl Kouadio\n\n\n37 min\n\n\n\n\n\n\n\n\n\n\n\n\nAPF filter\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\n-Cheryl Kouadio -Safa Bouzayene -Mariyam Ouyassin\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nCorr√©lation de Spearman vs corr√©lation de Pearson\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nCheryl KOUADIO\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le mod√®le d‚ÄôHeston\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque de march√© en asset management\n\n\nRisque de cr√©dit\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nCheryl KOUADIO\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nRisques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)\n\n\nRisque de cr√©dit\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nCheryl KOUADIO\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le Black-Scholes\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFeatures selection\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\nCheryl KOUADIO\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nCheryl KOUADIO\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Trick and SVM\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nCheryl KOUADIO\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nRidge regression vs.¬†Lasso regression\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\nCheryl KOUADIO\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nLa r√©glementation prudentielle\n\n\nRisque de cr√©dit\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nCheryl KOUADIO\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nInvestissement Socialement Responsable (ISR): De quoi parle-t-on ?\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nCheryl KOUADIO\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu‚Äôest ce que c‚Äôest ?\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nCheryl Kouadio\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise\n\n\nAnalyse financi√®re\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nCheryl KOUADIO\n\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nCheryl Kouadio\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nCheryl Kouadio\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nInvestissement responsable\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nCheryl KOUADIO\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "glossary/m2mo/calcul_sto.html",
    "href": "glossary/m2mo/calcul_sto.html",
    "title": "Calcul stochastique",
    "section": "",
    "text": "Sauts et mod√®les avanc√©s\n\nProcessus de Poisson compos√©\n\nProcessus de sauts obtenu par sommation de tailles al√©atoires aux temps de Poisson.\n\nMesure de Poisson\n\nMesure al√©atoire comptant les sauts d‚Äôun processus.\n\nCompensateur\n\nTerme moyen rendant une mesure de saut martingale apr√®s compensation.\n\nIt√¥ avec sauts\n\nExtension de la formule d‚ÄôIt√¥ aux processus discontinus.\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGlossary - Derivatives market\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo/EDP\n\n\n\n\n\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html",
    "title": "Types of interest rates and rates derivatives",
    "section": "",
    "text": "The interest rate market is one of the largest and most liquid segments of the financial markets.\nIt encompasses a wide range of instruments, including government bonds, corporate bonds, interest rate swaps, and options on interest rates. The modeling of interest rates is crucial for pricing and managing the risks associated with these instruments. To understand the dynamics of interest rates, we first introduce the different types of interest rates that are used in creating and pricing interest rate derivatives, as well as the concept of the yield curve, which is a fundamental tool for analyzing the term structure of interest rates. Moreover, we introduce several interest rate derivatives currently traded in the market."
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#zero-coupon-bond-price-and-simple-spot-interest-rate",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#zero-coupon-bond-price-and-simple-spot-interest-rate",
    "title": "Types of interest rates and rates derivatives",
    "section": "Zero-coupon bond price and simple spot interest rate",
    "text": "Zero-coupon bond price and simple spot interest rate\nAssume we pay \\(B(t,T)\\) at time \\(t\\) to buy a zero-coupon bond that will pay \\(1\\) at maturity \\(T\\). Hence, \\(B(t,T) \\mapsto 1\\) at maturity \\(T\\). Alternatively, we can invest \\(B(t,T)\\) in the money market account that pays the simple spot rate \\(L(t,T)\\). At maturity \\(T\\), we obtain \\(B(t,T)(1 + (T-t)L(t,T))\\). Since we are in a no-arbitrage framework, we must have the same payoff at maturity \\(T\\) for both strategies, which implies \\(1 = B(t,T)(1 + (T-t)L(t,T))\\). Hence, we deduce the following link between the zero-coupon bond price and the simple spot interest rate:\n\\[\nL(t,T) = \\frac{1}{T-t} \\left( \\frac{1}{B(t,T)} - 1 \\right).\n\\]\nIn continuous compounding, we have \\(1 = B(t,T) e^{(T-t) R(t,T)}\\), which gives\n\\[\nR(t,T) = -\\frac{1}{T-t} \\ln(B(t,T)).\n\\]\nTherefore, the short-term interest rate is defined as \\(\\lim_{T \\to t} L(t,T) = \\lim_{T \\to t} R(t,T) = r_t\\).\n\n\n\n\n\n\nThe yield curve or the term structure of interest rates\n\n\n\nThe yield curve is a graphical representation of the relationship between interest rates and maturities. It is the function \\(T \\mapsto L(t,T)\\) for a given date \\(t\\). For each date \\(t\\), we can build a yield curve using market instruments (bonds, FRA, swaps, etc.) and use it to price interest rate derivatives."
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#zero-coupon-bond-price-and-simple-forward-interest-rate",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#zero-coupon-bond-price-and-simple-forward-interest-rate",
    "title": "Types of interest rates and rates derivatives",
    "section": "Zero-coupon bond price and simple forward interest rate",
    "text": "Zero-coupon bond price and simple forward interest rate\nThe forward rate is the rate that can be locked at time \\(t\\) for a loan starting at date \\(T\\) and ending at date \\(S\\). At time \\(t\\), one can invest in a zero-coupon bond maturing at \\(S\\) and obtain \\(B(t,S) \\mapsto 1\\) at maturity \\(S\\). Alternatively, one can invest in a zero-coupon bond maturing at \\(T\\), obtain \\(B(t,T) \\mapsto 1\\) at time \\(T\\), and then reinvest the proceeds from \\(T\\) to \\(S\\) at the forward rate \\(L(t;T,S)\\). At maturity \\(S\\), this yields \\(1 + (S-T)L(t;T,S)\\). Under no-arbitrage, we must have \\(B(t,S) = B(t,T)\\frac{1}{1 + (S-T)L(t;T,S)}\\). Hence, \\[\nL(t; T,S) = \\frac{1}{S-T} \\left( \\frac{B(t,T)}{B(t,S)} - 1 \\right).\n\\]\nThe instantaneous forward rate satisfies \\(f(t,T) = -\\frac{\\partial}{\\partial T} \\ln B(t,T)\\). Moreover,\\(B(t,T) = e^{-\\int_t^T f(t,u) du}.\\)"
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#fra",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#fra",
    "title": "Types of interest rates and rates derivatives",
    "section": "FRA",
    "text": "FRA\nA FRA (Forward Rate Agreement) is a contract that fixes at time \\(t\\) an interest rate \\(K\\) for the period \\([T,S]\\) on a nominal \\(N\\). Its payoff at time \\(S\\) is\n\\[\nN(S-T)(L(T,S) - K).\n\\]\nIts price at time \\(t\\) is\n\\[\n\\Pi^{\\text{FRA}}(t,T,S,K,N) = N B(t,S) (S-T) (L(t;T,S) - K).\n\\]"
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#swap",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#swap",
    "title": "Types of interest rates and rates derivatives",
    "section": "Swap",
    "text": "Swap\nA swap exchanges fixed and floating interest rates between two counterparties over dates \\(0 \\le T_0 &lt; T_1 &lt; \\dots &lt; T_n\\) on a nominal \\(N\\). The fixed leg pays \\(N R (T_i - T_{i-1})\\). The floating leg pays \\(N L(T_{i-1},T_i)(T_i - T_{i-1})\\). The swap can be seen as a sum of FRA. Its price is\n\\[\n\\begin{align}\n\\Pi^{\\text{SW}} (t,T_0,T_n,R,N)\n&= \\sum_{i=1}^n \\Pi^{\\text{FRA}} (t,T_{i-1},T_i,R,N) \\\\\n&= N \\left(B(t,T_0) - B(t,T_n) - \\sum_{i=1}^n B(t,T_i)(T_i - T_{i-1})R \\right)\n\\end{align}\n\\]\n\n\n\n\n\n\nSwap rate\n\n\n\nThe swap rate \\(S(t,T_0,T_n)\\) is the rate that makes the swap price equal to zero:\n\\[\nS(t,T_0,T_n) = \\frac{B(t,T_0) - B(t,T_n)}{\\sum_{i=1}^n B(t,T_i)(T_i - T_{i-1})}.\n\\]"
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#capfloor",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#capfloor",
    "title": "Types of interest rates and rates derivatives",
    "section": "Cap/Floor",
    "text": "Cap/Floor\nA caplet (resp. floorlet) is an option that gives the right to the buyer to receive (resp. pay) the excess of the floating rate \\(L(T_{i-1},T_i)\\) over a strike \\(K\\) on a nominal \\(N\\). The caplet payoff is\n\\[\nN (T_i - T_{i-1}) \\max(L(T_{i-1},T_i) - K, 0).\n\\]\nThe floorlet payoff is\n\\[\nN (T_i - T_{i-1}) \\max(K - L(T_{i-1},T_i), 0).\n\\]\nHence a cap is a chain of caplets for \\(i = 1, \\dots, n\\), and a floor is a chain of floorlets for \\(i = 1, \\dots, n\\).\n\n\n\n\n\n\nCaplet/Floorlet and Cap/Floor parity\n\n\n\nThere exists a parity called the caplet‚Äìfloorlet parity.\nIt means that a long caplet and a short floorlet with the same strike \\(K\\) and maturity structure is equivalent to entering a payer interest rate swap with fixed rate \\(K\\).\nMathematically,\n\\[\n\\begin{align}\n\\Pi^{\\text{Caplet}} (t,T,T + \\delta,K,1)-\\Pi^{\\text{Floorlet}} (t,T,T + \\delta,K,1) &=\\Pi^{\\text{FRA}} (t,T,T + \\delta,K,1).\\\\\n\\Leftrightarrow\n\\Pi^{\\text{Cap}} (t,T,T + \\delta,K,1)-\\Pi^{\\text{Floor}} (t,T,T + \\delta,K,1) &=\\Pi^{\\text{SW}} (t,T,T + \\delta,K,1).\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/m2mo/yield_curves/00_yield_curve.html#swaption",
    "href": "posts/m2mo/yield_curves/00_yield_curve.html#swaption",
    "title": "Types of interest rates and rates derivatives",
    "section": "Swaption",
    "text": "Swaption\nA swaption is an option based on interest rate swap of maturity \\(T\\) and fixed rate \\(R\\). Assuming \\(T=T_0\\) is the starting date of the swap, the payoff of a swaption is \\[\n\\begin{align}\n( \\Pi^{\\text{SW}} (T_0,T_0,T_n,R,N) )_+ &= N(1 -\\sum_{i=1}^n c_i B(T_0,T_i))_+ (1)\\\\\n&= N \\sum_{i=1}^n B(T_0,T_i) (T_i,T_{i-1}) ( S(T_0,T_0,T_n) - R)_+ .\n\\end{align}\n\\]\nThus, the swaption can be seen as a put option on a classical bond with coupon \\(c_i\\)"
  },
  {
    "objectID": "posts/m2mo/yield_curves/01_term_structure.html",
    "href": "posts/m2mo/yield_curves/01_term_structure.html",
    "title": "Introducing the Vasicek model",
    "section": "",
    "text": "An interest rate, also referred as the term structure, represents the cost of borrowing money from another counterparty, for example a bank. It reflects the price paid for the use of capital over time. Interest rates also embed market expectations. For instance, when interest rates are low, lenders are generally willing to provide funds at a lower cost, which may reflect confidence in economic and political stability. Conversely, higher interest rates can indicate greater perceived risk, such as concerns about inflation, credit risk, or macroeconomic instability.\nTo obtain a global view of interest rates at a given date \\(t\\), one typically plots the interest rate as a function of maturity. This gives rise to what is called the term structure of interest rates. It describes the relationship between interest rates and the time to maturity of debt. In other words, it shows the rate at which investors are willing to lend money for different horizons (i.e.¬†dates of reimbursement). To extract the interest rate, it is common to use a financial product, such as a zero-coupon bond, because the link between price and interest rate is simple and determined by an analytical formula.\nIn this article, we focus on a specific model of the term structure of interest rates: the Vasicek model. We first explain its relationship with zero-coupon bond pricing and show how to move from the short-rate dynamics to bond prices, then introduce the model and its advantages. Finally, we discuss the limitations of the model, and suggest possible extensions to better capture the observed shape of the yield curve.\n\nThe yield curve vs the zero-coupon bond pricing\nAs mentioned in the introduction, the yield curve is a function \\(T \\mapsto r(T)\\) that represents, for each given maturity, the interest rate at which an individual can borrow or is willing to lend. In normal economic conditions, the yield curve is often upward sloping, meaning that long-term interest rates are higher than short-term rates. However, during periods of economic stress or when markets anticipate a recession, the curve may flatten or even become inverted. Therefore, the shape of the yield curve provides valuable information about market expectations regarding future interest rates, economic growth, and inflation.\n\n\n\n\n\n\n\n\n\nNormal yield curve\n\n\n\n\n\n\n\nInverted yield curve\n\n\n\n\n\n\nLeft: In normal economic conditions, the yield curve is typically upward sloping, meaning that long-term rates exceed short-term rates.\nRight: An inverted yield curve occurs when short-term rates are higher than long-term rates, often reflecting recession expectations.\n\nInterest rates are also used as discount factors. They allow us to determine the present value of money to be received at a future date. For example, if A lends 1 euro today at an interest rate \\(r\\), it means that he will be reimbursed \\(1 + r\\) at time \\(T\\). Therefore, the 1 euro lent today will be worth \\(1+r\\) in the future. Conversely, by a simple proportional reasoning, 1 euro to be received in the future corresponds to a present value of \\(\\frac{1}{1+r}\\) today.\nTo get more informations about interest rate use, check the article intitulated Types of interest rates and rates derivatives.\nInterest rate is not a traded product in the market. So they cannot be observe directly. Hence, one often use derivatives in other th know the quantity of the derivatives. In the case of a zero-coupon bond (ZC bond), the link between price and interest rate is particularly simple, as it is given by an analytical formula. In fact, let \\(B(t,T)\\) be the price of a zero-coupon bond and \\(R(t,T)\\) the interest rate associated. In continuous compounding interest rate, we have that : \\[\nR(t,T) = -\\frac{1}{T-t} \\ln(B(t,T)).\n\\]\nTherefore, one can represent the term structure through the zero-coupon bond prices as a function of maturity. By the way, since a zero-coupon bond is a product traded in the market, its price can be expressed as the expected discounted payoff under the risk-neutral measure:\n\\[\nB(t,T) = \\mathbb{E}^{\\mathbb{Q}}\\left[ \\exp\\left(-\\int_t^T r_s ds \\right) \\middle| \\mathcal{F}_t \\right]. \\quad (*)\n\\]\nHence, modeling the short rate process (\\(r_t\\)) directly determines the bond price dynamics. In other words, specifying a model for \\(r_t\\) allows us to derive the entire term structure of interest rates and compute zero-coupon bond prices for all maturities.\n\n\nA specific case of term structure modelling : The vasicek model\nThe Vasicek model (Vasicek 1977) was introduced in 1977 by Old≈ôich Va≈°√≠ƒçek to model the short rate process \\(r_t\\). It is inspired by the Ornstein‚ÄìUhlenbeck process, which is a Gaussian Markov process.\nWe work on \\((\\Omega,(\\mathcal{F}_t), \\mathbb{Q})\\), and the model assumes that the dynamics of \\(r_t\\) are given by:\n\\[\ndr_t = k(\\theta - r_t),dt + \\sigma, dW_t,\n\\]\nwith \\(W_t\\) a Brownian motion under the probability measure \\(\\mathbb{Q}\\).\nThe drift \\(k(\\theta - r_t)\\) represents the force that keeps pulling the short rate \\(r_t\\) towards its long-term mean \\(\\theta\\), with \\(k&gt;0\\) as the mean-reversion speed. The process has a constant instantaneous variance \\(\\sigma^2\\), causing the short rate to fluctuate around \\(\\theta\\).\nMoreover, the SDE admits a unique strong solution since the coefficients are Lipschitz and of linear growth. Furthermore, by applying It√¥‚Äôs formula to \\(e^{kt}r_t\\), we have for \\(0 \\le s \\le t\\):\n\\[\nr_t = r_s e^{-k(t-s)} + \\theta \\left( 1 - e^{-k(t-s)} \\right) + \\sigma \\int_s^t e^{-k(t-u)} dW_u.\n\\]\nHence, the conditional moments of \\(r_t\\) are given by:\n\n\\(\\mathbb{E}[r_t \\mid \\mathcal{F}_s] = r_s e^{-k(t-s)} + \\theta \\left( 1 - e^{-k(t-s)} \\right)\\)\n\\(\\mathbb{V}[r_t \\mid \\mathcal{F}_s] = \\frac{\\sigma^2}{2k} \\left( 1 - e^{-2k(t-s)} \\right)\\).\n\nUsing the link between the short rate and the zero-coupon bond price (*), since \\(\\int_t^T r_u,du \\mid \\mathcal{F}_t\\) is Gaussian, pricing reduces to computing the Laplace transform of a normal random variable. Hence, omitting some computational details, the bond price can be written in an affine form with respect to the short rate:\n\\[\n\\begin{align}\nB(t,T) &=  \\mathbb{E}^{\\mathbb{Q}}\\left[ \\exp\\left(-\\int_t^T r_u,du \\right) \\middle| \\mathcal{F}_t \\right] [4pt]\n&= \\exp\\left( m(t,T) - n(t,T), r_t\\right),\n\\end{align}\n\\]\nwith \\(n(t,T) := \\frac{ 1 - e^{-k(T-t)}}{k},\\) and \\(m(t,T) := \\left( \\theta - \\frac{\\sigma^2}{2k^2} \\right)\\left(B(t,T) - (T-t)\\right) - \\frac{\\sigma^2}{4k},B(t,T)^2.\\)\nNow, recalling the link between the zero-coupon bond price \\(B(t,T)\\) and the term structure interest rate \\(R(t,T)\\), i.e.¬†\\(R(t,T) = -\\frac{1}{T-t}\\ln B(t,T)\\), we obtain an analytical expression for the yield:\n\\[\nR(t,T) = R(\\infty) + (r_t - R(\\infty)) \\frac{ 1 - e^{-k(T-t)}}{k(T-t)} + \\frac{\\sigma^2}{4k^3 (T-t)} \\left( 1 - e^{-k(T-t)} \\right)^2,\n\\]\nwith \\(R(\\infty) = \\lim_{T \\rightarrow \\infty} R(t,T) = \\theta - \\frac{\\sigma^2}{2k^2}.\\)\n\n\nLimitations of the Vasicek model\nDespite its widespread use, the Vasicek model has several limitations due to the assumptions it relies on:\n\nNegative interest rates : The model allows the short rate to become negative, since it follows a Gaussian process. Historically, this was considered unrealistic. However, recent market environments with negative policy rates have made the Vasicek framework more relevant in certain contexts for modeling short-rate dynamics and extracting a yield curve.\nConstant volatility : The model assumes constant volatility \\(\\sigma\\), which is often violated in real-world markets. In practice, interest rate volatility is time-dependent and state-dependent. Moreover, market data (e.g., cap and swaption prices) exhibit volatility smiles and skews, which cannot be captured by a single Gaussian factor with constant volatility.\nOne-factor model : The Vasicek model is a one-factor model, meaning that the entire term structure is driven by a single source of randomness (one Brownian motion). This implies perfect correlation between rate movements across maturities. Such a simplification may not capture the empirical complexity of interest rate dynamics, where multiple factors (level, slope, curvature) are typically required.\nCalibration to the initial yield curve : Due to the small number of parameters \\((k, \\theta, \\sigma)\\), the Vasicek model cannot perfectly match the observed initial zero-coupon yield curve. To overcome this limitation, one can introduce a time-dependent mean-reversion level \\(\\theta(t)\\). This leads to the Hull‚ÄìWhite extension (Hull and White 2001), which allows exact calibration to the initial term structure.\n\n\n\n\n\n\nReferences\n\nHull, John, and Alan White. 2001. ‚ÄúThe General Hull‚ÄìWhite Model and Supercalibration.‚Äù Financial Analysts Journal 57 (6): 34‚Äì43.\n\n\nVasicek, Oldrich. 1977. ‚ÄúAn Equilibrium Characterization of the Term Structure.‚Äù Journal of Financial Economics 5 (2): 177‚Äì88."
  }
]